{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import h5py\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "from sklearn.manifold import TSNE, MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dropout, concatenate, Concatenate, Activation, Input, Dense, Conv2D, GRU, MaxPooling2D, MaxPooling1D, Flatten, Reshape, LeakyReLU, PReLU, BatchNormalization, Bidirectional, TimeDistributed, Lambda, GlobalMaxPool1D, GlobalMaxPool2D, GlobalAveragePooling2D, Multiply, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.initializers import random_normal, glorot_uniform, glorot_normal\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mel = './melspecs/'\n",
    "path_files = '../files/'\n",
    "\n",
    "train_triplet_file = 'train_triplets_50_70_single.pckl'\n",
    "train_gt_file = 'train_gt_50_70_single.pckl'\n",
    "train_cons_file = 'train_cons_50_70_single.pckl'\n",
    "train_trials_file = 'train_trials_50_70_single.pckl'\n",
    "\n",
    "test_triplet_file = 'test_triplets_50_70_single.pckl'\n",
    "test_gt_file = 'test_gt_50_70_single.pckl'\n",
    "test_cons_file = 'test_cons_50_70_single.pckl'\n",
    "test_trials_file = 'test_trials_50_70_single.pckl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "luscinia_triplets_file = 'luscinia_triplets_filtered.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "luscinia_triplets = []\n",
    "with open(path_files+luscinia_triplets_file, 'r',  newline='') as csvfile:\n",
    "    csv_r = csv.reader(csvfile, delimiter=',')\n",
    "    for row in csv_r:\n",
    "        luscinia_triplets.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "luscinia_triplets = luscinia_triplets[1:]\n",
    "luscinia_train_len = round(8*len(luscinia_triplets)/10)\n",
    "luscinia_val_len = len(luscinia_triplets) - luscinia_train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_files+'mean_std_luscinia_pretraining.pckl', 'rb')\n",
    "train_dict = pickle.load(f)\n",
    "M_l = train_dict['mean']\n",
    "S_l = train_dict['std']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_files+'training_setup_1_ordered_acc_single_cons_50_70_trials.pckl', 'rb')\n",
    "train_dict = pickle.load(f)\n",
    "train_keys = train_dict['train_keys']\n",
    "training_triplets = train_dict['train_triplets']\n",
    "val_keys = train_dict['val_keys']\n",
    "validation_triplets = train_dict['vali_triplets']\n",
    "test_triplet = train_dict['test_triplets']\n",
    "test_keys = train_dict['test_keys']\n",
    "M = train_dict['train_mean']\n",
    "S = train_dict['train_std']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convBNpr(a, dilation, num_filters, kernel):\n",
    "    c1 = Conv2D(filters=num_filters, kernel_size=kernel, strides=(1, 1), dilation_rate=dilation, padding='same', use_bias=False, kernel_initializer=glorot_uniform(seed=123), kernel_regularizer=regularizers.l2(1e-4))(a)\n",
    "    c1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(c1)\n",
    "    c1 = LeakyReLU(alpha=0.3)(c1)\n",
    "    return c1\n",
    "def createModelMatrix(emb_size, input_shape=(170, 150, 1)):\n",
    "    a = Input(shape=(input_shape)) \n",
    "    \n",
    "    c = convBNpr(a, 1, 12, (3,3))\n",
    "    c = convBNpr(c, 2, 32, (3,3))\n",
    "    c = convBNpr(c, 4, 64, (3,3))\n",
    "    c = convBNpr(c, 8, 128, (3,3))\n",
    "    # attention with sigmoid\n",
    "    a1 = Conv2D(filters=128, kernel_size=(1,1), strides=(1, 1), padding='same', activation = 'sigmoid',  use_bias=True, kernel_regularizer=regularizers.l2(1e-4),kernel_initializer=glorot_uniform(seed=123))(c)\n",
    "\n",
    "    \n",
    "    # sum of sum of attention\n",
    "    s = Lambda(lambda x: K.sum(K.sum(x,axis=1, keepdims=True), axis=2, keepdims=True))(a1)\n",
    "    s = Lambda(lambda x: K.repeat_elements(x, 170, axis=1))(s)\n",
    "    s = Lambda(lambda x: K.repeat_elements(x, 150, axis=2))(s)\n",
    "    \n",
    "    # probability matrix of attention\n",
    "    p = Lambda(lambda x: x[0]/x[1])([a1,s])\n",
    "    \n",
    "    # inner product of attention and projection matrices\n",
    "    m = Multiply()([c, p])\n",
    "    \n",
    "    # output\n",
    "    out_sum = Lambda(lambda x: K.sum(K.sum(x, axis=1), axis=1))(m)\n",
    "    \n",
    "    # attention side\n",
    "    d3 = Dense(emb_size*100, kernel_initializer=glorot_normal(seed=222), kernel_regularizer=regularizers.l2(1e-4),activation='relu')(out_sum)\n",
    "    d3 = Dropout(.2, seed=222)(d3)\n",
    "    d4 = Dense(emb_size*10, kernel_initializer=glorot_normal(seed=333), kernel_regularizer=regularizers.l2(1e-4),activation='relu')(d3)\n",
    "    d4 = Dropout(.2, seed=333)(d4)\n",
    "    d5 = Dense(emb_size, kernel_initializer=glorot_normal(seed=132), kernel_regularizer=regularizers.l2(1e-4))(d4)\n",
    "    d5 = Dropout(.2, seed = 132)(d5)\n",
    "    \n",
    "    # maxpool side\n",
    "    x = convBNpr(c, 1, 64, (3,3)) \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = convBNpr(x, 1, 32, (3,3))\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = convBNpr(x, 1, 12, (3,3))\n",
    "\n",
    "    f = Flatten()(x)\n",
    "    df1 = Dense(emb_size*100, kernel_initializer=glorot_normal(seed=456), kernel_regularizer=regularizers.l2(1e-4), activation='relu')(f)#\n",
    "    df1 = Dropout(.2, seed=456)(df1)\n",
    "    df2 = Dense(emb_size*10, kernel_initializer=glorot_normal(seed=654), kernel_regularizer=regularizers.l2(1e-4), activation='relu')(df1)#\n",
    "    df2 = Dropout(.2, seed=654)(df2)\n",
    "    df3 = Dense(emb_size, kernel_initializer=glorot_normal(seed=546), kernel_regularizer=regularizers.l2(1e-4))(df2)#\n",
    "    df3 = Dropout(.2, seed=546)(df3)\n",
    "\n",
    "    concat = Concatenate(axis=-1)([d5, df3])\n",
    "    dd = Dense(emb_size, kernel_initializer=glorot_normal(seed=999), kernel_regularizer=regularizers.l2(1e-4))(concat)\n",
    "\n",
    "    sph = Lambda(lambda  x: K.l2_normalize(x,axis=1))(dd)\n",
    "    \n",
    "    # base model creation\n",
    "    base_model = Model(a,sph) \n",
    "    \n",
    "    # triplet framework\n",
    "    input_anchor = Input(shape=(input_shape))\n",
    "    input_positive = Input(shape=(input_shape))\n",
    "    input_negative = Input(shape=(input_shape)) \n",
    "    \n",
    "    net_anchor = base_model(input_anchor)\n",
    "    net_positive = base_model(input_positive)\n",
    "    net_negative = base_model(input_negative)\n",
    "    \n",
    "    base_model.summary()\n",
    "    \n",
    "    merged_vector = concatenate([net_anchor, net_positive, net_negative], axis=-1)\n",
    "    \n",
    "    model = Model([input_anchor, input_positive, input_negative], outputs=merged_vector)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_weighted_triplet_loss(margin, emb_size, m = 0 , w = 0, lh = 1):\n",
    "    def lossFunction(y_true,y_pred):\n",
    "        \n",
    "        weight = y_true[:, 0] # acc\n",
    "        cons = y_true[:, 1] # consistency\n",
    "        trials = y_true[:, 2] # number of trials\n",
    "        \n",
    "        anchor = y_pred[:, 0:emb_size]\n",
    "        positive = y_pred[:, emb_size:emb_size*2]\n",
    "        negative = y_pred[:, emb_size*2:emb_size*3]\n",
    "\n",
    "        # distance between the anchor and the positive\n",
    "        pos_dist = K.sqrt(K.sum(K.square(anchor - positive), axis=1)) # l2 distance\n",
    "        #pos_dist = K.sum(K.abs(anchor-positive), axis=1) # l1 distance\n",
    "\n",
    "        # distance between the anchor and the negative\n",
    "        neg_dist = K.sqrt(K.sum(K.square(anchor - negative), axis=1)) # l2 distance\n",
    "        #neg_dist = K.sum(K.abs(anchor-negative), axis=1) # l1 distance\n",
    "\n",
    "        loss_h = 0\n",
    "        loss_l = 0\n",
    "        \n",
    "        if lh == 1:\n",
    "            # DOES NOT WORK WITH MASKED LOSS\n",
    "            # low-high margin loss\n",
    "            p_c = K.square(neg_dist) - K.square(pos_dist) - margin  \n",
    "            p_i = K.square(neg_dist) - K.square(pos_dist)\n",
    "            \n",
    "            loss_1 = cons*(1-K.exp(p_c)) + (1-cons)*(1-K.exp(-K.abs(p_i)))\n",
    "            \n",
    "        if m != 0:\n",
    "            # masked loss\n",
    "            basic_loss = pos_dist - neg_dist + margin\n",
    "            \n",
    "            threshold = K.max(basic_loss) * m\n",
    "            mask = 2 + margin - K.maximum(basic_loss, threshold) \n",
    "\n",
    "            loss_1 = basic_loss * mask\n",
    "        \n",
    "        if w == 1:\n",
    "            # weighted based on acc\n",
    "            weighted_loss = weight*loss_1\n",
    "        else:\n",
    "            # non-weighted\n",
    "            weighted_loss = loss_1\n",
    "            \n",
    "        loss = K.maximum(weighted_loss, 0.0)\n",
    "\n",
    "        return loss\n",
    "    return lossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_some_low(triplet_list, cons, acc):\n",
    "    low_margin = []\n",
    "    high_margin = []\n",
    "    \n",
    "    for i in range(len(triplet_list)):\n",
    "        if float(triplet_list[i][-1]) < cons: # low margin\n",
    "            if float(triplet_list[i][-2]) >= acc: # ACC \n",
    "                low_margin.append(triplet_list[i])\n",
    "        else: # high margin\n",
    "            high_margin.append(triplet_list[i])\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(low_margin)\n",
    "    random.shuffle(high_margin)\n",
    "    \n",
    "    low_margin.extend(high_margin)\n",
    "        \n",
    "    return low_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_input(triplet_list, cons, hi_balance = 6, lo_balance = 6):\n",
    "    batchsize = hi_balance + lo_balance\n",
    "    low_margin = []\n",
    "    high_margin = []\n",
    "    \n",
    "    for i in range(len(triplet_list)):\n",
    "        if float(triplet_list[i][-1]) < cons: # low margin\n",
    "            low_margin.append(triplet_list[i])\n",
    "        else: # high margin\n",
    "            high_margin.append(triplet_list[i])\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(low_margin)\n",
    "    random.shuffle(high_margin)\n",
    "    \n",
    "    new_triplet_list = []\n",
    "    maxlen = np.maximum(len(low_margin), len(high_margin))\n",
    "    \n",
    "    hi_start = 0\n",
    "    lo_start = 0\n",
    "    for i in range(0,int(maxlen/hi_balance)*batchsize,batchsize):\n",
    "        for j in range(hi_start,hi_start+hi_balance,1):\n",
    "            new_triplet_list.append(high_margin[np.mod(j,len(high_margin))])\n",
    "        hi_start+=hi_balance\n",
    "        for j in range(lo_start, lo_start+lo_balance,1):\n",
    "            new_triplet_list.append(low_margin[np.mod(j,len(low_margin))])\n",
    "        lo_start+=lo_balance\n",
    "            \n",
    "    return low_margin, high_margin, new_triplet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_mixed(triplet_list, M, S, luscinia_triplets, M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel):\n",
    "    \n",
    "    acc_gt = np.zeros((batchsize, emb_size))\n",
    "   \n",
    "    random.seed(123)\n",
    "    random.shuffle(luscinia_triplets)\n",
    "    \n",
    "    while 1:\n",
    "    \n",
    "        anchors_input = np.empty((batchsize, 170, 150, 1))\n",
    "        positives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        negatives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        \n",
    "        imax = int(len(triplet_list)/(lo+hi))\n",
    "        \n",
    "        list_cnt = 0\n",
    "        luscinia_cnt = 0\n",
    "        \n",
    "        for i in range(imax):        \n",
    "            for j in range(batchsize):\n",
    "                \n",
    "                if j < (lo+hi):\n",
    "                    triplet = triplet_list[list_cnt]\n",
    "                    list_cnt += 1\n",
    "                    \n",
    "                    tr_anc = triplet[3][:-4]+'.pckl'\n",
    "                    tr_pos = triplet[1][:-4]+'.pckl'\n",
    "                    tr_neg = triplet[2][:-4]+'.pckl'\n",
    "                    acc_gt[j][0] = float(triplet[-2]) # acc\n",
    "                    acc_gt[j][1] = 1 if float(triplet[-1])>=0.7 else 0 # cons\n",
    "                    acc_gt[j][2] = int(triplet[-3]) # number of trials\n",
    "\n",
    "                else:\n",
    "                    triplet = luscinia_triplets[luscinia_cnt]\n",
    "                    luscinia_cnt += 1\n",
    "                    \n",
    "                    tr_anc = triplet[2][:-4]+'.pckl'\n",
    "                    tr_pos = triplet[0][:-4]+'.pckl'\n",
    "                    tr_neg = triplet[1][:-4]+'.pckl'\n",
    "                    acc_gt[j][0] = 1 # acc\n",
    "                    acc_gt[j][1] = 1  # cons\n",
    "                    acc_gt[j][2] = 1 # number of trials\n",
    "                \n",
    "                f = open(path_mel+tr_anc, 'rb')\n",
    "                anc = pickle.load(f).T\n",
    "                f.close()\n",
    "                anc = (anc - M)/S\n",
    "                anc = np.expand_dims(anc, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_pos, 'rb')\n",
    "                pos = pickle.load(f).T\n",
    "                f.close()\n",
    "                pos = (pos - M)/S\n",
    "                pos = np.expand_dims(pos, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_neg, 'rb')\n",
    "                neg = pickle.load(f).T\n",
    "                f.close()\n",
    "                neg = (neg - M)/S\n",
    "                neg = np.expand_dims(neg, axis=-1)\n",
    "                \n",
    "                anchors_input[j] = anc\n",
    "                positives_input[j] = pos\n",
    "                negatives_input[j] = neg\n",
    "                \n",
    "            yield [anchors_input, positives_input, negatives_input], acc_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_luscinia(triplet_list, M, S, batchsize, emb_size, path_mel, ordered = True):\n",
    "    \n",
    "    acc_gt = np.zeros((batchsize, emb_size))\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(triplet_list) \n",
    "    \n",
    "    while 1:\n",
    "    \n",
    "        anchors_input = np.empty((batchsize, 170, 150, 1))\n",
    "        positives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        negatives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        \n",
    "        imax = int(len(triplet_list)/batchsize)\n",
    "                \n",
    "        for i in range(imax):        \n",
    "            for j in range(batchsize):\n",
    "                triplet = triplet_list[i*batchsize+j]\n",
    "                \n",
    "                tr_anc = triplet[2][:-4]+'.pckl'\n",
    "                tr_pos = triplet[0][:-4]+'.pckl'\n",
    "                tr_neg = triplet[1][:-4]+'.pckl'\n",
    "                acc_gt[j][0] = 1 # acc\n",
    "                acc_gt[j][1] = 1  # cons\n",
    "                acc_gt[j][2] = 1 # number of trials\n",
    "                 \n",
    "                f = open(path_mel+tr_anc, 'rb')\n",
    "                anc = pickle.load(f).T\n",
    "                f.close()\n",
    "                anc = (anc - M)/S\n",
    "                anc = np.expand_dims(anc, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_pos, 'rb')\n",
    "                pos = pickle.load(f).T\n",
    "                f.close()\n",
    "                pos = (pos - M)/S\n",
    "                pos = np.expand_dims(pos, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_neg, 'rb')\n",
    "                neg = pickle.load(f).T\n",
    "                f.close()\n",
    "                neg = (neg - M)/S\n",
    "                neg = np.expand_dims(neg, axis=-1)\n",
    "                \n",
    "                anchors_input[j] = anc\n",
    "                positives_input[j] = pos\n",
    "                negatives_input[j] = neg\n",
    "                \n",
    "            yield [anchors_input, positives_input, negatives_input], acc_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(triplet_list, M, S, batchsize, emb_size, path_mel, ordered = True):\n",
    "    \n",
    "    acc_gt = np.zeros((batchsize, emb_size))\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(triplet_list) \n",
    "    \n",
    "    while 1:\n",
    "    \n",
    "        anchors_input = np.empty((batchsize, 170, 150, 1))\n",
    "        positives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        negatives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        \n",
    "        imax = int(len(triplet_list)/batchsize)\n",
    "                \n",
    "        for i in range(imax):        \n",
    "            for j in range(batchsize):\n",
    "                triplet = triplet_list[i*batchsize+j]\n",
    "                \n",
    "                tr_anc = triplet[3][:-4]+'.pckl'\n",
    "                \n",
    "                if ordered == False:\n",
    "                    if triplet[-1] == '0':\n",
    "                        tr_pos = triplet[1][:-4]+'.pckl'\n",
    "                        tr_neg = triplet[2][:-4]+'.pckl'\n",
    "                    else:\n",
    "                        tr_pos = triplet[2][:-4]+'.pckl'\n",
    "                        tr_neg = triplet[1][:-4]+'.pckl'\n",
    "                else: \n",
    "                    tr_pos = triplet[1][:-4]+'.pckl'\n",
    "                    tr_neg = triplet[2][:-4]+'.pckl'\n",
    "                    acc_gt[j][0] = float(triplet[-2]) # acc\n",
    "                    acc_gt[j][1] = 1 if float(triplet[-1])>=0.7 else 0 # cons\n",
    "                    acc_gt[j][2] = int(triplet[-3]) # number of trials\n",
    "                   \n",
    "                f = open(path_mel+tr_anc, 'rb')\n",
    "                anc = pickle.load(f).T\n",
    "                f.close()\n",
    "                anc = (anc - M)/S\n",
    "                anc = np.expand_dims(anc, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_pos, 'rb')\n",
    "                pos = pickle.load(f).T\n",
    "                f.close()\n",
    "                pos = (pos - M)/S\n",
    "                pos = np.expand_dims(pos, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_neg, 'rb')\n",
    "                neg = pickle.load(f).T\n",
    "                f.close()\n",
    "                neg = (neg - M)/S\n",
    "                neg = np.expand_dims(neg, axis=-1)\n",
    "                \n",
    "                anchors_input[j] = anc\n",
    "                positives_input[j] = pos\n",
    "                negatives_input[j] = neg\n",
    "                \n",
    "            yield [anchors_input, positives_input, negatives_input], acc_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 170, 150, 12) 108         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 170, 150, 12) 48          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 170, 150, 12) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 170, 150, 32) 3456        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 170, 150, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 170, 150, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 170, 150, 64) 18432       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 170, 150, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 170, 150, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 170, 150, 128 73728       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 170, 150, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 170, 150, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 170, 150, 64) 73728       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 170, 150, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 170, 150, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 85, 75, 64)   0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 85, 75, 32)   18432       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 170, 150, 128 16512       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 85, 75, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1, 1, 128)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 85, 75, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 170, 1, 128)  0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 42, 37, 32)   0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 170, 150, 128 0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 42, 37, 12)   3456        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 170, 150, 128 0           conv2d_4[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 42, 37, 12)   48          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 170, 150, 128 0           leaky_re_lu_3[0][0]              \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 42, 37, 12)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 128)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 18648)        0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1600)         206400      lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1600)         29838400    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1600)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1600)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 160)          256160      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 160)          256160      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 160)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 160)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2576        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2576        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32)           0           dropout_2[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           528         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16)           0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,772,028\n",
      "Trainable params: 30,771,340\n",
      "Non-trainable params: 688\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 16)           30772028    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48)           0           model[0][0]                      \n",
      "                                                                 model[1][0]                      \n",
      "                                                                 model[2][0]                      \n",
      "==================================================================================================\n",
      "Total params: 30,772,028\n",
      "Trainable params: 30,771,340\n",
      "Non-trainable params: 688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size=16\n",
    "margin = 0.1\n",
    "m = 0\n",
    "lr = 1e-8\n",
    "adam = Adam(lr = lr)\n",
    "\n",
    "triplet_model = createModelMatrix(emb_size=emb_size, input_shape=(170, 150, 1))\n",
    "triplet_model.summary()\n",
    "triplet_model.compile(loss=masked_weighted_triplet_loss(margin=margin, emb_size=emb_size, m=m, w = 0, lh = 1),optimizer=adam) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE (pretraining on Luscinia triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "348/348 [==============================] - 171s 346ms/step - loss: 0.5265 - val_loss: 0.4591\n",
      "Epoch 2/1000\n",
      "348/348 [==============================] - 119s 342ms/step - loss: 0.4536 - val_loss: 0.4486\n",
      "Epoch 3/1000\n",
      "348/348 [==============================] - 119s 342ms/step - loss: 0.4392 - val_loss: 0.4454\n",
      "Epoch 4/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4334 - val_loss: 0.4415\n",
      "Epoch 5/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4304 - val_loss: 0.4401\n",
      "Epoch 6/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4283 - val_loss: 0.4373\n",
      "Epoch 7/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4273 - val_loss: 0.4341\n",
      "Epoch 8/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4250 - val_loss: 0.4324\n",
      "Epoch 9/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4237 - val_loss: 0.4305\n",
      "Epoch 10/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4225 - val_loss: 0.4294\n",
      "Epoch 11/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4211 - val_loss: 0.4287\n",
      "Epoch 12/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4205 - val_loss: 0.4278\n",
      "Epoch 13/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4192 - val_loss: 0.4261\n",
      "Epoch 14/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4182 - val_loss: 0.4254\n",
      "Epoch 15/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4169 - val_loss: 0.4238\n",
      "Epoch 16/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4155 - val_loss: 0.4229\n",
      "Epoch 17/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4143 - val_loss: 0.4208\n",
      "Epoch 18/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4128 - val_loss: 0.4198\n",
      "Epoch 19/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4117 - val_loss: 0.4165\n",
      "Epoch 20/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4103 - val_loss: 0.4160\n",
      "Epoch 21/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.4086 - val_loss: 0.4148\n",
      "Epoch 22/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4069 - val_loss: 0.4140\n",
      "Epoch 23/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4053 - val_loss: 0.4140\n",
      "Epoch 24/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4038 - val_loss: 0.4105\n",
      "Epoch 25/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4022 - val_loss: 0.4080\n",
      "Epoch 26/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4006 - val_loss: 0.4055\n",
      "Epoch 27/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3990 - val_loss: 0.4061\n",
      "Epoch 28/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.3972 - val_loss: 0.4034\n",
      "Epoch 29/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3955 - val_loss: 0.4016\n",
      "Epoch 30/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3937 - val_loss: 0.3982\n",
      "Epoch 31/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3920 - val_loss: 0.3959\n",
      "Epoch 32/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3904 - val_loss: 0.3936\n",
      "Epoch 33/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3887 - val_loss: 0.3930\n",
      "Epoch 34/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3867 - val_loss: 0.3898\n",
      "Epoch 35/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3849 - val_loss: 0.3885\n",
      "Epoch 36/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3829 - val_loss: 0.3864\n",
      "Epoch 37/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3809 - val_loss: 0.3850\n",
      "Epoch 38/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3789 - val_loss: 0.3829\n",
      "Epoch 39/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3768 - val_loss: 0.3799\n",
      "Epoch 40/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3747 - val_loss: 0.3797\n",
      "Epoch 41/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3724 - val_loss: 0.3767\n",
      "Epoch 42/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3701 - val_loss: 0.3738\n",
      "Epoch 43/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3678 - val_loss: 0.3706\n",
      "Epoch 44/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3654 - val_loss: 0.3684\n",
      "Epoch 45/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3631 - val_loss: 0.3654\n",
      "Epoch 46/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3608 - val_loss: 0.3624\n",
      "Epoch 47/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3583 - val_loss: 0.3606\n",
      "Epoch 48/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3560 - val_loss: 0.3587\n",
      "Epoch 49/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3537 - val_loss: 0.3563\n",
      "Epoch 50/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3515 - val_loss: 0.3542\n",
      "Epoch 51/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3491 - val_loss: 0.3525\n",
      "Epoch 52/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3467 - val_loss: 0.3510\n",
      "Epoch 53/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3447 - val_loss: 0.3481\n",
      "Epoch 54/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3423 - val_loss: 0.3459\n",
      "Epoch 55/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3402 - val_loss: 0.3432\n",
      "Epoch 56/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3381 - val_loss: 0.3413\n",
      "Epoch 57/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3359 - val_loss: 0.3405\n",
      "Epoch 58/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3335 - val_loss: 0.3379\n",
      "Epoch 59/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3311 - val_loss: 0.3344\n",
      "Epoch 60/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3287 - val_loss: 0.3322\n",
      "Epoch 61/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3263 - val_loss: 0.3293\n",
      "Epoch 62/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3237 - val_loss: 0.3266\n",
      "Epoch 63/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3212 - val_loss: 0.3241\n",
      "Epoch 64/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3187 - val_loss: 0.3216\n",
      "Epoch 65/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3161 - val_loss: 0.3200\n",
      "Epoch 66/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3136 - val_loss: 0.3164\n",
      "Epoch 67/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3111 - val_loss: 0.3137\n",
      "Epoch 68/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3085 - val_loss: 0.3119\n",
      "Epoch 69/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3058 - val_loss: 0.3094\n",
      "Epoch 70/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3031 - val_loss: 0.3067\n",
      "Epoch 71/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3005 - val_loss: 0.3051\n",
      "Epoch 72/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2979 - val_loss: 0.3013\n",
      "Epoch 73/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2955 - val_loss: 0.2980\n",
      "Epoch 74/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2933 - val_loss: 0.2962\n",
      "Epoch 75/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2914 - val_loss: 0.2943\n",
      "Epoch 76/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2894 - val_loss: 0.2921\n",
      "Epoch 77/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2873 - val_loss: 0.2896\n",
      "Epoch 78/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2853 - val_loss: 0.2877\n",
      "Epoch 79/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2832 - val_loss: 0.2846\n",
      "Epoch 80/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2810 - val_loss: 0.2818\n",
      "Epoch 81/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2789 - val_loss: 0.2802\n",
      "Epoch 82/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2768 - val_loss: 0.2783\n",
      "Epoch 83/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2747 - val_loss: 0.2765\n",
      "Epoch 84/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2724 - val_loss: 0.2739\n",
      "Epoch 85/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2701 - val_loss: 0.2720\n",
      "Epoch 86/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2677 - val_loss: 0.2700\n",
      "Epoch 87/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2655 - val_loss: 0.2678\n",
      "Epoch 88/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2633 - val_loss: 0.2673\n",
      "Epoch 89/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2613 - val_loss: 0.2636\n",
      "Epoch 90/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2593 - val_loss: 0.2633\n",
      "Epoch 91/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2574 - val_loss: 0.2602\n",
      "Epoch 92/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2555 - val_loss: 0.2584\n",
      "Epoch 93/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2538 - val_loss: 0.2567\n",
      "Epoch 94/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2522 - val_loss: 0.2559\n",
      "Epoch 95/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2506 - val_loss: 0.2518\n",
      "Epoch 96/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2490 - val_loss: 0.2513\n",
      "Epoch 97/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2473 - val_loss: 0.2494\n",
      "Epoch 98/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2455 - val_loss: 0.2474\n",
      "Epoch 99/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2436 - val_loss: 0.2456\n",
      "Epoch 100/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2418 - val_loss: 0.2454\n",
      "Epoch 101/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2400 - val_loss: 0.2430\n",
      "Epoch 102/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2382 - val_loss: 0.2421\n",
      "Epoch 103/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2365 - val_loss: 0.2400\n",
      "Epoch 104/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2346 - val_loss: 0.2379\n",
      "Epoch 105/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2326 - val_loss: 0.2356\n",
      "Epoch 106/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2305 - val_loss: 0.2336\n",
      "Epoch 107/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2284 - val_loss: 0.2311\n",
      "Epoch 108/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2262 - val_loss: 0.2286\n",
      "Epoch 109/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2241 - val_loss: 0.2276\n",
      "Epoch 110/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2220 - val_loss: 0.2255\n",
      "Epoch 111/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2200 - val_loss: 0.2241\n",
      "Epoch 112/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2179 - val_loss: 0.2212\n",
      "Epoch 113/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.2157 - val_loss: 0.2199\n",
      "Epoch 114/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2135 - val_loss: 0.2170\n",
      "Epoch 115/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2116 - val_loss: 0.2136\n",
      "Epoch 116/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2100 - val_loss: 0.2119\n",
      "Epoch 117/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.2084 - val_loss: 0.2109\n",
      "Epoch 118/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2070 - val_loss: 0.2103\n",
      "Epoch 119/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2056 - val_loss: 0.2089\n",
      "Epoch 120/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2041 - val_loss: 0.2081\n",
      "Epoch 121/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2027 - val_loss: 0.2075\n",
      "Epoch 122/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2013 - val_loss: 0.2060\n",
      "Epoch 123/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1998 - val_loss: 0.2047\n",
      "Epoch 124/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1985 - val_loss: 0.2024\n",
      "Epoch 125/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1972 - val_loss: 0.2012\n",
      "Epoch 126/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1960 - val_loss: 0.1994\n",
      "Epoch 127/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1947 - val_loss: 0.1975\n",
      "Epoch 128/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1933 - val_loss: 0.1960\n",
      "Epoch 129/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1919 - val_loss: 0.1944\n",
      "Epoch 130/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1904 - val_loss: 0.1927\n",
      "Epoch 131/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1890 - val_loss: 0.1917\n",
      "Epoch 132/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1876 - val_loss: 0.1912\n",
      "Epoch 133/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1863 - val_loss: 0.1896\n",
      "Epoch 134/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1852 - val_loss: 0.1905\n",
      "Epoch 135/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1838 - val_loss: 0.1888\n",
      "Epoch 136/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1825 - val_loss: 0.1872\n",
      "Epoch 137/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1813 - val_loss: 0.1845\n",
      "Epoch 138/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1800 - val_loss: 0.1838\n",
      "Epoch 139/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1788 - val_loss: 0.1828\n",
      "Epoch 140/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1775 - val_loss: 0.1808\n",
      "Epoch 141/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1762 - val_loss: 0.1796\n",
      "Epoch 142/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1748 - val_loss: 0.1779\n",
      "Epoch 143/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1734 - val_loss: 0.1770\n",
      "Epoch 144/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1722 - val_loss: 0.1735\n",
      "Epoch 145/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1711 - val_loss: 0.1730\n",
      "Epoch 146/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1702 - val_loss: 0.1720\n",
      "Epoch 147/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1693 - val_loss: 0.1706\n",
      "Epoch 148/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1684 - val_loss: 0.1696\n",
      "Epoch 149/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1674 - val_loss: 0.1682\n",
      "Epoch 150/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1663 - val_loss: 0.1683\n",
      "Epoch 151/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1653 - val_loss: 0.1662\n",
      "Epoch 152/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1642 - val_loss: 0.1651\n",
      "Epoch 153/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1631 - val_loss: 0.1639\n",
      "Epoch 154/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1618 - val_loss: 0.1628\n",
      "Epoch 155/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1605 - val_loss: 0.1617\n",
      "Epoch 156/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1591 - val_loss: 0.1595\n",
      "Epoch 157/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1580 - val_loss: 0.1589\n",
      "Epoch 158/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1571 - val_loss: 0.1579\n",
      "Epoch 159/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1563 - val_loss: 0.1576\n",
      "Epoch 160/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1555 - val_loss: 0.1570\n",
      "Epoch 161/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1548 - val_loss: 0.1562\n",
      "Epoch 162/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1540 - val_loss: 0.1553\n",
      "Epoch 163/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1533 - val_loss: 0.1543\n",
      "Epoch 164/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1525 - val_loss: 0.1545\n",
      "Epoch 165/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1516 - val_loss: 0.1530\n",
      "Epoch 166/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1507 - val_loss: 0.1523\n",
      "Epoch 167/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1498 - val_loss: 0.1518\n",
      "Epoch 168/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1488 - val_loss: 0.1523\n",
      "Epoch 169/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1480 - val_loss: 0.1506\n",
      "Epoch 170/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1471 - val_loss: 0.1496\n",
      "Epoch 171/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1463 - val_loss: 0.1492\n",
      "Epoch 172/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1455 - val_loss: 0.1484\n",
      "Epoch 173/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1447 - val_loss: 0.1465\n",
      "Epoch 174/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1438 - val_loss: 0.1452\n",
      "Epoch 175/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1429 - val_loss: 0.1441\n",
      "Epoch 176/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1419 - val_loss: 0.1438\n",
      "Epoch 177/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1410 - val_loss: 0.1430\n",
      "Epoch 178/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1400 - val_loss: 0.1419\n",
      "Epoch 179/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1390 - val_loss: 0.1404\n",
      "Epoch 180/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1380 - val_loss: 0.1395\n",
      "Epoch 181/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1370 - val_loss: 0.1389\n",
      "Epoch 182/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1361 - val_loss: 0.1386\n",
      "Epoch 183/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1353 - val_loss: 0.1382\n",
      "Epoch 184/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1345 - val_loss: 0.1367\n",
      "Epoch 185/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1338 - val_loss: 0.1359\n",
      "Epoch 186/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1331 - val_loss: 0.1347\n",
      "Epoch 187/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1324 - val_loss: 0.1338\n",
      "Epoch 188/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1318 - val_loss: 0.1332\n",
      "Epoch 189/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1311 - val_loss: 0.1325\n",
      "Epoch 190/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1305 - val_loss: 0.1324\n",
      "Epoch 191/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1299 - val_loss: 0.1320\n",
      "Epoch 192/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1292 - val_loss: 0.1320\n",
      "Epoch 193/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1285 - val_loss: 0.1299\n",
      "Epoch 194/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1278 - val_loss: 0.1300\n",
      "Epoch 195/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1272 - val_loss: 0.1293\n",
      "Epoch 196/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1265 - val_loss: 0.1293\n",
      "Epoch 197/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1259 - val_loss: 0.1283\n",
      "Epoch 198/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1254 - val_loss: 0.1269\n",
      "Epoch 199/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1248 - val_loss: 0.1266\n",
      "Epoch 200/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1243 - val_loss: 0.1260\n",
      "Epoch 201/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1237 - val_loss: 0.1254\n",
      "Epoch 202/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1231 - val_loss: 0.1252\n",
      "Epoch 203/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1224 - val_loss: 0.1243\n",
      "Epoch 204/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1217 - val_loss: 0.1238\n",
      "Epoch 205/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1210 - val_loss: 0.1230\n",
      "Epoch 206/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1202 - val_loss: 0.1222\n",
      "Epoch 207/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1194 - val_loss: 0.1212\n",
      "Epoch 208/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1187 - val_loss: 0.1209\n",
      "Epoch 209/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1178 - val_loss: 0.1202\n",
      "Epoch 210/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1170 - val_loss: 0.1195\n",
      "Epoch 211/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1162 - val_loss: 0.1183\n",
      "Epoch 212/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1155 - val_loss: 0.1176\n",
      "Epoch 213/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1149 - val_loss: 0.1167\n",
      "Epoch 214/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1143 - val_loss: 0.1159\n",
      "Epoch 215/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1138 - val_loss: 0.1165\n",
      "Epoch 216/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1133 - val_loss: 0.1156\n",
      "Epoch 217/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1127 - val_loss: 0.1157\n",
      "Epoch 218/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1123 - val_loss: 0.1137\n",
      "Epoch 219/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1116 - val_loss: 0.1133\n",
      "Epoch 220/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1111 - val_loss: 0.1122\n",
      "Epoch 221/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1105 - val_loss: 0.1115\n",
      "Epoch 222/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1099 - val_loss: 0.1106\n",
      "Epoch 223/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1092 - val_loss: 0.1103\n",
      "Epoch 224/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1086 - val_loss: 0.1095\n",
      "Epoch 225/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1081 - val_loss: 0.1090\n",
      "Epoch 226/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1075 - val_loss: 0.1084\n",
      "Epoch 227/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1069 - val_loss: 0.1081\n",
      "Epoch 228/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1063 - val_loss: 0.1075\n",
      "Epoch 229/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1057 - val_loss: 0.1077\n",
      "Epoch 230/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1050 - val_loss: 0.1067\n",
      "Epoch 231/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1044 - val_loss: 0.1061\n",
      "Epoch 232/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1037 - val_loss: 0.1054\n",
      "Epoch 233/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1031 - val_loss: 0.1045\n",
      "Epoch 234/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1025 - val_loss: 0.1038\n",
      "Epoch 235/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1018 - val_loss: 0.1032\n",
      "Epoch 236/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1011 - val_loss: 0.1025\n",
      "Epoch 237/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.1004 - val_loss: 0.1018\n",
      "Epoch 238/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0997 - val_loss: 0.1012\n",
      "Epoch 239/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0991 - val_loss: 0.1014\n",
      "Epoch 240/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0986 - val_loss: 0.1004\n",
      "Epoch 241/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0981 - val_loss: 0.1000\n",
      "Epoch 242/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0977 - val_loss: 0.0995\n",
      "Epoch 243/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0972 - val_loss: 0.1002\n",
      "Epoch 244/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0968 - val_loss: 0.0994\n",
      "Epoch 245/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0964 - val_loss: 0.0992\n",
      "Epoch 246/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0961 - val_loss: 0.0987\n",
      "Epoch 247/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0957 - val_loss: 0.0981\n",
      "Epoch 248/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0953 - val_loss: 0.0975\n",
      "Epoch 249/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0949 - val_loss: 0.0968\n",
      "Epoch 250/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0944 - val_loss: 0.0963\n",
      "Epoch 251/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0940 - val_loss: 0.0957\n",
      "Epoch 252/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0935 - val_loss: 0.0951\n",
      "Epoch 253/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0931 - val_loss: 0.0955\n",
      "Epoch 254/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0927 - val_loss: 0.0946\n",
      "Epoch 255/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0924 - val_loss: 0.0938\n",
      "Epoch 256/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0920 - val_loss: 0.0937\n",
      "Epoch 257/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0916 - val_loss: 0.0933\n",
      "Epoch 258/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0912 - val_loss: 0.0934\n",
      "Epoch 259/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0909 - val_loss: 0.0930\n",
      "Epoch 260/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0905 - val_loss: 0.0923\n",
      "Epoch 261/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0901 - val_loss: 0.0919\n",
      "Epoch 262/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0897 - val_loss: 0.0919\n",
      "Epoch 263/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0894 - val_loss: 0.0922\n",
      "Epoch 264/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0890 - val_loss: 0.0919\n",
      "Epoch 265/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0887 - val_loss: 0.0912\n",
      "Epoch 266/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0883 - val_loss: 0.0908\n",
      "Epoch 267/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0879 - val_loss: 0.0902\n",
      "Epoch 268/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0875 - val_loss: 0.0898\n",
      "Epoch 269/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0871 - val_loss: 0.0898\n",
      "Epoch 270/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0866 - val_loss: 0.0897\n",
      "Epoch 271/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0862 - val_loss: 0.0892\n",
      "Epoch 272/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0857 - val_loss: 0.0887\n",
      "Epoch 273/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0853 - val_loss: 0.0886\n",
      "Epoch 274/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0848 - val_loss: 0.0887\n",
      "Epoch 275/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0843 - val_loss: 0.0889\n",
      "Epoch 276/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0838 - val_loss: 0.0890\n",
      "Epoch 277/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0833 - val_loss: 0.0863\n",
      "Epoch 278/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0828 - val_loss: 0.0859\n",
      "Epoch 279/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0824 - val_loss: 0.0853\n",
      "Epoch 280/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0819 - val_loss: 0.0846\n",
      "Epoch 281/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0814 - val_loss: 0.0842\n",
      "Epoch 282/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0811 - val_loss: 0.0844\n",
      "Epoch 283/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0807 - val_loss: 0.0843\n",
      "Epoch 284/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0803 - val_loss: 0.0839\n",
      "Epoch 285/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0799 - val_loss: 0.0836\n",
      "Epoch 286/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0795 - val_loss: 0.0829\n",
      "Epoch 287/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0791 - val_loss: 0.0827\n",
      "Epoch 288/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0787 - val_loss: 0.0825\n",
      "Epoch 289/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0783 - val_loss: 0.0822\n",
      "Epoch 290/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0779 - val_loss: 0.0817\n",
      "Epoch 291/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0774 - val_loss: 0.0821\n",
      "Epoch 292/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0771 - val_loss: 0.0802\n",
      "Epoch 293/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0766 - val_loss: 0.0801\n",
      "Epoch 294/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0763 - val_loss: 0.0797\n",
      "Epoch 295/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0760 - val_loss: 0.0794\n",
      "Epoch 296/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0756 - val_loss: 0.0791\n",
      "Epoch 297/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0753 - val_loss: 0.0782\n",
      "Epoch 298/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0749 - val_loss: 0.0778\n",
      "Epoch 299/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0746 - val_loss: 0.0774\n",
      "Epoch 300/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0743 - val_loss: 0.0770\n",
      "Epoch 301/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0739 - val_loss: 0.0766\n",
      "Epoch 302/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0735 - val_loss: 0.0762\n",
      "Epoch 303/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0730 - val_loss: 0.0757\n",
      "Epoch 304/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0726 - val_loss: 0.0750\n",
      "Epoch 305/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0721 - val_loss: 0.0745\n",
      "Epoch 306/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0716 - val_loss: 0.0740\n",
      "Epoch 307/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0712 - val_loss: 0.0734\n",
      "Epoch 308/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0708 - val_loss: 0.0727\n",
      "Epoch 309/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0706 - val_loss: 0.0724\n",
      "Epoch 310/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0702 - val_loss: 0.0719\n",
      "Epoch 311/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0699 - val_loss: 0.0715\n",
      "Epoch 312/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0696 - val_loss: 0.0712\n",
      "Epoch 313/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0692 - val_loss: 0.0719\n",
      "Epoch 314/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0690 - val_loss: 0.0715\n",
      "Epoch 315/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0686 - val_loss: 0.0710\n",
      "Epoch 316/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0683 - val_loss: 0.0704\n",
      "Epoch 317/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0680 - val_loss: 0.0703\n",
      "Epoch 318/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0677 - val_loss: 0.0702\n",
      "Epoch 319/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0674 - val_loss: 0.0698\n",
      "Epoch 320/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0670 - val_loss: 0.0695\n",
      "Epoch 321/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0667 - val_loss: 0.0688\n",
      "Epoch 322/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0664 - val_loss: 0.0693\n",
      "Epoch 323/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0662 - val_loss: 0.0693\n",
      "Epoch 324/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0660 - val_loss: 0.0689\n",
      "Epoch 325/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0658 - val_loss: 0.0686\n",
      "Epoch 326/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0656 - val_loss: 0.0691\n",
      "Epoch 327/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0654 - val_loss: 0.0696\n",
      "Epoch 328/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0652 - val_loss: 0.0693\n",
      "Epoch 329/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0650 - val_loss: 0.0685\n",
      "Epoch 330/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0647 - val_loss: 0.0682\n",
      "Epoch 331/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0644 - val_loss: 0.0689\n",
      "Epoch 332/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0641 - val_loss: 0.0688\n",
      "Epoch 333/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0639 - val_loss: 0.0672\n",
      "Epoch 334/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0636 - val_loss: 0.0672\n",
      "Epoch 335/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0633 - val_loss: 0.0674\n",
      "Epoch 336/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0630 - val_loss: 0.0666\n",
      "Epoch 337/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0627 - val_loss: 0.0668\n",
      "Epoch 338/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0624 - val_loss: 0.0657\n",
      "Epoch 339/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0621 - val_loss: 0.0659\n",
      "Epoch 340/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0618 - val_loss: 0.0656\n",
      "Epoch 341/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0615 - val_loss: 0.0659\n",
      "Epoch 342/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0612 - val_loss: 0.0651\n",
      "Epoch 343/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0608 - val_loss: 0.0645\n",
      "Epoch 344/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0605 - val_loss: 0.0660\n",
      "Epoch 345/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0602 - val_loss: 0.0640\n",
      "Epoch 346/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0598 - val_loss: 0.0637\n",
      "Epoch 347/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0595 - val_loss: 0.0635\n",
      "Epoch 348/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0593 - val_loss: 0.0639\n",
      "Epoch 349/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0591 - val_loss: 0.0638\n",
      "Epoch 350/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0589 - val_loss: 0.0636\n",
      "Epoch 351/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0587 - val_loss: 0.0635\n",
      "Epoch 352/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0584 - val_loss: 0.0632\n",
      "Epoch 353/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0581 - val_loss: 0.0629\n",
      "Epoch 354/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0578 - val_loss: 0.0628\n",
      "Epoch 355/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0575 - val_loss: 0.0620\n",
      "Epoch 356/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0573 - val_loss: 0.0618\n",
      "Epoch 357/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0571 - val_loss: 0.0619\n",
      "Epoch 358/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0570 - val_loss: 0.0613\n",
      "Epoch 359/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0569 - val_loss: 0.0615\n",
      "Epoch 360/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0566 - val_loss: 0.0615\n",
      "Epoch 361/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0565 - val_loss: 0.0619\n",
      "Epoch 362/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0563 - val_loss: 0.0614\n",
      "Epoch 363/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0561 - val_loss: 0.0616\n",
      "Epoch 364/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0558 - val_loss: 0.0613\n",
      "Epoch 365/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0556 - val_loss: 0.0610\n",
      "Epoch 366/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0553 - val_loss: 0.0605\n",
      "Epoch 367/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0551 - val_loss: 0.0597\n",
      "Epoch 368/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0548 - val_loss: 0.0593\n",
      "Epoch 369/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0547 - val_loss: 0.0594\n",
      "Epoch 370/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0546 - val_loss: 0.0585\n",
      "Epoch 371/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0544 - val_loss: 0.0581\n",
      "Epoch 372/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0543 - val_loss: 0.0579\n",
      "Epoch 373/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0541 - val_loss: 0.0578\n",
      "Epoch 374/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0539 - val_loss: 0.0584\n",
      "Epoch 375/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0538 - val_loss: 0.0578\n",
      "Epoch 376/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0537 - val_loss: 0.0576\n",
      "Epoch 377/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0535 - val_loss: 0.0572\n",
      "Epoch 378/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0534 - val_loss: 0.0572\n",
      "Epoch 379/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0532 - val_loss: 0.0571\n",
      "Epoch 380/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0530 - val_loss: 0.0570\n",
      "Epoch 381/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0529 - val_loss: 0.0568\n",
      "Epoch 382/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0527 - val_loss: 0.0565\n",
      "Epoch 383/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0524 - val_loss: 0.0563\n",
      "Epoch 384/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0522 - val_loss: 0.0578\n",
      "Epoch 385/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0519 - val_loss: 0.0562\n",
      "Epoch 386/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0518 - val_loss: 0.0548\n",
      "Epoch 387/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0516 - val_loss: 0.0543\n",
      "Epoch 388/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0515 - val_loss: 0.0543\n",
      "Epoch 389/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0514 - val_loss: 0.0542\n",
      "Epoch 390/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0512 - val_loss: 0.0541\n",
      "Epoch 391/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0511 - val_loss: 0.0547\n",
      "Epoch 392/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0509 - val_loss: 0.0546\n",
      "Epoch 393/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0508 - val_loss: 0.0545\n",
      "Epoch 394/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0506 - val_loss: 0.0547\n",
      "Epoch 395/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0504 - val_loss: 0.0549\n",
      "Epoch 396/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0503 - val_loss: 0.0544\n",
      "Epoch 397/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0501 - val_loss: 0.0550\n",
      "Epoch 398/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0500 - val_loss: 0.0545\n",
      "Epoch 399/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0499 - val_loss: 0.0542\n",
      "\n",
      "Epoch 00399: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-07.\n",
      "Epoch 400/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0497 - val_loss: 0.0541\n",
      "Epoch 401/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0497 - val_loss: 0.0534\n",
      "Epoch 402/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0496 - val_loss: 0.0535\n",
      "Epoch 403/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0496 - val_loss: 0.0538\n",
      "Epoch 404/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0495 - val_loss: 0.0537\n",
      "Epoch 405/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0494 - val_loss: 0.0537\n",
      "Epoch 406/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0493 - val_loss: 0.0536\n",
      "Epoch 407/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0492 - val_loss: 0.0527\n",
      "Epoch 408/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0491 - val_loss: 0.0526\n",
      "Epoch 409/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0491 - val_loss: 0.0524\n",
      "Epoch 410/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0490 - val_loss: 0.0523\n",
      "Epoch 411/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0489 - val_loss: 0.0525\n",
      "Epoch 412/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0488 - val_loss: 0.0524\n",
      "Epoch 413/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0486 - val_loss: 0.0525\n",
      "Epoch 414/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0485 - val_loss: 0.0524\n",
      "Epoch 415/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0483 - val_loss: 0.0524\n",
      "Epoch 416/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0482 - val_loss: 0.0531\n",
      "Epoch 417/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0480 - val_loss: 0.0515\n",
      "Epoch 418/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0479 - val_loss: 0.0509\n",
      "Epoch 419/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0478 - val_loss: 0.0508\n",
      "Epoch 420/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0477 - val_loss: 0.0506\n",
      "Epoch 421/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0476 - val_loss: 0.0504\n",
      "Epoch 422/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0475 - val_loss: 0.0503\n",
      "Epoch 423/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0474 - val_loss: 0.0502\n",
      "Epoch 424/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0473 - val_loss: 0.0501\n",
      "Epoch 425/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0471 - val_loss: 0.0499\n",
      "Epoch 426/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0470 - val_loss: 0.0497\n",
      "Epoch 427/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0468 - val_loss: 0.0497\n",
      "Epoch 428/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0467 - val_loss: 0.0492\n",
      "Epoch 429/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0466 - val_loss: 0.0492\n",
      "Epoch 430/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0464 - val_loss: 0.0492\n",
      "Epoch 431/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0463 - val_loss: 0.0491\n",
      "Epoch 432/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0462 - val_loss: 0.0490\n",
      "Epoch 433/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0460 - val_loss: 0.0491\n",
      "Epoch 434/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0459 - val_loss: 0.0489\n",
      "Epoch 435/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0458 - val_loss: 0.0490\n",
      "Epoch 436/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0457 - val_loss: 0.0490\n",
      "Epoch 437/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0456 - val_loss: 0.0489\n",
      "Epoch 438/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0455 - val_loss: 0.0488\n",
      "Epoch 439/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0454 - val_loss: 0.0486\n",
      "Epoch 440/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0452 - val_loss: 0.0485\n",
      "Epoch 441/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0450 - val_loss: 0.0483\n",
      "Epoch 442/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0449 - val_loss: 0.0481\n",
      "Epoch 443/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0446 - val_loss: 0.0480\n",
      "Epoch 444/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0444 - val_loss: 0.0480\n",
      "Epoch 445/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0443 - val_loss: 0.0479\n",
      "Epoch 446/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0441 - val_loss: 0.0477\n",
      "Epoch 447/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0440 - val_loss: 0.0480\n",
      "Epoch 448/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0439 - val_loss: 0.0465\n",
      "Epoch 449/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0438 - val_loss: 0.0464\n",
      "Epoch 450/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0437 - val_loss: 0.0465\n",
      "Epoch 451/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0437 - val_loss: 0.0463\n",
      "Epoch 452/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0436 - val_loss: 0.0460\n",
      "Epoch 453/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0436 - val_loss: 0.0461\n",
      "Epoch 454/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0435 - val_loss: 0.0462\n",
      "Epoch 455/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0435 - val_loss: 0.0466\n",
      "Epoch 456/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0434 - val_loss: 0.0465\n",
      "Epoch 457/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0433 - val_loss: 0.0465\n",
      "Epoch 458/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0433 - val_loss: 0.0464\n",
      "Epoch 459/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0432 - val_loss: 0.0463\n",
      "Epoch 460/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0431 - val_loss: 0.0462\n",
      "Epoch 461/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0430 - val_loss: 0.0458\n",
      "Epoch 462/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0429 - val_loss: 0.0457\n",
      "Epoch 463/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0428 - val_loss: 0.0456\n",
      "Epoch 464/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0428 - val_loss: 0.0455\n",
      "Epoch 465/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0427 - val_loss: 0.0454\n",
      "Epoch 466/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0426 - val_loss: 0.0454\n",
      "Epoch 467/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0426 - val_loss: 0.0454\n",
      "Epoch 468/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0425 - val_loss: 0.0451\n",
      "Epoch 469/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0425 - val_loss: 0.0453\n",
      "Epoch 470/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0424 - val_loss: 0.0452\n",
      "Epoch 471/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0424 - val_loss: 0.0451\n",
      "Epoch 472/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0423 - val_loss: 0.0450\n",
      "Epoch 473/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0422 - val_loss: 0.0445\n",
      "Epoch 474/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0421 - val_loss: 0.0445\n",
      "Epoch 475/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0420 - val_loss: 0.0444\n",
      "Epoch 476/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0419 - val_loss: 0.0444\n",
      "Epoch 477/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0418 - val_loss: 0.0443\n",
      "Epoch 478/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0417 - val_loss: 0.0444\n",
      "Epoch 479/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0416 - val_loss: 0.0447\n",
      "Epoch 480/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0416 - val_loss: 0.0447\n",
      "Epoch 481/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0415 - val_loss: 0.0447\n",
      "Epoch 482/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0415 - val_loss: 0.0447\n",
      "Epoch 483/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0414 - val_loss: 0.0447\n",
      "Epoch 484/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0414 - val_loss: 0.0449\n",
      "Epoch 485/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0413 - val_loss: 0.0448\n",
      "Epoch 486/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0413 - val_loss: 0.0443\n",
      "Epoch 487/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0412 - val_loss: 0.0443\n",
      "\n",
      "Epoch 00487: ReduceLROnPlateau reducing learning rate to 2.499999993688107e-07.\n",
      "Epoch 488/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0412 - val_loss: 0.0440\n",
      "Epoch 489/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0411 - val_loss: 0.0442\n",
      "Epoch 490/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0411 - val_loss: 0.0441\n",
      "Epoch 491/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0411 - val_loss: 0.0442\n",
      "Epoch 492/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0410 - val_loss: 0.0442\n",
      "Epoch 493/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0410 - val_loss: 0.0442\n",
      "Epoch 494/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0410 - val_loss: 0.0442\n",
      "Epoch 495/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0409 - val_loss: 0.0441\n",
      "Epoch 496/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0409 - val_loss: 0.0444\n",
      "Epoch 497/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0408 - val_loss: 0.0444\n",
      "Epoch 498/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0408 - val_loss: 0.0444\n",
      "\n",
      "Epoch 00498: ReduceLROnPlateau reducing learning rate to 1.2499999968440534e-07.\n",
      "Epoch 499/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0443\n",
      "Epoch 500/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0442\n",
      "Epoch 501/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0441\n",
      "Epoch 502/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0440\n",
      "Epoch 503/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0406 - val_loss: 0.0440\n",
      "Epoch 504/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0406 - val_loss: 0.0440\n",
      "Epoch 505/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0406 - val_loss: 0.0439\n",
      "Epoch 506/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0405 - val_loss: 0.0439\n",
      "Epoch 507/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0405 - val_loss: 0.0438\n",
      "Epoch 508/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0404 - val_loss: 0.0438\n",
      "Epoch 509/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0404 - val_loss: 0.0437\n",
      "Epoch 510/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0403 - val_loss: 0.0431\n",
      "Epoch 511/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0403 - val_loss: 0.0430\n",
      "Epoch 512/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0402 - val_loss: 0.0430\n",
      "Epoch 513/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0402 - val_loss: 0.0430\n",
      "Epoch 514/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0402 - val_loss: 0.0429\n",
      "Epoch 515/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0401 - val_loss: 0.0429\n",
      "Epoch 516/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0401 - val_loss: 0.0429\n",
      "Epoch 517/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0400 - val_loss: 0.0429\n",
      "Epoch 518/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0400 - val_loss: 0.0430\n",
      "Epoch 519/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0400 - val_loss: 0.0430\n",
      "Epoch 520/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0430\n",
      "Epoch 521/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0430\n",
      "Epoch 522/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0432\n",
      "Epoch 523/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0431\n",
      "\n",
      "Epoch 00523: ReduceLROnPlateau reducing learning rate to 6.249999984220267e-08.\n",
      "Epoch 524/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 525/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 526/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 527/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 528/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 529/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 530/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 531/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "Epoch 532/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "Epoch 533/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00533: ReduceLROnPlateau reducing learning rate to 3.1249999921101335e-08.\n",
      "Epoch 534/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "Epoch 535/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0429\n",
      "Epoch 00535: early stopping\n"
     ]
    }
   ],
   "source": [
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu \n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_backup.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "history = triplet_model.fit(train_generator_luscinia(luscinia_triplets[:int(luscinia_train_len/10)], M_l, S_l, batchsize, emb_size, path_mel),\n",
    "                           steps_per_epoch=int(int(luscinia_train_len/10)/batchsize), epochs=1000, verbose=1,\n",
    "                           validation_data=train_generator_luscinia(luscinia_triplets[luscinia_train_len:luscinia_train_len+200], M_l, S_l, batchsize, emb_size, path_mel),\n",
    "                           validation_steps=int(200/batchsize), callbacks=[cpCallback, reduce_lr, earlystop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE trained (training on bird decisions after pretraining on Luscinia triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "triplet_model.load_weights('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_backup.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "58/58 [==============================] - 28s 394ms/step - loss: 0.1178 - val_loss: 0.1106\n",
      "Epoch 2/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1201 - val_loss: 0.1079\n",
      "Epoch 3/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1196 - val_loss: 0.1073\n",
      "Epoch 4/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1166 - val_loss: 0.1070\n",
      "Epoch 5/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1183 - val_loss: 0.1068\n",
      "Epoch 6/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1158 - val_loss: 0.1066\n",
      "Epoch 7/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1166 - val_loss: 0.1064\n",
      "Epoch 8/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1157 - val_loss: 0.1062\n",
      "Epoch 9/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1146 - val_loss: 0.1060\n",
      "Epoch 10/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1154 - val_loss: 0.1058\n",
      "Epoch 11/1000\n",
      "58/58 [==============================] - 22s 382ms/step - loss: 0.1144 - val_loss: 0.1056\n",
      "Epoch 12/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1155 - val_loss: 0.1054\n",
      "Epoch 13/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1149 - val_loss: 0.1053\n",
      "Epoch 14/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1144 - val_loss: 0.1051\n",
      "Epoch 15/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1145 - val_loss: 0.1049\n",
      "Epoch 16/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1145 - val_loss: 0.1048\n",
      "Epoch 17/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1156 - val_loss: 0.1046\n",
      "Epoch 18/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1148 - val_loss: 0.1044\n",
      "Epoch 19/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1123 - val_loss: 0.1043\n",
      "Epoch 20/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1138 - val_loss: 0.1041\n",
      "Epoch 21/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1109 - val_loss: 0.1040\n",
      "Epoch 22/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1112 - val_loss: 0.1038\n",
      "Epoch 23/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1100 - val_loss: 0.1037\n",
      "Epoch 24/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1109 - val_loss: 0.1036\n",
      "Epoch 25/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1115 - val_loss: 0.1034\n",
      "Epoch 26/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1109 - val_loss: 0.1033\n",
      "Epoch 27/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1100 - val_loss: 0.1032\n",
      "Epoch 28/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1115 - val_loss: 0.1030\n",
      "Epoch 29/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1100 - val_loss: 0.1029\n",
      "Epoch 30/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1113 - val_loss: 0.1028\n",
      "Epoch 31/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1081 - val_loss: 0.1026\n",
      "Epoch 32/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1111 - val_loss: 0.1025\n",
      "Epoch 33/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1086 - val_loss: 0.1024\n",
      "Epoch 34/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1096 - val_loss: 0.1023\n",
      "Epoch 35/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1096 - val_loss: 0.1022\n",
      "Epoch 36/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1074 - val_loss: 0.1021\n",
      "Epoch 37/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1076 - val_loss: 0.1020\n",
      "Epoch 38/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1099 - val_loss: 0.1019\n",
      "Epoch 39/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1074 - val_loss: 0.1018\n",
      "Epoch 40/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1095 - val_loss: 0.1017\n",
      "Epoch 41/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1076 - val_loss: 0.1016\n",
      "Epoch 42/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1067 - val_loss: 0.1015\n",
      "Epoch 43/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1087 - val_loss: 0.1014\n",
      "Epoch 44/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1061 - val_loss: 0.1013\n",
      "Epoch 45/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1074 - val_loss: 0.1013\n",
      "Epoch 46/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1080 - val_loss: 0.1012\n",
      "Epoch 47/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1068 - val_loss: 0.1011\n",
      "Epoch 48/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1071 - val_loss: 0.1010\n",
      "Epoch 49/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1058 - val_loss: 0.1009\n",
      "Epoch 50/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1061 - val_loss: 0.1008\n",
      "Epoch 51/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1072 - val_loss: 0.1008\n",
      "Epoch 52/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1070 - val_loss: 0.1007\n",
      "Epoch 53/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1077 - val_loss: 0.1006\n",
      "Epoch 54/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1065 - val_loss: 0.1005\n",
      "Epoch 55/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1061 - val_loss: 0.1005\n",
      "Epoch 56/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1070 - val_loss: 0.1004\n",
      "Epoch 57/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1057 - val_loss: 0.1003\n",
      "Epoch 58/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1057 - val_loss: 0.1003\n",
      "Epoch 59/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1042 - val_loss: 0.1002\n",
      "Epoch 60/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1059 - val_loss: 0.1001\n",
      "Epoch 61/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1049 - val_loss: 0.1001\n",
      "Epoch 62/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1052 - val_loss: 0.1000\n",
      "Epoch 63/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1046 - val_loss: 0.1000\n",
      "Epoch 64/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1035 - val_loss: 0.0999\n",
      "Epoch 65/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1027 - val_loss: 0.0998\n",
      "Epoch 66/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1037 - val_loss: 0.0998\n",
      "Epoch 67/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1054 - val_loss: 0.0997\n",
      "Epoch 68/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0997\n",
      "Epoch 69/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1045 - val_loss: 0.0996\n",
      "Epoch 70/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1042 - val_loss: 0.0996\n",
      "Epoch 71/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1043 - val_loss: 0.0995\n",
      "Epoch 72/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.1029 - val_loss: 0.0995\n",
      "Epoch 73/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1036 - val_loss: 0.0994\n",
      "Epoch 74/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1025 - val_loss: 0.0994\n",
      "Epoch 75/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1024 - val_loss: 0.0994\n",
      "Epoch 76/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1038 - val_loss: 0.0993\n",
      "Epoch 77/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0993\n",
      "Epoch 78/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1022 - val_loss: 0.0992\n",
      "Epoch 79/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1040 - val_loss: 0.0992\n",
      "Epoch 80/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1027 - val_loss: 0.0991\n",
      "Epoch 81/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1037 - val_loss: 0.0991\n",
      "Epoch 82/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1027 - val_loss: 0.0991\n",
      "Epoch 83/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0990\n",
      "Epoch 84/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1030 - val_loss: 0.0990\n",
      "Epoch 85/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1025 - val_loss: 0.0990\n",
      "Epoch 86/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1031 - val_loss: 0.0989\n",
      "Epoch 87/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1026 - val_loss: 0.0989\n",
      "Epoch 88/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1033 - val_loss: 0.0989\n",
      "Epoch 89/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1024 - val_loss: 0.0988\n",
      "Epoch 90/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1021 - val_loss: 0.0988\n",
      "Epoch 91/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1021 - val_loss: 0.0988\n",
      "Epoch 92/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0987\n",
      "Epoch 93/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1022 - val_loss: 0.0987\n",
      "Epoch 94/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1011 - val_loss: 0.0987\n",
      "Epoch 95/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1014 - val_loss: 0.0987\n",
      "Epoch 96/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0986\n",
      "Epoch 97/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1013 - val_loss: 0.0986\n",
      "Epoch 98/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1015 - val_loss: 0.0986\n",
      "Epoch 99/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1007 - val_loss: 0.0986\n",
      "Epoch 100/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1017 - val_loss: 0.0985\n",
      "Epoch 101/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1016 - val_loss: 0.0985\n",
      "Epoch 102/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1021 - val_loss: 0.0985\n",
      "Epoch 103/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1010 - val_loss: 0.0985\n",
      "Epoch 104/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1006 - val_loss: 0.0984\n",
      "Epoch 105/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0984\n",
      "Epoch 106/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1014 - val_loss: 0.0984\n",
      "Epoch 107/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1006 - val_loss: 0.0984\n",
      "Epoch 108/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1009 - val_loss: 0.0983\n",
      "Epoch 109/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1006 - val_loss: 0.0983\n",
      "Epoch 110/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1008 - val_loss: 0.0983\n",
      "Epoch 111/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0997 - val_loss: 0.0983\n",
      "Epoch 112/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0998 - val_loss: 0.0983\n",
      "Epoch 113/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1008 - val_loss: 0.0983\n",
      "Epoch 114/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0982\n",
      "Epoch 115/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1017 - val_loss: 0.0982\n",
      "Epoch 116/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1003 - val_loss: 0.0982\n",
      "Epoch 117/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1007 - val_loss: 0.0982\n",
      "Epoch 118/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1003 - val_loss: 0.0981\n",
      "Epoch 119/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1004 - val_loss: 0.0981\n",
      "Epoch 120/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0989 - val_loss: 0.0981\n",
      "Epoch 121/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0983 - val_loss: 0.0981\n",
      "Epoch 122/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1003 - val_loss: 0.0981\n",
      "Epoch 123/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1001 - val_loss: 0.0981\n",
      "Epoch 124/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0995 - val_loss: 0.0981\n",
      "Epoch 125/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1001 - val_loss: 0.0980\n",
      "Epoch 126/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0980\n",
      "Epoch 127/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0980 - val_loss: 0.0980\n",
      "Epoch 128/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0980\n",
      "Epoch 129/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1013 - val_loss: 0.0980\n",
      "Epoch 130/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0980\n",
      "Epoch 131/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0979\n",
      "Epoch 132/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0995 - val_loss: 0.0979\n",
      "Epoch 133/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.1007 - val_loss: 0.0979\n",
      "Epoch 134/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0996 - val_loss: 0.0979\n",
      "Epoch 135/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0979\n",
      "Epoch 136/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0979\n",
      "Epoch 137/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0986 - val_loss: 0.0979\n",
      "Epoch 138/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1003 - val_loss: 0.0979\n",
      "Epoch 139/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0996 - val_loss: 0.0979\n",
      "Epoch 140/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0978\n",
      "Epoch 141/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0978\n",
      "Epoch 142/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0987 - val_loss: 0.0978\n",
      "Epoch 143/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0992 - val_loss: 0.0978\n",
      "Epoch 144/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0978\n",
      "Epoch 145/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 146/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1002 - val_loss: 0.0978\n",
      "Epoch 147/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 148/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0983 - val_loss: 0.0978\n",
      "Epoch 149/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1006 - val_loss: 0.0978\n",
      "Epoch 150/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0977\n",
      "Epoch 151/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1002 - val_loss: 0.0977\n",
      "Epoch 152/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0988 - val_loss: 0.0977\n",
      "Epoch 153/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1004 - val_loss: 0.0977\n",
      "Epoch 154/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0977\n",
      "Epoch 155/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0977\n",
      "Epoch 156/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0992 - val_loss: 0.0977\n",
      "Epoch 157/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0977\n",
      "Epoch 158/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 159/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 4.999999969612645e-09.\n",
      "Epoch 160/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0988 - val_loss: 0.0977\n",
      "Epoch 161/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 162/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0997 - val_loss: 0.0977\n",
      "Epoch 163/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0977\n",
      "Epoch 164/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0977\n",
      "Epoch 165/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0995 - val_loss: 0.0977\n",
      "Epoch 166/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0977\n",
      "Epoch 167/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0977\n",
      "Epoch 168/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0978 - val_loss: 0.0977\n",
      "Epoch 169/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0990 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 2.4999999848063226e-09.\n",
      "Epoch 170/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0977\n",
      "Epoch 171/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0977\n",
      "Epoch 172/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 173/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0977\n",
      "Epoch 174/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0977\n",
      "Epoch 175/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0977\n",
      "Epoch 176/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 177/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0995 - val_loss: 0.0976\n",
      "Epoch 178/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0977 - val_loss: 0.0976\n",
      "Epoch 179/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0975 - val_loss: 0.0976\n",
      "Epoch 180/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0981 - val_loss: 0.0976\n",
      "Epoch 181/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0998 - val_loss: 0.0976\n",
      "Epoch 182/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 183/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0976\n",
      "Epoch 184/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 185/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0995 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 1.2499999924031613e-09.\n",
      "Epoch 186/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0994 - val_loss: 0.0976\n",
      "Epoch 187/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0969 - val_loss: 0.0976\n",
      "Epoch 188/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 189/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 190/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0985 - val_loss: 0.0976\n",
      "Epoch 191/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 192/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 193/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 194/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.0975 - val_loss: 0.0976\n",
      "Epoch 195/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0975 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 6.249999962015806e-10.\n",
      "Epoch 196/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1002 - val_loss: 0.0976\n",
      "Epoch 197/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 198/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 199/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 200/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 201/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 202/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0971 - val_loss: 0.0976\n",
      "Epoch 203/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0976\n",
      "Epoch 204/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 205/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 3.124999981007903e-10.\n",
      "Epoch 206/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0972 - val_loss: 0.0976\n",
      "Epoch 207/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 208/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 209/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 210/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0974 - val_loss: 0.0976\n",
      "Epoch 211/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 212/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 213/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 214/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0971 - val_loss: 0.0976\n",
      "Epoch 215/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0994 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 1.5624999905039516e-10.\n",
      "Epoch 216/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 217/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 218/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1000 - val_loss: 0.0976\n",
      "Epoch 219/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 220/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0978 - val_loss: 0.0976\n",
      "Epoch 221/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 222/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 223/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0976 - val_loss: 0.0976\n",
      "Epoch 224/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0997 - val_loss: 0.0976\n",
      "Epoch 225/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0981 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 7.812499952519758e-11.\n",
      "Epoch 226/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 227/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 228/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 229/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 230/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 231/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 232/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 233/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0974 - val_loss: 0.0976\n",
      "Epoch 234/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0994 - val_loss: 0.0976\n",
      "Epoch 235/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 3.906249976259879e-11.\n",
      "Epoch 236/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 237/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0974 - val_loss: 0.0976\n",
      "Epoch 238/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0977 - val_loss: 0.0976\n",
      "Epoch 239/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 240/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 241/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 242/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 243/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 244/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 245/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 1.9531249881299395e-11.\n",
      "Epoch 246/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 247/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0998 - val_loss: 0.0976\n",
      "Epoch 248/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 249/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 250/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 251/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 252/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 253/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0985 - val_loss: 0.0976\n",
      "Epoch 254/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 255/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00255: ReduceLROnPlateau reducing learning rate to 9.765624940649698e-12.\n",
      "Epoch 256/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 257/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 258/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 259/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 260/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 261/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 262/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0976\n",
      "Epoch 263/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 264/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 265/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0985 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 4.882812470324849e-12.\n",
      "Epoch 266/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0990 - val_loss: 0.0976\n",
      "Epoch 267/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 268/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 269/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 270/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 271/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0977 - val_loss: 0.0976\n",
      "Epoch 272/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 273/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0972 - val_loss: 0.0976\n",
      "Epoch 00273: early stopping\n"
     ]
    }
   ],
   "source": [
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu \n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_trained.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "dis_tr_triplets = discard_some_low(training_triplets, 0.7, 0.7)\n",
    "dis_val_triplets = discard_some_low(validation_triplets, 0.7, 0.7)\n",
    "\n",
    "history = triplet_model.fit(train_generator(dis_tr_triplets, M, S, batchsize, emb_size, path_mel),\n",
    "                           steps_per_epoch=int(len(dis_tr_triplets)/batchsize), epochs=1000, verbose=1,\n",
    "                           validation_data=train_generator(dis_val_triplets, M,S, batchsize, emb_size, path_mel),\n",
    "                           validation_steps=int(len(dis_val_triplets)/batchsize), callbacks=[cpCallback, reduce_lr,earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIXED (training on both bird decisions and Luscinia triplets - w/o pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "117/117 [==============================] - 50s 380ms/step - loss: 0.6301 - val_loss: 0.5736\n",
      "Epoch 2/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.6108 - val_loss: 0.5940\n",
      "Epoch 3/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.6125 - val_loss: 0.5691\n",
      "Epoch 4/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5836 - val_loss: 0.5496\n",
      "Epoch 5/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5807 - val_loss: 0.5335\n",
      "Epoch 6/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5562 - val_loss: 0.5218\n",
      "Epoch 7/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5476 - val_loss: 0.5129\n",
      "Epoch 8/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5425 - val_loss: 0.5063\n",
      "Epoch 9/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.5374 - val_loss: 0.5008\n",
      "Epoch 10/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5245 - val_loss: 0.4966\n",
      "Epoch 11/1000\n",
      "117/117 [==============================] - 44s 375ms/step - loss: 0.5251 - val_loss: 0.4928\n",
      "Epoch 12/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5145 - val_loss: 0.4898\n",
      "Epoch 13/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5166 - val_loss: 0.4878\n",
      "Epoch 14/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5131 - val_loss: 0.4860\n",
      "Epoch 15/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5089 - val_loss: 0.4847\n",
      "Epoch 16/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5063 - val_loss: 0.4836\n",
      "Epoch 17/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5037 - val_loss: 0.4827\n",
      "Epoch 18/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4987 - val_loss: 0.4817\n",
      "Epoch 19/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5007 - val_loss: 0.4808\n",
      "Epoch 20/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4962 - val_loss: 0.4801\n",
      "Epoch 21/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4969 - val_loss: 0.4796\n",
      "Epoch 22/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4932 - val_loss: 0.4791\n",
      "Epoch 23/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4944 - val_loss: 0.4787\n",
      "Epoch 24/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4937 - val_loss: 0.4782\n",
      "Epoch 25/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4924 - val_loss: 0.4778\n",
      "Epoch 26/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4897 - val_loss: 0.4774\n",
      "Epoch 27/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4895 - val_loss: 0.4769\n",
      "Epoch 28/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4886 - val_loss: 0.4766\n",
      "Epoch 29/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4868 - val_loss: 0.4761\n",
      "Epoch 30/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4866 - val_loss: 0.4757\n",
      "Epoch 31/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4851 - val_loss: 0.4755\n",
      "Epoch 32/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4878 - val_loss: 0.4754\n",
      "Epoch 33/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4852 - val_loss: 0.4752\n",
      "Epoch 34/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4839 - val_loss: 0.4750\n",
      "Epoch 35/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4822 - val_loss: 0.4748\n",
      "Epoch 36/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4832 - val_loss: 0.4746\n",
      "Epoch 37/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4821 - val_loss: 0.4744\n",
      "Epoch 38/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4808 - val_loss: 0.4742\n",
      "Epoch 39/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4823 - val_loss: 0.4741\n",
      "Epoch 40/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4813 - val_loss: 0.4740\n",
      "Epoch 41/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4791 - val_loss: 0.4738\n",
      "Epoch 42/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4792 - val_loss: 0.4737\n",
      "Epoch 43/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4818 - val_loss: 0.4735\n",
      "Epoch 44/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4766 - val_loss: 0.4732\n",
      "Epoch 45/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4789 - val_loss: 0.4728\n",
      "Epoch 46/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4783 - val_loss: 0.4726\n",
      "Epoch 47/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4794 - val_loss: 0.4725\n",
      "Epoch 48/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4788 - val_loss: 0.4724\n",
      "Epoch 49/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4787 - val_loss: 0.4722\n",
      "Epoch 50/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4766 - val_loss: 0.4721\n",
      "Epoch 51/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4764 - val_loss: 0.4719\n",
      "Epoch 52/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4740 - val_loss: 0.4717\n",
      "Epoch 53/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4773 - val_loss: 0.4715\n",
      "Epoch 54/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4736 - val_loss: 0.4714\n",
      "Epoch 55/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4767 - val_loss: 0.4712\n",
      "Epoch 56/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4729 - val_loss: 0.4711\n",
      "Epoch 57/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4750 - val_loss: 0.4709\n",
      "Epoch 58/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4749 - val_loss: 0.4707\n",
      "Epoch 59/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4733 - val_loss: 0.4705\n",
      "Epoch 60/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4733 - val_loss: 0.4704\n",
      "Epoch 61/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4737 - val_loss: 0.4703\n",
      "Epoch 62/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4711 - val_loss: 0.4701\n",
      "Epoch 63/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4718 - val_loss: 0.4699\n",
      "Epoch 64/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4717 - val_loss: 0.4698\n",
      "Epoch 65/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4714 - val_loss: 0.4697\n",
      "Epoch 66/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4717 - val_loss: 0.4696\n",
      "Epoch 67/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4712 - val_loss: 0.4695\n",
      "Epoch 68/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4688 - val_loss: 0.4694\n",
      "Epoch 69/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4709 - val_loss: 0.4693\n",
      "Epoch 70/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4701 - val_loss: 0.4691\n",
      "Epoch 71/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4702 - val_loss: 0.4690\n",
      "Epoch 72/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4687 - val_loss: 0.4688\n",
      "Epoch 73/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4680 - val_loss: 0.4687\n",
      "Epoch 74/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4708 - val_loss: 0.4686\n",
      "Epoch 75/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4677 - val_loss: 0.4685\n",
      "Epoch 76/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4686 - val_loss: 0.4683\n",
      "Epoch 77/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4686 - val_loss: 0.4681\n",
      "Epoch 78/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4668 - val_loss: 0.4680\n",
      "Epoch 79/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4689 - val_loss: 0.4679\n",
      "Epoch 80/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4677 - val_loss: 0.4678\n",
      "Epoch 81/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4670 - val_loss: 0.4677\n",
      "Epoch 82/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4662 - val_loss: 0.4675\n",
      "Epoch 83/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4687 - val_loss: 0.4674\n",
      "Epoch 84/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4686 - val_loss: 0.4673\n",
      "Epoch 85/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4672 - val_loss: 0.4672\n",
      "Epoch 86/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4676 - val_loss: 0.4670\n",
      "Epoch 87/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4653 - val_loss: 0.4669\n",
      "Epoch 88/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4665 - val_loss: 0.4668\n",
      "Epoch 89/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4655 - val_loss: 0.4668\n",
      "Epoch 90/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4644 - val_loss: 0.4667\n",
      "Epoch 91/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4660 - val_loss: 0.4665\n",
      "Epoch 92/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4638 - val_loss: 0.4665\n",
      "Epoch 93/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4642 - val_loss: 0.4664\n",
      "Epoch 94/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4650 - val_loss: 0.4663\n",
      "Epoch 95/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4649 - val_loss: 0.4663\n",
      "Epoch 96/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4621 - val_loss: 0.4662\n",
      "Epoch 97/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4648 - val_loss: 0.4661\n",
      "Epoch 98/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4645 - val_loss: 0.4660\n",
      "Epoch 99/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4650 - val_loss: 0.4659\n",
      "Epoch 100/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4636 - val_loss: 0.4658\n",
      "Epoch 101/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4631 - val_loss: 0.4657\n",
      "Epoch 102/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4628 - val_loss: 0.4656\n",
      "Epoch 103/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4617 - val_loss: 0.4655\n",
      "Epoch 104/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4629 - val_loss: 0.4654\n",
      "Epoch 105/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4626 - val_loss: 0.4653\n",
      "Epoch 106/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4636 - val_loss: 0.4652\n",
      "Epoch 107/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4617 - val_loss: 0.4651\n",
      "Epoch 108/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4626 - val_loss: 0.4650\n",
      "Epoch 109/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4619 - val_loss: 0.4649\n",
      "Epoch 110/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4626 - val_loss: 0.4648\n",
      "Epoch 111/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4623 - val_loss: 0.4647\n",
      "Epoch 112/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4624 - val_loss: 0.4646\n",
      "Epoch 113/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4619 - val_loss: 0.4645\n",
      "Epoch 114/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4601 - val_loss: 0.4644\n",
      "Epoch 115/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4605 - val_loss: 0.4644\n",
      "Epoch 116/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4607 - val_loss: 0.4643\n",
      "Epoch 117/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4604 - val_loss: 0.4642\n",
      "Epoch 118/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4612 - val_loss: 0.4642\n",
      "Epoch 119/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4608 - val_loss: 0.4641\n",
      "Epoch 120/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4598 - val_loss: 0.4640\n",
      "Epoch 121/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4595 - val_loss: 0.4638\n",
      "Epoch 122/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4596 - val_loss: 0.4638\n",
      "Epoch 123/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4604 - val_loss: 0.4637\n",
      "Epoch 124/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4600 - val_loss: 0.4636\n",
      "Epoch 125/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4590 - val_loss: 0.4635\n",
      "Epoch 126/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4588 - val_loss: 0.4634\n",
      "Epoch 127/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4593 - val_loss: 0.4634\n",
      "Epoch 128/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4595 - val_loss: 0.4633\n",
      "Epoch 129/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4584 - val_loss: 0.4632\n",
      "Epoch 130/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4592 - val_loss: 0.4631\n",
      "Epoch 131/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4591 - val_loss: 0.4631\n",
      "Epoch 132/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4580 - val_loss: 0.4630\n",
      "Epoch 133/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4585 - val_loss: 0.4629\n",
      "Epoch 134/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4585 - val_loss: 0.4628\n",
      "Epoch 135/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4584 - val_loss: 0.4628\n",
      "Epoch 136/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4580 - val_loss: 0.4627\n",
      "Epoch 137/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4571 - val_loss: 0.4627\n",
      "Epoch 138/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4576 - val_loss: 0.4626\n",
      "Epoch 139/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4586 - val_loss: 0.4626\n",
      "Epoch 140/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4592 - val_loss: 0.4625\n",
      "Epoch 141/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4582 - val_loss: 0.4624\n",
      "Epoch 142/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4576 - val_loss: 0.4624\n",
      "Epoch 143/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4565 - val_loss: 0.4624\n",
      "Epoch 144/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4577 - val_loss: 0.4623\n",
      "Epoch 145/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4570 - val_loss: 0.4622\n",
      "Epoch 146/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4568 - val_loss: 0.4622\n",
      "Epoch 147/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4578 - val_loss: 0.4621\n",
      "Epoch 148/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4584 - val_loss: 0.4620\n",
      "Epoch 149/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4569 - val_loss: 0.4620\n",
      "Epoch 150/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4561 - val_loss: 0.4620\n",
      "Epoch 151/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4574 - val_loss: 0.4619\n",
      "Epoch 152/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4566 - val_loss: 0.4618\n",
      "Epoch 153/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4565 - val_loss: 0.4617\n",
      "Epoch 154/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4572 - val_loss: 0.4617\n",
      "Epoch 155/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4562 - val_loss: 0.4617\n",
      "Epoch 156/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4561 - val_loss: 0.4616\n",
      "Epoch 157/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4567 - val_loss: 0.4616\n",
      "Epoch 158/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4555 - val_loss: 0.4615\n",
      "Epoch 159/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4556 - val_loss: 0.4615\n",
      "Epoch 160/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4550 - val_loss: 0.4614\n",
      "Epoch 161/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4560 - val_loss: 0.4613\n",
      "Epoch 162/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4613\n",
      "Epoch 163/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4563 - val_loss: 0.4612\n",
      "Epoch 164/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4560 - val_loss: 0.4612\n",
      "Epoch 165/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4554 - val_loss: 0.4611\n",
      "Epoch 166/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4555 - val_loss: 0.4611\n",
      "Epoch 167/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4553 - val_loss: 0.4611\n",
      "Epoch 168/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4542 - val_loss: 0.4610\n",
      "Epoch 169/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4536 - val_loss: 0.4610\n",
      "Epoch 170/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4554 - val_loss: 0.4610\n",
      "Epoch 171/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4552 - val_loss: 0.4610\n",
      "Epoch 172/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4540 - val_loss: 0.4609\n",
      "Epoch 173/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4609\n",
      "Epoch 174/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4609\n",
      "Epoch 175/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4548 - val_loss: 0.4608\n",
      "Epoch 176/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4540 - val_loss: 0.4608\n",
      "Epoch 177/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4533 - val_loss: 0.4608\n",
      "Epoch 178/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4519 - val_loss: 0.4607\n",
      "Epoch 179/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4607\n",
      "Epoch 180/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4531 - val_loss: 0.4606\n",
      "Epoch 181/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4534 - val_loss: 0.4606\n",
      "Epoch 182/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4535 - val_loss: 0.4606\n",
      "Epoch 183/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4538 - val_loss: 0.4605\n",
      "Epoch 184/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4541 - val_loss: 0.4605\n",
      "Epoch 185/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4522 - val_loss: 0.4604\n",
      "Epoch 186/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4604\n",
      "Epoch 187/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4528 - val_loss: 0.4604\n",
      "Epoch 188/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4530 - val_loss: 0.4603\n",
      "Epoch 189/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4539 - val_loss: 0.4603\n",
      "Epoch 190/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4531 - val_loss: 0.4602\n",
      "Epoch 191/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4521 - val_loss: 0.4602\n",
      "Epoch 192/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4514 - val_loss: 0.4602\n",
      "Epoch 193/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4517 - val_loss: 0.4601\n",
      "Epoch 194/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4522 - val_loss: 0.4601\n",
      "Epoch 195/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4523 - val_loss: 0.4601\n",
      "Epoch 196/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4512 - val_loss: 0.4600\n",
      "Epoch 197/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4513 - val_loss: 0.4599\n",
      "Epoch 198/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4523 - val_loss: 0.4599\n",
      "Epoch 199/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4506 - val_loss: 0.4599\n",
      "Epoch 200/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4510 - val_loss: 0.4599\n",
      "Epoch 201/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4598\n",
      "Epoch 202/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4511 - val_loss: 0.4598\n",
      "Epoch 203/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4597\n",
      "Epoch 204/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4596\n",
      "Epoch 205/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4508 - val_loss: 0.4596\n",
      "Epoch 206/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4518 - val_loss: 0.4595\n",
      "Epoch 207/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4523 - val_loss: 0.4594\n",
      "Epoch 208/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4594\n",
      "Epoch 209/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4505 - val_loss: 0.4594\n",
      "Epoch 210/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4507 - val_loss: 0.4593\n",
      "Epoch 211/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4511 - val_loss: 0.4593\n",
      "Epoch 212/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4505 - val_loss: 0.4592\n",
      "Epoch 213/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4506 - val_loss: 0.4592\n",
      "Epoch 214/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4512 - val_loss: 0.4592\n",
      "Epoch 215/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4513 - val_loss: 0.4592\n",
      "Epoch 216/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4505 - val_loss: 0.4591\n",
      "Epoch 217/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4494 - val_loss: 0.4591\n",
      "Epoch 218/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4499 - val_loss: 0.4591\n",
      "Epoch 219/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4493 - val_loss: 0.4591\n",
      "Epoch 220/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4494 - val_loss: 0.4591\n",
      "Epoch 221/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4496 - val_loss: 0.4590\n",
      "Epoch 222/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4504 - val_loss: 0.4590\n",
      "Epoch 223/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4495 - val_loss: 0.4590\n",
      "Epoch 224/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4506 - val_loss: 0.4590\n",
      "Epoch 225/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4511 - val_loss: 0.4590\n",
      "Epoch 226/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4499 - val_loss: 0.4590\n",
      "Epoch 227/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4498 - val_loss: 0.4590\n",
      "Epoch 228/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4494 - val_loss: 0.4590\n",
      "Epoch 229/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4497 - val_loss: 0.4590\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "Epoch 230/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4495 - val_loss: 0.4590\n",
      "Epoch 231/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4495 - val_loss: 0.4590\n",
      "Epoch 232/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4499 - val_loss: 0.4589\n",
      "Epoch 233/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4493 - val_loss: 0.4589\n",
      "Epoch 234/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4486 - val_loss: 0.4589\n",
      "Epoch 235/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4485 - val_loss: 0.4589\n",
      "Epoch 236/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4501 - val_loss: 0.4589\n",
      "Epoch 237/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4487 - val_loss: 0.4588\n",
      "Epoch 238/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4483 - val_loss: 0.4588\n",
      "Epoch 239/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4486 - val_loss: 0.4588\n",
      "Epoch 240/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4492 - val_loss: 0.4588\n",
      "Epoch 241/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4488 - val_loss: 0.4587\n",
      "Epoch 242/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4491 - val_loss: 0.4587\n",
      "Epoch 243/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4485 - val_loss: 0.4587\n",
      "Epoch 244/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4480 - val_loss: 0.4587\n",
      "Epoch 245/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4484 - val_loss: 0.4587\n",
      "Epoch 246/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4482 - val_loss: 0.4587\n",
      "Epoch 247/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4498 - val_loss: 0.4587\n",
      "Epoch 248/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4490 - val_loss: 0.4586\n",
      "Epoch 249/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4489 - val_loss: 0.4586\n",
      "Epoch 250/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4586\n",
      "Epoch 251/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4586\n",
      "Epoch 252/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4496 - val_loss: 0.4586\n",
      "Epoch 253/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4482 - val_loss: 0.4586\n",
      "Epoch 254/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4487 - val_loss: 0.4586\n",
      "Epoch 255/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4479 - val_loss: 0.4586\n",
      "Epoch 256/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4474 - val_loss: 0.4586\n",
      "Epoch 257/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4484 - val_loss: 0.4585\n",
      "Epoch 258/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4483 - val_loss: 0.4585\n",
      "Epoch 259/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4585\n",
      "Epoch 260/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4585\n",
      "Epoch 261/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4492 - val_loss: 0.4585\n",
      "Epoch 262/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4585\n",
      "Epoch 263/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4491 - val_loss: 0.4584\n",
      "Epoch 264/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4481 - val_loss: 0.4584\n",
      "Epoch 265/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4483 - val_loss: 0.4584\n",
      "Epoch 266/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4584\n",
      "Epoch 267/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4584\n",
      "Epoch 268/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4471 - val_loss: 0.4583\n",
      "Epoch 269/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4474 - val_loss: 0.4583\n",
      "Epoch 270/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4583\n",
      "Epoch 271/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4487 - val_loss: 0.4582\n",
      "Epoch 272/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4582\n",
      "Epoch 273/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4478 - val_loss: 0.4582\n",
      "Epoch 274/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4489 - val_loss: 0.4582\n",
      "Epoch 275/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4582\n",
      "Epoch 276/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4479 - val_loss: 0.4581\n",
      "Epoch 277/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4581\n",
      "Epoch 278/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4484 - val_loss: 0.4581\n",
      "Epoch 279/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4480 - val_loss: 0.4581\n",
      "Epoch 280/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4580\n",
      "Epoch 281/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4580\n",
      "Epoch 282/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4580\n",
      "Epoch 283/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4476 - val_loss: 0.4580\n",
      "Epoch 284/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4472 - val_loss: 0.4580\n",
      "Epoch 285/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4580\n",
      "Epoch 286/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4580\n",
      "Epoch 287/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4579\n",
      "Epoch 288/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4475 - val_loss: 0.4579\n",
      "Epoch 289/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4468 - val_loss: 0.4579\n",
      "Epoch 290/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4579\n",
      "Epoch 291/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4579\n",
      "Epoch 292/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4486 - val_loss: 0.4579\n",
      "Epoch 293/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4464 - val_loss: 0.4579\n",
      "Epoch 294/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4475 - val_loss: 0.4578\n",
      "Epoch 295/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4459 - val_loss: 0.4578\n",
      "Epoch 296/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4463 - val_loss: 0.4578\n",
      "Epoch 297/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4577\n",
      "Epoch 298/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4453 - val_loss: 0.4577\n",
      "Epoch 299/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4577\n",
      "Epoch 300/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4472 - val_loss: 0.4577\n",
      "Epoch 301/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4576\n",
      "Epoch 302/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4469 - val_loss: 0.4576\n",
      "Epoch 303/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4576\n",
      "Epoch 304/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4472 - val_loss: 0.4576\n",
      "Epoch 305/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4467 - val_loss: 0.4575\n",
      "Epoch 306/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4575\n",
      "Epoch 307/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4471 - val_loss: 0.4576\n",
      "Epoch 308/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4473 - val_loss: 0.4576\n",
      "Epoch 309/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4472 - val_loss: 0.4576\n",
      "Epoch 310/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4576\n",
      "Epoch 311/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4468 - val_loss: 0.4576\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "Epoch 312/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4576\n",
      "Epoch 313/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4471 - val_loss: 0.4576\n",
      "Epoch 314/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4464 - val_loss: 0.4576\n",
      "Epoch 315/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4460 - val_loss: 0.4576\n",
      "Epoch 316/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4469 - val_loss: 0.4575\n",
      "Epoch 317/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4449 - val_loss: 0.4575\n",
      "Epoch 318/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4575\n",
      "Epoch 319/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4575\n",
      "Epoch 320/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4467 - val_loss: 0.4575\n",
      "Epoch 321/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4575\n",
      "Epoch 322/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4471 - val_loss: 0.4575\n",
      "Epoch 323/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4575\n",
      "Epoch 324/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4463 - val_loss: 0.4575\n",
      "Epoch 325/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4575\n",
      "Epoch 326/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4574\n",
      "Epoch 327/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4455 - val_loss: 0.4574\n",
      "Epoch 328/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4446 - val_loss: 0.4574\n",
      "Epoch 329/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4458 - val_loss: 0.4574\n",
      "Epoch 330/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4467 - val_loss: 0.4574\n",
      "Epoch 331/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4459 - val_loss: 0.4574\n",
      "Epoch 332/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4574\n",
      "Epoch 333/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4454 - val_loss: 0.4574\n",
      "Epoch 334/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4457 - val_loss: 0.4574\n",
      "Epoch 335/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4463 - val_loss: 0.4574\n",
      "Epoch 336/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4456 - val_loss: 0.4574\n",
      "Epoch 337/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4573\n",
      "Epoch 338/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "Epoch 339/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4443 - val_loss: 0.4573\n",
      "Epoch 340/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00340: ReduceLROnPlateau reducing learning rate to 1.2500000146076218e-08.\n",
      "Epoch 341/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4451 - val_loss: 0.4573\n",
      "Epoch 342/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 343/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4463 - val_loss: 0.4573\n",
      "Epoch 344/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 345/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 346/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4573\n",
      "Epoch 347/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 348/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 349/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 350/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 351/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4444 - val_loss: 0.4573\n",
      "Epoch 352/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4452 - val_loss: 0.4573\n",
      "Epoch 353/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00353: ReduceLROnPlateau reducing learning rate to 6.250000073038109e-09.\n",
      "Epoch 354/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4458 - val_loss: 0.4573\n",
      "Epoch 355/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4573\n",
      "Epoch 356/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4448 - val_loss: 0.4573\n",
      "Epoch 357/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4467 - val_loss: 0.4573\n",
      "Epoch 358/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4459 - val_loss: 0.4573\n",
      "Epoch 359/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "Epoch 360/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4452 - val_loss: 0.4573\n",
      "Epoch 361/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4451 - val_loss: 0.4573\n",
      "Epoch 362/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 363/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4458 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00363: ReduceLROnPlateau reducing learning rate to 3.1250000365190544e-09.\n",
      "Epoch 364/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "Epoch 365/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 366/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 367/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4452 - val_loss: 0.4573\n",
      "Epoch 368/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4446 - val_loss: 0.4573\n",
      "Epoch 369/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 370/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4456 - val_loss: 0.4573\n",
      "Epoch 371/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 372/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4442 - val_loss: 0.4573\n",
      "Epoch 373/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4453 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00373: ReduceLROnPlateau reducing learning rate to 1.5625000182595272e-09.\n",
      "Epoch 374/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 375/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 376/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 377/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4444 - val_loss: 0.4573\n",
      "Epoch 378/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 379/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4467 - val_loss: 0.4573\n",
      "Epoch 380/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4449 - val_loss: 0.4573\n",
      "Epoch 381/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 382/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 383/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00383: ReduceLROnPlateau reducing learning rate to 7.812500091297636e-10.\n",
      "Epoch 384/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 385/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4441 - val_loss: 0.4573\n",
      "Epoch 386/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 387/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 388/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 389/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4459 - val_loss: 0.4573\n",
      "Epoch 390/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4444 - val_loss: 0.4573\n",
      "Epoch 391/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4573\n",
      "Epoch 392/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4573\n",
      "Epoch 393/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4447 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00393: ReduceLROnPlateau reducing learning rate to 3.906250045648818e-10.\n",
      "Epoch 394/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 395/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4467 - val_loss: 0.4573\n",
      "Epoch 396/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 397/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 398/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4465 - val_loss: 0.4573\n",
      "Epoch 399/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 400/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 401/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 402/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 00402: early stopping\n"
     ]
    }
   ],
   "source": [
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu #26\n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_MIXED_margin_loss.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "dis_tr_triplets = discard_some_low(training_triplets, 0.7, 0.7)\n",
    "dis_val_triplets = discard_some_low(validation_triplets, 0.7, 0.7)\n",
    "\n",
    "low_margin, high_margin, bal_training_triplets = balance_input(dis_tr_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "vlow_margin, vhigh_margin, bal_val_triplets = balance_input(dis_val_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "\n",
    "\n",
    "history = triplet_model.fit(train_generator_mixed(bal_training_triplets, M, S, luscinia_triplets[:luscinia_train_len],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                steps_per_epoch=int(len(bal_training_triplets)/(lo+hi)), epochs=1000, verbose=1,\n",
    "                                validation_data=train_generator_mixed(bal_val_triplets, M, S, luscinia_triplets[luscinia_train_len:],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                validation_steps=int(len(bal_val_triplets)/(lo+hi)), callbacks=[cpCallback, reduce_lr, earlystop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE + MIXED (training on both bird decisions and Luscinia triplets - w/ pretraining on Luscinia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "triplet_model.load_weights('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_backup.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "117/117 [==============================] - 50s 380ms/step - loss: 0.0973 - val_loss: 0.0863\n",
      "Epoch 2/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0958 - val_loss: 0.0856\n",
      "Epoch 3/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0949 - val_loss: 0.0852\n",
      "Epoch 4/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0937 - val_loss: 0.0849\n",
      "Epoch 5/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0921 - val_loss: 0.0845\n",
      "Epoch 6/1000\n",
      "117/117 [==============================] - 44s 375ms/step - loss: 0.0927 - val_loss: 0.0842\n",
      "Epoch 7/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0911 - val_loss: 0.0839\n",
      "Epoch 8/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0919 - val_loss: 0.0836\n",
      "Epoch 9/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0894 - val_loss: 0.0834\n",
      "Epoch 10/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0896 - val_loss: 0.0831\n",
      "Epoch 11/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0880 - val_loss: 0.0828\n",
      "Epoch 12/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0874 - val_loss: 0.0826\n",
      "Epoch 13/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0871 - val_loss: 0.0824\n",
      "Epoch 14/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0871 - val_loss: 0.0822\n",
      "Epoch 15/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0865 - val_loss: 0.0820\n",
      "Epoch 16/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0837 - val_loss: 0.0818\n",
      "Epoch 17/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0843 - val_loss: 0.0816\n",
      "Epoch 18/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0842 - val_loss: 0.0815\n",
      "Epoch 19/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0828 - val_loss: 0.0813\n",
      "Epoch 20/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0832 - val_loss: 0.0812\n",
      "Epoch 21/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0832 - val_loss: 0.0810\n",
      "Epoch 22/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0824 - val_loss: 0.0809\n",
      "Epoch 23/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0821 - val_loss: 0.0807\n",
      "Epoch 24/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0799 - val_loss: 0.0806\n",
      "Epoch 25/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0817 - val_loss: 0.0805\n",
      "Epoch 26/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0806 - val_loss: 0.0804\n",
      "Epoch 27/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0807 - val_loss: 0.0803\n",
      "Epoch 28/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0788 - val_loss: 0.0801\n",
      "Epoch 29/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0803 - val_loss: 0.0800\n",
      "Epoch 30/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0802 - val_loss: 0.0799\n",
      "Epoch 31/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0807 - val_loss: 0.0798\n",
      "Epoch 32/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0788 - val_loss: 0.0797\n",
      "Epoch 33/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0781 - val_loss: 0.0797\n",
      "Epoch 34/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0802 - val_loss: 0.0796\n",
      "Epoch 35/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0789 - val_loss: 0.0795\n",
      "Epoch 36/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0791 - val_loss: 0.0794\n",
      "Epoch 37/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0778 - val_loss: 0.0794\n",
      "Epoch 38/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0779 - val_loss: 0.0793\n",
      "Epoch 39/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0781 - val_loss: 0.0792\n",
      "Epoch 40/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0777 - val_loss: 0.0792\n",
      "Epoch 41/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0786 - val_loss: 0.0791\n",
      "Epoch 42/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0780 - val_loss: 0.0791\n",
      "Epoch 43/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0783 - val_loss: 0.0790\n",
      "Epoch 44/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0779 - val_loss: 0.0790\n",
      "Epoch 45/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0777 - val_loss: 0.0789\n",
      "Epoch 46/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0765 - val_loss: 0.0789\n",
      "Epoch 47/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0770 - val_loss: 0.0788\n",
      "Epoch 48/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0763 - val_loss: 0.0788\n",
      "Epoch 49/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0768 - val_loss: 0.0788\n",
      "Epoch 50/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.0769 - val_loss: 0.0787\n",
      "Epoch 51/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0768 - val_loss: 0.0787\n",
      "Epoch 52/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0758 - val_loss: 0.0787\n",
      "Epoch 53/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0762 - val_loss: 0.0787\n",
      "Epoch 54/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0760 - val_loss: 0.0786\n",
      "Epoch 55/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0766 - val_loss: 0.0786\n",
      "Epoch 56/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0753 - val_loss: 0.0786\n",
      "Epoch 57/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0749 - val_loss: 0.0786\n",
      "Epoch 58/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.0753 - val_loss: 0.0785\n",
      "Epoch 59/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0751 - val_loss: 0.0785\n",
      "Epoch 60/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0754 - val_loss: 0.0785\n",
      "Epoch 61/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0755 - val_loss: 0.0785\n",
      "Epoch 62/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0752 - val_loss: 0.0785\n",
      "Epoch 63/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0749 - val_loss: 0.0785\n",
      "Epoch 64/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0750 - val_loss: 0.0785\n",
      "Epoch 65/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.0751 - val_loss: 0.0785\n",
      "Epoch 66/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0750 - val_loss: 0.0785\n",
      "Epoch 67/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0748 - val_loss: 0.0785\n",
      "Epoch 68/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0756 - val_loss: 0.0784\n",
      "Epoch 69/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0747 - val_loss: 0.0784\n",
      "Epoch 70/1000\n",
      " 66/117 [===============>..............] - ETA: 17s - loss: 0.0731"
     ]
    }
   ],
   "source": [
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu \n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_MIXED_margin_loss.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "dis_tr_triplets = discard_some_low(training_triplets, 0.7, 0.7)\n",
    "dis_val_triplets = discard_some_low(validation_triplets, 0.7, 0.7)\n",
    "\n",
    "low_margin, high_margin, bal_training_triplets = balance_input(dis_tr_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "vlow_margin, vhigh_margin, bal_val_triplets = balance_input(dis_val_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "\n",
    "\n",
    "history = triplet_model.fit(train_generator_mixed(bal_training_triplets, M, S, luscinia_triplets[:luscinia_train_len],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                steps_per_epoch=int(len(bal_training_triplets)/(lo+hi)), epochs=1000, verbose=1,\n",
    "                                validation_data=train_generator_mixed(bal_val_triplets, M, S, luscinia_triplets[luscinia_train_len:],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                validation_steps=int(len(bal_val_triplets)/(lo+hi)), callbacks=[cpCallback, reduce_lr, earlystop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (for both ambiguous and unambiguous triplets - based on different distance margins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_high_evaluation(path_mel, vhigh_margin, vlow_margin, triplet_model, margin = 0.00000001, max_margin = 0.0000001, step = 0.00000001):\n",
    "    # pos, neg, anc\n",
    "    while margin < max_margin:\n",
    "        acc_cnt = 0\n",
    "        high_cnt = 0\n",
    "        low_cnt = 0\n",
    "        for triplet in vhigh_margin:\n",
    "            tr_pos = triplet[1][:-4]+'.pckl'\n",
    "            tr_neg = triplet[2][:-4]+'.pckl'\n",
    "            tr_anc = triplet[3][:-4]+'.pckl'\n",
    "\n",
    "            f = open(path_mel+tr_anc, 'rb')\n",
    "            anc = pickle.load(f).T\n",
    "            f.close()\n",
    "            anc = (anc - M)/S\n",
    "            anc = np.expand_dims(anc, axis=0)\n",
    "            anc = np.expand_dims(anc, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_pos, 'rb')\n",
    "            pos = pickle.load(f).T\n",
    "            f.close()\n",
    "            pos = (pos - M)/S\n",
    "            pos = np.expand_dims(pos, axis=0)\n",
    "            pos = np.expand_dims(pos, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_neg, 'rb')\n",
    "            neg = pickle.load(f).T\n",
    "            f.close()\n",
    "            neg = (neg - M)/S\n",
    "            neg = np.expand_dims(neg, axis=0)\n",
    "            neg = np.expand_dims(neg, axis=-1)\n",
    "\n",
    "            y_pred = triplet_model.predict([anc, pos, neg])\n",
    "\n",
    "            anchor1 = y_pred[:, 0:emb_size]\n",
    "            positive1 = y_pred[:, emb_size:emb_size*2]\n",
    "            negative1 = y_pred[:, emb_size*2:emb_size*3]\n",
    "\n",
    "            pos_dist = np.sqrt(np.sum(np.square(anchor1 - positive1), axis=1))[0]\n",
    "            neg_dist = np.sqrt(np.sum(np.square(anchor1 - negative1), axis=1))[0]\n",
    "\n",
    "            if np.square(neg_dist) > np.square(pos_dist) + margin:\n",
    "                acc_cnt += 1\n",
    "                high_cnt += 1\n",
    "        for triplet in vlow_margin:\n",
    "            tr_pos = triplet[1][:-4]+'.pckl'\n",
    "            tr_neg = triplet[2][:-4]+'.pckl'\n",
    "            tr_anc = triplet[3][:-4]+'.pckl'\n",
    "\n",
    "            f = open(path_mel+tr_anc, 'rb')\n",
    "            anc = pickle.load(f).T\n",
    "            f.close()\n",
    "            anc = (anc - M)/S\n",
    "            anc = np.expand_dims(anc, axis=0)\n",
    "            anc = np.expand_dims(anc, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_pos, 'rb')\n",
    "            pos = pickle.load(f).T\n",
    "            f.close()\n",
    "            pos = (pos - M)/S\n",
    "            pos = np.expand_dims(pos, axis=0)\n",
    "            pos = np.expand_dims(pos, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_neg, 'rb')\n",
    "            neg = pickle.load(f).T\n",
    "            f.close()\n",
    "            neg = (neg - M)/S\n",
    "            neg = np.expand_dims(neg, axis=0)\n",
    "            neg = np.expand_dims(neg, axis=-1)\n",
    "\n",
    "            y_pred = triplet_model.predict([anc, pos, neg])\n",
    "\n",
    "            anchor1 = y_pred[:, 0:emb_size]\n",
    "            positive1 = y_pred[:, emb_size:emb_size*2]\n",
    "            negative1 = y_pred[:, emb_size*2:emb_size*3]\n",
    "\n",
    "            pos_dist = np.sqrt(np.sum(np.square(anchor1 - positive1), axis=1))[0]\n",
    "            neg_dist = np.sqrt(np.sum(np.square(anchor1 - negative1), axis=1))[0]\n",
    "\n",
    "            if np.abs(np.square(pos_dist) - np.square(neg_dist)) <= margin:\n",
    "                acc_cnt+=1\n",
    "                low_cnt+=1\n",
    "        print('MARGIN = ', margin)\n",
    "        print('Macro-average Low-High margin accuracy: ',0.5*(high_cnt/(len(vhigh_margin)) + low_cnt/(len(vlow_margin)))*100, '%')\n",
    "        print('Micro-average Low-High margin accuracy: ',(acc_cnt/(len(vhigh_margin)+len(vlow_margin)))*100, '%') \n",
    "        print('High margin accuracy: ',(high_cnt/(len(vhigh_margin)))*100, '%')  \n",
    "        print('Low margin accuracy: ',(low_cnt/(len(vlow_margin)))*100, '%')  \n",
    "        margin += step\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate sets between low-margin (ambiguous) and high-margin (unambiguous) triplets\n",
    "low_margin = pickle.load( open(path_files+'train_triplets_low_50_70_ACC70.pckl', 'rb'))\n",
    "vlow_margin = pickle.load(open(path_files+'val_triplets_low_50_70_ACC70.pckl', 'rb'))\n",
    "high_margin = pickle.load(open(path_files+'train_triplets_high_50_70_ACC70.pckl', 'rb'))\n",
    "vhigh_margin =pickle.load(open(path_files+'val_triplets_high_50_70_ACC70.pckl', 'rb'))\n",
    "tlow_margin =pickle.load(open(path_files+'test_triplets_low_50_70_ACC70.pckl', 'rb'))\n",
    "thigh_margin = pickle.load(open(path_files+'test_triplets_high_50_70_ACC70.pckl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation on a high margin and low margin set of the same split\n",
    "low_high_evauation(path_mel, high_margin, low_margin, triplet_model, margin = 0.0, max_margin = 0.01, step = 0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
