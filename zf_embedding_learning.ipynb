{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import h5py\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "from sklearn.manifold import TSNE, MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dropout, concatenate, Concatenate, Activation, Input, Dense, Conv2D, GRU, MaxPooling2D, MaxPooling1D, Flatten, Reshape, LeakyReLU, PReLU, BatchNormalization, Bidirectional, TimeDistributed, Lambda, GlobalMaxPool1D, GlobalMaxPool2D, GlobalAveragePooling2D, Multiply, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.initializers import random_normal, glorot_uniform, glorot_normal\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mel = './melspecs/'\n",
    "path_files = '../files/'\n",
    "\n",
    "train_triplet_file = 'train_triplets_50_70_single.pckl'\n",
    "train_gt_file = 'train_gt_50_70_single.pckl'\n",
    "train_cons_file = 'train_cons_50_70_single.pckl'\n",
    "train_trials_file = 'train_trials_50_70_single.pckl'\n",
    "\n",
    "test_triplet_file = 'test_triplets_50_70_single.pckl'\n",
    "test_gt_file = 'test_gt_50_70_single.pckl'\n",
    "test_cons_file = 'test_cons_50_70_single.pckl'\n",
    "test_trials_file = 'test_trials_50_70_single.pckl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "luscinia_triplets_file = 'luscinia_triplets_filtered.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "luscinia_triplets = []\n",
    "with open(path_files+luscinia_triplets_file, 'r',  newline='') as csvfile:\n",
    "    csv_r = csv.reader(csvfile, delimiter=',')\n",
    "    for row in csv_r:\n",
    "        luscinia_triplets.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83566 20891\n"
     ]
    }
   ],
   "source": [
    "luscinia_triplets = luscinia_triplets[1:]\n",
    "luscinia_train_len = round(8*len(luscinia_triplets)/10)\n",
    "luscinia_val_len = len(luscinia_triplets) - luscinia_train_len\n",
    "print(luscinia_train_len, luscinia_val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_files+'mean_std_luscinia_pretraining.pckl', 'rb')\n",
    "train_dict = pickle.load(f)\n",
    "M_l = train_dict['mean']\n",
    "S_l = train_dict['std']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_files+'training_setup_1_ordered_acc_single_cons_50_70_trials.pckl', 'rb')\n",
    "train_dict = pickle.load(f)\n",
    "train_keys = train_dict['train_keys']\n",
    "training_triplets = train_dict['train_triplets']\n",
    "val_keys = train_dict['val_keys']\n",
    "validation_triplets = train_dict['vali_triplets']\n",
    "test_triplet = train_dict['test_triplets']\n",
    "test_keys = train_dict['test_keys']\n",
    "M = train_dict['train_mean']\n",
    "S = train_dict['train_std']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convBNpr(a, dilation, num_filters, kernel):\n",
    "    c1 = Conv2D(filters=num_filters, kernel_size=kernel, strides=(1, 1), dilation_rate=dilation, padding='same', use_bias=False, kernel_initializer=glorot_uniform(seed=123), kernel_regularizer=regularizers.l2(1e-4))(a)\n",
    "    c1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(c1)\n",
    "    # c1 = Activation('relu')(c1)\n",
    "    c1 = LeakyReLU(alpha=0.3)(c1)\n",
    "    return c1\n",
    "def createModelMatrix(emb_size, input_shape=(170, 150, 1)):\n",
    "    a = Input(shape=(input_shape)) \n",
    "    \n",
    "    c = convBNpr(a, 1, 12, (3,3))\n",
    "    c = convBNpr(c, 2, 32, (3,3))\n",
    "    c = convBNpr(c, 4, 64, (3,3))\n",
    "    c = convBNpr(c, 8, 128, (3,3))\n",
    "    # attention with sigmoid\n",
    "    a1 = Conv2D(filters=128, kernel_size=(1,1), strides=(1, 1), padding='same', activation = 'sigmoid',  use_bias=True, kernel_regularizer=regularizers.l2(1e-4),kernel_initializer=glorot_uniform(seed=123))(c)\n",
    "    \n",
    "    # projection with leaky relu \n",
    "   # c1 = convBNpr(c, 1, 128, (1,1))\n",
    "    \n",
    "    # sum of sum of attention\n",
    "    s = Lambda(lambda x: K.sum(K.sum(x,axis=1, keepdims=True), axis=2, keepdims=True))(a1)\n",
    "    s = Lambda(lambda x: K.repeat_elements(x, 170, axis=1))(s)\n",
    "    s = Lambda(lambda x: K.repeat_elements(x, 150, axis=2))(s)\n",
    "    \n",
    "    # probability matrix of attention\n",
    "    p = Lambda(lambda x: x[0]/x[1])([a1,s])\n",
    "    \n",
    "    # inner product of attention and projection matrices\n",
    "    m = Multiply()([c, p])\n",
    "    \n",
    "    # output\n",
    "    out_sum = Lambda(lambda x: K.sum(K.sum(x, axis=1), axis=1))(m)\n",
    "    \n",
    "    # attention side\n",
    "    #d1 = Dense(emb_size, kernel_initializer=glorot_normal(seed=321), activation='relu')(out_sum)\n",
    "    #d2 = Dense(emb_size*10, kernel_initializer=glorot_normal(seed=111), activation='relu')(d1)\n",
    "    d3 = Dense(emb_size*100, kernel_initializer=glorot_normal(seed=222), kernel_regularizer=regularizers.l2(1e-4),activation='relu')(out_sum)\n",
    "    d3 = Dropout(.2, seed=222)(d3)\n",
    "    d4 = Dense(emb_size*10, kernel_initializer=glorot_normal(seed=333), kernel_regularizer=regularizers.l2(1e-4),activation='relu')(d3)\n",
    "    d4 = Dropout(.2, seed=333)(d4)\n",
    "    d5 = Dense(emb_size, kernel_initializer=glorot_normal(seed=132), kernel_regularizer=regularizers.l2(1e-4))(d4)\n",
    "    d5 = Dropout(.2, seed = 132)(d5)\n",
    "    \n",
    "    # maxpool side\n",
    "    x = convBNpr(c, 1, 64, (3,3)) \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = convBNpr(x, 1, 32, (3,3))\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = convBNpr(x, 1, 12, (3,3))\n",
    "\n",
    "    f = Flatten()(x)\n",
    "    df1 = Dense(emb_size*100, kernel_initializer=glorot_normal(seed=456), kernel_regularizer=regularizers.l2(1e-4), activation='relu')(f)#\n",
    "    df1 = Dropout(.2, seed=456)(df1)\n",
    "    df2 = Dense(emb_size*10, kernel_initializer=glorot_normal(seed=654), kernel_regularizer=regularizers.l2(1e-4), activation='relu')(df1)#\n",
    "    df2 = Dropout(.2, seed=654)(df2)\n",
    "    df3 = Dense(emb_size, kernel_initializer=glorot_normal(seed=546), kernel_regularizer=regularizers.l2(1e-4))(df2)#\n",
    "    df3 = Dropout(.2, seed=546)(df3)\n",
    "\n",
    "    concat = Concatenate(axis=-1)([d5, df3])\n",
    "    dd = Dense(emb_size, kernel_initializer=glorot_normal(seed=999), kernel_regularizer=regularizers.l2(1e-4))(concat)\n",
    "\n",
    "    sph = Lambda(lambda  x: K.l2_normalize(x,axis=1))(dd)\n",
    "    \n",
    "    # base model creation\n",
    "    base_model = Model(a,sph) \n",
    "    \n",
    "    # triplet framework\n",
    "    input_anchor = Input(shape=(input_shape))\n",
    "    input_positive = Input(shape=(input_shape))\n",
    "    input_negative = Input(shape=(input_shape)) \n",
    "    \n",
    "    net_anchor = base_model(input_anchor)\n",
    "    net_positive = base_model(input_positive)\n",
    "    net_negative = base_model(input_negative)\n",
    "    \n",
    "    base_model.summary()\n",
    "    \n",
    "    merged_vector = concatenate([net_anchor, net_positive, net_negative], axis=-1)\n",
    "    \n",
    "    model = Model([input_anchor, input_positive, input_negative], outputs=merged_vector)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_weighted_triplet_loss(margin, emb_size, m = 0 , w = 0, lh = 1):\n",
    "    def lossFunction(y_true,y_pred):\n",
    "        \n",
    "        weight = y_true[:, 0] # acc\n",
    "        cons = y_true[:, 1] # consistency\n",
    "        trials = y_true[:, 2] # number of trials\n",
    "        \n",
    "        #m = K.constant(0.5)\n",
    "        anchor = y_pred[:, 0:emb_size]\n",
    "        positive = y_pred[:, emb_size:emb_size*2]\n",
    "        negative = y_pred[:, emb_size*2:emb_size*3]\n",
    "\n",
    "        # distance between the anchor and the positive\n",
    "        pos_dist = K.sqrt(K.sum(K.square(anchor - positive), axis=1)) # l2 distance\n",
    "        #pos_dist = K.sum(K.abs(anchor-positive), axis=1) # l1 distance\n",
    "\n",
    "        # distance between the anchor and the negative\n",
    "        neg_dist = K.sqrt(K.sum(K.square(anchor - negative), axis=1)) # l2 distance\n",
    "        #neg_dist = K.sum(K.abs(anchor-negative), axis=1) # l1 distance\n",
    "\n",
    "        loss_h = 0\n",
    "        loss_l = 0\n",
    "        \n",
    "        if lh == 1:\n",
    "            # DOES NOT WORK WITH MASKED LOSS\n",
    "            # low-high margin loss\n",
    "            p_c = K.square(neg_dist) - K.square(pos_dist) - margin  \n",
    "            p_i = K.square(neg_dist) - K.square(pos_dist)\n",
    "            \n",
    "            loss_1 = cons*(1-K.exp(p_c)) + (1-cons)*(1-K.exp(-K.abs(p_i)))\n",
    "            \n",
    "            #loss_1 = cons*K.exp(-p_c) + (1-cons)*(1-K.exp(-K.abs(p_c))) # haptic loss\n",
    "            #loss_1 = K.minimum(trials, K.mean(trials))*loss_1\n",
    "        if m != 0:\n",
    "            # masked loss\n",
    "            basic_loss = pos_dist - neg_dist + margin\n",
    "            \n",
    "            threshold = K.max(basic_loss) * m\n",
    "            mask = 2 + margin - K.maximum(basic_loss, threshold) \n",
    "\n",
    "            loss_1 = basic_loss * mask\n",
    "        \n",
    "        if w == 1:\n",
    "            # weighted based on acc\n",
    "            weighted_loss = weight*loss_1\n",
    "        else:\n",
    "            # non-weighted\n",
    "            weighted_loss = loss_1\n",
    "            \n",
    "        loss = K.maximum(weighted_loss, 0.0)\n",
    "\n",
    "        return loss\n",
    "    return lossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_some_low(triplet_list, cons, acc):\n",
    "    low_margin = []\n",
    "    high_margin = []\n",
    "    \n",
    "    for i in range(len(triplet_list)):\n",
    "        if float(triplet_list[i][-1]) < cons: # low margin\n",
    "            if float(triplet_list[i][-2]) >= acc: # ACC \n",
    "                low_margin.append(triplet_list[i])\n",
    "        else: # high margin\n",
    "            high_margin.append(triplet_list[i])\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(low_margin)\n",
    "    random.shuffle(high_margin)\n",
    "    \n",
    "    low_margin.extend(high_margin)\n",
    "        \n",
    "    return low_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_input(triplet_list, cons, hi_balance = 6, lo_balance = 6):\n",
    "    batchsize = hi_balance + lo_balance\n",
    "    low_margin = []\n",
    "    high_margin = []\n",
    "    \n",
    "    for i in range(len(triplet_list)):\n",
    "        if float(triplet_list[i][-1]) < cons: # low margin\n",
    "            low_margin.append(triplet_list[i])\n",
    "        else: # high margin\n",
    "            high_margin.append(triplet_list[i])\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(low_margin)\n",
    "    random.shuffle(high_margin)\n",
    "    \n",
    "    new_triplet_list = []\n",
    "    maxlen = np.maximum(len(low_margin), len(high_margin))\n",
    "    \n",
    "    hi_start = 0\n",
    "    lo_start = 0\n",
    "    for i in range(0,int(maxlen/hi_balance)*batchsize,batchsize):\n",
    "        for j in range(hi_start,hi_start+hi_balance,1):\n",
    "            new_triplet_list.append(high_margin[np.mod(j,len(high_margin))])\n",
    "        hi_start+=hi_balance\n",
    "        for j in range(lo_start, lo_start+lo_balance,1):\n",
    "            new_triplet_list.append(low_margin[np.mod(j,len(low_margin))])\n",
    "        lo_start+=lo_balance\n",
    "            \n",
    "    return low_margin, high_margin, new_triplet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_mixed(triplet_list, M, S, luscinia_triplets, M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel):\n",
    "    \n",
    "    acc_gt = np.zeros((batchsize, emb_size))\n",
    "   \n",
    "    random.seed(123)\n",
    "    #random.shuffle(triplet_list) \n",
    "    random.shuffle(luscinia_triplets)\n",
    "    \n",
    "    while 1:\n",
    "    \n",
    "        anchors_input = np.empty((batchsize, 170, 150, 1))\n",
    "        positives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        negatives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        \n",
    "        imax = int(len(triplet_list)/(lo+hi))\n",
    "        \n",
    "        list_cnt = 0\n",
    "        luscinia_cnt = 0\n",
    "        \n",
    "        for i in range(imax):        \n",
    "            for j in range(batchsize):\n",
    "                \n",
    "                if j < (lo+hi):\n",
    "                    triplet = triplet_list[list_cnt]\n",
    "                    list_cnt += 1\n",
    "                    \n",
    "                    tr_anc = triplet[3][:-4]+'.pckl'\n",
    "                    tr_pos = triplet[1][:-4]+'.pckl'\n",
    "                    tr_neg = triplet[2][:-4]+'.pckl'\n",
    "                    acc_gt[j][0] = float(triplet[-2]) # acc\n",
    "                    acc_gt[j][1] = 1 if float(triplet[-1])>=0.7 else 0 # cons\n",
    "                    acc_gt[j][2] = int(triplet[-3]) # number of trials\n",
    "\n",
    "                else:\n",
    "                    triplet = luscinia_triplets[luscinia_cnt]\n",
    "                    luscinia_cnt += 1\n",
    "                    \n",
    "                    tr_anc = triplet[2][:-4]+'.pckl'\n",
    "                    tr_pos = triplet[0][:-4]+'.pckl'\n",
    "                    tr_neg = triplet[1][:-4]+'.pckl'\n",
    "                    acc_gt[j][0] = 1 # acc\n",
    "                    acc_gt[j][1] = 1  # cons\n",
    "                    acc_gt[j][2] = 1 # number of trials\n",
    "                \n",
    "                f = open(path_mel+tr_anc, 'rb')\n",
    "                anc = pickle.load(f).T\n",
    "                f.close()\n",
    "                anc = (anc - M)/S\n",
    "                anc = np.expand_dims(anc, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_pos, 'rb')\n",
    "                pos = pickle.load(f).T\n",
    "                f.close()\n",
    "                pos = (pos - M)/S\n",
    "                pos = np.expand_dims(pos, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_neg, 'rb')\n",
    "                neg = pickle.load(f).T\n",
    "                f.close()\n",
    "                neg = (neg - M)/S\n",
    "                neg = np.expand_dims(neg, axis=-1)\n",
    "                \n",
    "                anchors_input[j] = anc\n",
    "                positives_input[j] = pos\n",
    "                negatives_input[j] = neg\n",
    "                \n",
    "            yield [anchors_input, positives_input, negatives_input], acc_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_luscinia(triplet_list, M, S, batchsize, emb_size, path_mel, ordered = True):\n",
    "    \n",
    "    # if ordered == False:\n",
    "    # triplet list: [[cycle, left, right, anchor, decision], [...], ...]\n",
    "    # if decision = 0: positive <-- left AND negative <-- right\n",
    "    # if decision = 1: positive <-- right AND negative <-- left\n",
    "    \n",
    "    # if ordered == True:\n",
    "    #triplet list: [[cycle, positive, negative, anchor, extra_info], [...], ...]\n",
    "    \n",
    "    \n",
    "    #dummy_gt = np.zeros((batchsize, emb_size)) \n",
    "    acc_gt = np.zeros((batchsize, emb_size))\n",
    "    \n",
    "    #recnames = [f for f in listdir(path_mel) if isfile(join(path, f))]\n",
    "    \n",
    "    #anchornames = [recnames[i] for i in anchors]\n",
    "    #positivenames = [recnames[i] for i in positives]\n",
    "    #negativenames = [recnames[i] for i in negatives]\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(triplet_list) \n",
    "    \n",
    "    while 1:\n",
    "    \n",
    "        anchors_input = np.empty((batchsize, 170, 150, 1))\n",
    "        positives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        negatives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        \n",
    "        imax = int(len(triplet_list)/batchsize)\n",
    "                \n",
    "        for i in range(imax):        \n",
    "            for j in range(batchsize):\n",
    "                triplet = triplet_list[i*batchsize+j]\n",
    "                \n",
    "                tr_anc = triplet[2][:-4]+'.pckl'\n",
    "                tr_pos = triplet[0][:-4]+'.pckl'\n",
    "                tr_neg = triplet[1][:-4]+'.pckl'\n",
    "                acc_gt[j][0] = 1 # acc\n",
    "                acc_gt[j][1] = 1  # cons\n",
    "                acc_gt[j][2] = 1 # number of trials\n",
    "                 \n",
    "                f = open(path_mel+tr_anc, 'rb')\n",
    "                anc = pickle.load(f).T\n",
    "                f.close()\n",
    "                anc = (anc - M)/S\n",
    "                anc = np.expand_dims(anc, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_pos, 'rb')\n",
    "                pos = pickle.load(f).T\n",
    "                f.close()\n",
    "                pos = (pos - M)/S\n",
    "                pos = np.expand_dims(pos, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_neg, 'rb')\n",
    "                neg = pickle.load(f).T\n",
    "                f.close()\n",
    "                neg = (neg - M)/S\n",
    "                neg = np.expand_dims(neg, axis=-1)\n",
    "                \n",
    "                anchors_input[j] = anc\n",
    "                positives_input[j] = pos\n",
    "                negatives_input[j] = neg\n",
    "                \n",
    "            yield [anchors_input, positives_input, negatives_input], acc_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(triplet_list, M, S, batchsize, emb_size, path_mel, ordered = True):\n",
    "    \n",
    "    # if ordered == False:\n",
    "    # triplet list: [[cycle, left, right, anchor, decision], [...], ...]\n",
    "    # if decision = 0: positive <-- left AND negative <-- right\n",
    "    # if decision = 1: positive <-- right AND negative <-- left\n",
    "    \n",
    "    # if ordered == True:\n",
    "    #triplet list: [[cycle, positive, negative, anchor, extra_info], [...], ...]\n",
    "    \n",
    "    \n",
    "    #dummy_gt = np.zeros((batchsize, emb_size)) \n",
    "    acc_gt = np.zeros((batchsize, emb_size))\n",
    "    \n",
    "    #recnames = [f for f in listdir(path_mel) if isfile(join(path, f))]\n",
    "    \n",
    "    #anchornames = [recnames[i] for i in anchors]\n",
    "    #positivenames = [recnames[i] for i in positives]\n",
    "    #negativenames = [recnames[i] for i in negatives]\n",
    "    \n",
    "    random.seed(123)\n",
    "    random.shuffle(triplet_list) \n",
    "    \n",
    "    while 1:\n",
    "    \n",
    "        anchors_input = np.empty((batchsize, 170, 150, 1))\n",
    "        positives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        negatives_input = np.empty((batchsize, 170, 150, 1))\n",
    "        \n",
    "        imax = int(len(triplet_list)/batchsize)\n",
    "                \n",
    "        for i in range(imax):        \n",
    "            for j in range(batchsize):\n",
    "                triplet = triplet_list[i*batchsize+j]\n",
    "                \n",
    "                tr_anc = triplet[3][:-4]+'.pckl'\n",
    "                \n",
    "                if ordered == False:\n",
    "                    if triplet[-1] == '0':\n",
    "                        tr_pos = triplet[1][:-4]+'.pckl'\n",
    "                        tr_neg = triplet[2][:-4]+'.pckl'\n",
    "                    else:\n",
    "                        tr_pos = triplet[2][:-4]+'.pckl'\n",
    "                        tr_neg = triplet[1][:-4]+'.pckl'\n",
    "                else: # ordered == True\n",
    "                    tr_pos = triplet[1][:-4]+'.pckl'\n",
    "                    tr_neg = triplet[2][:-4]+'.pckl'\n",
    "                    #print(triplet)\n",
    "                    acc_gt[j][0] = float(triplet[-2]) # acc\n",
    "                    acc_gt[j][1] = 1 if float(triplet[-1])>=0.7 else 0 # cons\n",
    "                    acc_gt[j][2] = int(triplet[-3]) # number of trials\n",
    "                    #print(acc_gt[j])\n",
    "                    #input()\n",
    "                f = open(path_mel+tr_anc, 'rb')\n",
    "                anc = pickle.load(f).T\n",
    "                f.close()\n",
    "                anc = (anc - M)/S\n",
    "                anc = np.expand_dims(anc, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_pos, 'rb')\n",
    "                pos = pickle.load(f).T\n",
    "                f.close()\n",
    "                pos = (pos - M)/S\n",
    "                pos = np.expand_dims(pos, axis=-1)\n",
    "                \n",
    "                f = open(path_mel+tr_neg, 'rb')\n",
    "                neg = pickle.load(f).T\n",
    "                f.close()\n",
    "                neg = (neg - M)/S\n",
    "                neg = np.expand_dims(neg, axis=-1)\n",
    "                \n",
    "                anchors_input[j] = anc\n",
    "                positives_input[j] = pos\n",
    "                negatives_input[j] = neg\n",
    "                \n",
    "            yield [anchors_input, positives_input, negatives_input], acc_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 170, 150, 12) 108         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 170, 150, 12) 48          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 170, 150, 12) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 170, 150, 32) 3456        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 170, 150, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 170, 150, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 170, 150, 64) 18432       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 170, 150, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 170, 150, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 170, 150, 128 73728       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 170, 150, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 170, 150, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 170, 150, 64) 73728       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 170, 150, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 170, 150, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 85, 75, 64)   0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 85, 75, 32)   18432       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 170, 150, 128 16512       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 85, 75, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1, 1, 128)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 85, 75, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 170, 1, 128)  0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 42, 37, 32)   0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 170, 150, 128 0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 42, 37, 12)   3456        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 170, 150, 128 0           conv2d_4[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 42, 37, 12)   48          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 170, 150, 128 0           leaky_re_lu_3[0][0]              \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 42, 37, 12)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 128)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 18648)        0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1600)         206400      lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1600)         29838400    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1600)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1600)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 160)          256160      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 160)          256160      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 160)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 160)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2576        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2576        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32)           0           dropout_2[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           528         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16)           0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,772,028\n",
      "Trainable params: 30,771,340\n",
      "Non-trainable params: 688\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 170, 150, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 16)           30772028    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48)           0           model[0][0]                      \n",
      "                                                                 model[1][0]                      \n",
      "                                                                 model[2][0]                      \n",
      "==================================================================================================\n",
      "Total params: 30,772,028\n",
      "Trainable params: 30,771,340\n",
      "Non-trainable params: 688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "emb_size=16\n",
    "margin = 0.1\n",
    "m = 0\n",
    "lr = 1e-8\n",
    "adam = Adam(lr = lr)\n",
    "\n",
    "triplet_model = createModelMatrix(emb_size=emb_size, input_shape=(170, 150, 1))\n",
    "triplet_model.summary()\n",
    "triplet_model.compile(loss=masked_weighted_triplet_loss(margin=margin, emb_size=emb_size, m=m, w = 0, lh = 1),optimizer=adam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_distances = []\n",
    "acc_cnt = 0\n",
    "for t_triplet in luscinia_triplets[luscinia_train_len:]:\n",
    "    #print(t_triplet)\n",
    "    tr_anc = t_triplet[2][:-4]+'.pckl'\n",
    "    tr_pos = t_triplet[0][:-4]+'.pckl'\n",
    "    tr_neg = t_triplet[1][:-4]+'.pckl'\n",
    "    \n",
    "    f = open(path_mel+tr_anc, 'rb')\n",
    "    anc = pickle.load(f).T\n",
    "    f.close()\n",
    "    anc = (anc - M)/S\n",
    "    anc = np.expand_dims(anc, axis=-1)\n",
    "    anc = np.expand_dims(anc, axis=0)\n",
    "               \n",
    "    f = open(path_mel+tr_pos, 'rb')\n",
    "    pos = pickle.load(f).T\n",
    "    f.close()\n",
    "    pos = (pos - M)/S\n",
    "    pos = np.expand_dims(pos, axis=-1)\n",
    "    pos = np.expand_dims(pos, axis=0)\n",
    "                \n",
    "    f = open(path_mel+tr_neg, 'rb')\n",
    "    neg = pickle.load(f).T\n",
    "    f.close()\n",
    "    neg = (neg - M)/S\n",
    "    neg = np.expand_dims(neg, axis=-1)\n",
    "    neg = np.expand_dims(neg, axis=0)\n",
    "    \n",
    "    y_pred = triplet_model.predict([anc, pos, neg], batch_size=1, verbose=0)\n",
    "    \n",
    "    anchor1 = y_pred[:, 0:emb_size]\n",
    "    positive1 = y_pred[:, emb_size:emb_size*2]\n",
    "    negative1 = y_pred[:, emb_size*2:emb_size*3]\n",
    "    \n",
    "    #pos_dist = np.sqrt(np.sum(np.square(anchor1 - positive1), axis=1))[0]\n",
    "    #neg_dist = np.sqrt(np.sum(np.square(anchor1 - negative1), axis=1))[0]\n",
    "\n",
    "    pos_dist = np.sum(np.abs(anchor1 - positive1), axis=1)[0]\n",
    "    neg_dist = np.sum(np.abs(anchor1 - negative1), axis=1)[0]\n",
    "\n",
    "    if pos_dist < neg_dist:\n",
    "        acc_cnt += 1\n",
    "    new_distances.append([pos_dist,neg_dist])\n",
    "    \n",
    "print('training set accuracy: ',acc_cnt/luscinia_val_len*100, '%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "348/348 [==============================] - 171s 346ms/step - loss: 0.5265 - val_loss: 0.4591\n",
      "Epoch 2/1000\n",
      "348/348 [==============================] - 119s 342ms/step - loss: 0.4536 - val_loss: 0.4486\n",
      "Epoch 3/1000\n",
      "348/348 [==============================] - 119s 342ms/step - loss: 0.4392 - val_loss: 0.4454\n",
      "Epoch 4/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4334 - val_loss: 0.4415\n",
      "Epoch 5/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4304 - val_loss: 0.4401\n",
      "Epoch 6/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4283 - val_loss: 0.4373\n",
      "Epoch 7/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4273 - val_loss: 0.4341\n",
      "Epoch 8/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4250 - val_loss: 0.4324\n",
      "Epoch 9/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4237 - val_loss: 0.4305\n",
      "Epoch 10/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4225 - val_loss: 0.4294\n",
      "Epoch 11/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4211 - val_loss: 0.4287\n",
      "Epoch 12/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4205 - val_loss: 0.4278\n",
      "Epoch 13/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4192 - val_loss: 0.4261\n",
      "Epoch 14/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4182 - val_loss: 0.4254\n",
      "Epoch 15/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4169 - val_loss: 0.4238\n",
      "Epoch 16/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4155 - val_loss: 0.4229\n",
      "Epoch 17/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4143 - val_loss: 0.4208\n",
      "Epoch 18/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4128 - val_loss: 0.4198\n",
      "Epoch 19/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4117 - val_loss: 0.4165\n",
      "Epoch 20/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4103 - val_loss: 0.4160\n",
      "Epoch 21/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.4086 - val_loss: 0.4148\n",
      "Epoch 22/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4069 - val_loss: 0.4140\n",
      "Epoch 23/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4053 - val_loss: 0.4140\n",
      "Epoch 24/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4038 - val_loss: 0.4105\n",
      "Epoch 25/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4022 - val_loss: 0.4080\n",
      "Epoch 26/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.4006 - val_loss: 0.4055\n",
      "Epoch 27/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3990 - val_loss: 0.4061\n",
      "Epoch 28/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.3972 - val_loss: 0.4034\n",
      "Epoch 29/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3955 - val_loss: 0.4016\n",
      "Epoch 30/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3937 - val_loss: 0.3982\n",
      "Epoch 31/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3920 - val_loss: 0.3959\n",
      "Epoch 32/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3904 - val_loss: 0.3936\n",
      "Epoch 33/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3887 - val_loss: 0.3930\n",
      "Epoch 34/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3867 - val_loss: 0.3898\n",
      "Epoch 35/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3849 - val_loss: 0.3885\n",
      "Epoch 36/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3829 - val_loss: 0.3864\n",
      "Epoch 37/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3809 - val_loss: 0.3850\n",
      "Epoch 38/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3789 - val_loss: 0.3829\n",
      "Epoch 39/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3768 - val_loss: 0.3799\n",
      "Epoch 40/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3747 - val_loss: 0.3797\n",
      "Epoch 41/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3724 - val_loss: 0.3767\n",
      "Epoch 42/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3701 - val_loss: 0.3738\n",
      "Epoch 43/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3678 - val_loss: 0.3706\n",
      "Epoch 44/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3654 - val_loss: 0.3684\n",
      "Epoch 45/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3631 - val_loss: 0.3654\n",
      "Epoch 46/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3608 - val_loss: 0.3624\n",
      "Epoch 47/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3583 - val_loss: 0.3606\n",
      "Epoch 48/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3560 - val_loss: 0.3587\n",
      "Epoch 49/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3537 - val_loss: 0.3563\n",
      "Epoch 50/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3515 - val_loss: 0.3542\n",
      "Epoch 51/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3491 - val_loss: 0.3525\n",
      "Epoch 52/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3467 - val_loss: 0.3510\n",
      "Epoch 53/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3447 - val_loss: 0.3481\n",
      "Epoch 54/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3423 - val_loss: 0.3459\n",
      "Epoch 55/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3402 - val_loss: 0.3432\n",
      "Epoch 56/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3381 - val_loss: 0.3413\n",
      "Epoch 57/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3359 - val_loss: 0.3405\n",
      "Epoch 58/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3335 - val_loss: 0.3379\n",
      "Epoch 59/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3311 - val_loss: 0.3344\n",
      "Epoch 60/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3287 - val_loss: 0.3322\n",
      "Epoch 61/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3263 - val_loss: 0.3293\n",
      "Epoch 62/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3237 - val_loss: 0.3266\n",
      "Epoch 63/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3212 - val_loss: 0.3241\n",
      "Epoch 64/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3187 - val_loss: 0.3216\n",
      "Epoch 65/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3161 - val_loss: 0.3200\n",
      "Epoch 66/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3136 - val_loss: 0.3164\n",
      "Epoch 67/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3111 - val_loss: 0.3137\n",
      "Epoch 68/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3085 - val_loss: 0.3119\n",
      "Epoch 69/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3058 - val_loss: 0.3094\n",
      "Epoch 70/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3031 - val_loss: 0.3067\n",
      "Epoch 71/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.3005 - val_loss: 0.3051\n",
      "Epoch 72/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2979 - val_loss: 0.3013\n",
      "Epoch 73/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2955 - val_loss: 0.2980\n",
      "Epoch 74/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2933 - val_loss: 0.2962\n",
      "Epoch 75/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2914 - val_loss: 0.2943\n",
      "Epoch 76/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2894 - val_loss: 0.2921\n",
      "Epoch 77/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2873 - val_loss: 0.2896\n",
      "Epoch 78/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2853 - val_loss: 0.2877\n",
      "Epoch 79/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2832 - val_loss: 0.2846\n",
      "Epoch 80/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2810 - val_loss: 0.2818\n",
      "Epoch 81/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2789 - val_loss: 0.2802\n",
      "Epoch 82/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2768 - val_loss: 0.2783\n",
      "Epoch 83/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2747 - val_loss: 0.2765\n",
      "Epoch 84/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2724 - val_loss: 0.2739\n",
      "Epoch 85/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2701 - val_loss: 0.2720\n",
      "Epoch 86/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2677 - val_loss: 0.2700\n",
      "Epoch 87/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2655 - val_loss: 0.2678\n",
      "Epoch 88/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2633 - val_loss: 0.2673\n",
      "Epoch 89/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2613 - val_loss: 0.2636\n",
      "Epoch 90/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2593 - val_loss: 0.2633\n",
      "Epoch 91/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2574 - val_loss: 0.2602\n",
      "Epoch 92/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2555 - val_loss: 0.2584\n",
      "Epoch 93/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2538 - val_loss: 0.2567\n",
      "Epoch 94/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2522 - val_loss: 0.2559\n",
      "Epoch 95/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2506 - val_loss: 0.2518\n",
      "Epoch 96/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2490 - val_loss: 0.2513\n",
      "Epoch 97/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2473 - val_loss: 0.2494\n",
      "Epoch 98/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2455 - val_loss: 0.2474\n",
      "Epoch 99/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2436 - val_loss: 0.2456\n",
      "Epoch 100/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2418 - val_loss: 0.2454\n",
      "Epoch 101/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2400 - val_loss: 0.2430\n",
      "Epoch 102/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2382 - val_loss: 0.2421\n",
      "Epoch 103/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2365 - val_loss: 0.2400\n",
      "Epoch 104/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2346 - val_loss: 0.2379\n",
      "Epoch 105/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2326 - val_loss: 0.2356\n",
      "Epoch 106/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2305 - val_loss: 0.2336\n",
      "Epoch 107/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2284 - val_loss: 0.2311\n",
      "Epoch 108/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2262 - val_loss: 0.2286\n",
      "Epoch 109/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2241 - val_loss: 0.2276\n",
      "Epoch 110/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2220 - val_loss: 0.2255\n",
      "Epoch 111/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2200 - val_loss: 0.2241\n",
      "Epoch 112/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2179 - val_loss: 0.2212\n",
      "Epoch 113/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.2157 - val_loss: 0.2199\n",
      "Epoch 114/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2135 - val_loss: 0.2170\n",
      "Epoch 115/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2116 - val_loss: 0.2136\n",
      "Epoch 116/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2100 - val_loss: 0.2119\n",
      "Epoch 117/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.2084 - val_loss: 0.2109\n",
      "Epoch 118/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2070 - val_loss: 0.2103\n",
      "Epoch 119/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2056 - val_loss: 0.2089\n",
      "Epoch 120/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2041 - val_loss: 0.2081\n",
      "Epoch 121/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2027 - val_loss: 0.2075\n",
      "Epoch 122/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.2013 - val_loss: 0.2060\n",
      "Epoch 123/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1998 - val_loss: 0.2047\n",
      "Epoch 124/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1985 - val_loss: 0.2024\n",
      "Epoch 125/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1972 - val_loss: 0.2012\n",
      "Epoch 126/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1960 - val_loss: 0.1994\n",
      "Epoch 127/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1947 - val_loss: 0.1975\n",
      "Epoch 128/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1933 - val_loss: 0.1960\n",
      "Epoch 129/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1919 - val_loss: 0.1944\n",
      "Epoch 130/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1904 - val_loss: 0.1927\n",
      "Epoch 131/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1890 - val_loss: 0.1917\n",
      "Epoch 132/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1876 - val_loss: 0.1912\n",
      "Epoch 133/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1863 - val_loss: 0.1896\n",
      "Epoch 134/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1852 - val_loss: 0.1905\n",
      "Epoch 135/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1838 - val_loss: 0.1888\n",
      "Epoch 136/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1825 - val_loss: 0.1872\n",
      "Epoch 137/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1813 - val_loss: 0.1845\n",
      "Epoch 138/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1800 - val_loss: 0.1838\n",
      "Epoch 139/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1788 - val_loss: 0.1828\n",
      "Epoch 140/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1775 - val_loss: 0.1808\n",
      "Epoch 141/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1762 - val_loss: 0.1796\n",
      "Epoch 142/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1748 - val_loss: 0.1779\n",
      "Epoch 143/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1734 - val_loss: 0.1770\n",
      "Epoch 144/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1722 - val_loss: 0.1735\n",
      "Epoch 145/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1711 - val_loss: 0.1730\n",
      "Epoch 146/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1702 - val_loss: 0.1720\n",
      "Epoch 147/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1693 - val_loss: 0.1706\n",
      "Epoch 148/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1684 - val_loss: 0.1696\n",
      "Epoch 149/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1674 - val_loss: 0.1682\n",
      "Epoch 150/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1663 - val_loss: 0.1683\n",
      "Epoch 151/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1653 - val_loss: 0.1662\n",
      "Epoch 152/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1642 - val_loss: 0.1651\n",
      "Epoch 153/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1631 - val_loss: 0.1639\n",
      "Epoch 154/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1618 - val_loss: 0.1628\n",
      "Epoch 155/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1605 - val_loss: 0.1617\n",
      "Epoch 156/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1591 - val_loss: 0.1595\n",
      "Epoch 157/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1580 - val_loss: 0.1589\n",
      "Epoch 158/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1571 - val_loss: 0.1579\n",
      "Epoch 159/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1563 - val_loss: 0.1576\n",
      "Epoch 160/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1555 - val_loss: 0.1570\n",
      "Epoch 161/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1548 - val_loss: 0.1562\n",
      "Epoch 162/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1540 - val_loss: 0.1553\n",
      "Epoch 163/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1533 - val_loss: 0.1543\n",
      "Epoch 164/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1525 - val_loss: 0.1545\n",
      "Epoch 165/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1516 - val_loss: 0.1530\n",
      "Epoch 166/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1507 - val_loss: 0.1523\n",
      "Epoch 167/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1498 - val_loss: 0.1518\n",
      "Epoch 168/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1488 - val_loss: 0.1523\n",
      "Epoch 169/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1480 - val_loss: 0.1506\n",
      "Epoch 170/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1471 - val_loss: 0.1496\n",
      "Epoch 171/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1463 - val_loss: 0.1492\n",
      "Epoch 172/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1455 - val_loss: 0.1484\n",
      "Epoch 173/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1447 - val_loss: 0.1465\n",
      "Epoch 174/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1438 - val_loss: 0.1452\n",
      "Epoch 175/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1429 - val_loss: 0.1441\n",
      "Epoch 176/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1419 - val_loss: 0.1438\n",
      "Epoch 177/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1410 - val_loss: 0.1430\n",
      "Epoch 178/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1400 - val_loss: 0.1419\n",
      "Epoch 179/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1390 - val_loss: 0.1404\n",
      "Epoch 180/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1380 - val_loss: 0.1395\n",
      "Epoch 181/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1370 - val_loss: 0.1389\n",
      "Epoch 182/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1361 - val_loss: 0.1386\n",
      "Epoch 183/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1353 - val_loss: 0.1382\n",
      "Epoch 184/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1345 - val_loss: 0.1367\n",
      "Epoch 185/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1338 - val_loss: 0.1359\n",
      "Epoch 186/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1331 - val_loss: 0.1347\n",
      "Epoch 187/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1324 - val_loss: 0.1338\n",
      "Epoch 188/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1318 - val_loss: 0.1332\n",
      "Epoch 189/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1311 - val_loss: 0.1325\n",
      "Epoch 190/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1305 - val_loss: 0.1324\n",
      "Epoch 191/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1299 - val_loss: 0.1320\n",
      "Epoch 192/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1292 - val_loss: 0.1320\n",
      "Epoch 193/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1285 - val_loss: 0.1299\n",
      "Epoch 194/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1278 - val_loss: 0.1300\n",
      "Epoch 195/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1272 - val_loss: 0.1293\n",
      "Epoch 196/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1265 - val_loss: 0.1293\n",
      "Epoch 197/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1259 - val_loss: 0.1283\n",
      "Epoch 198/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1254 - val_loss: 0.1269\n",
      "Epoch 199/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1248 - val_loss: 0.1266\n",
      "Epoch 200/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1243 - val_loss: 0.1260\n",
      "Epoch 201/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1237 - val_loss: 0.1254\n",
      "Epoch 202/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1231 - val_loss: 0.1252\n",
      "Epoch 203/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1224 - val_loss: 0.1243\n",
      "Epoch 204/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1217 - val_loss: 0.1238\n",
      "Epoch 205/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1210 - val_loss: 0.1230\n",
      "Epoch 206/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1202 - val_loss: 0.1222\n",
      "Epoch 207/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1194 - val_loss: 0.1212\n",
      "Epoch 208/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1187 - val_loss: 0.1209\n",
      "Epoch 209/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1178 - val_loss: 0.1202\n",
      "Epoch 210/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1170 - val_loss: 0.1195\n",
      "Epoch 211/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1162 - val_loss: 0.1183\n",
      "Epoch 212/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1155 - val_loss: 0.1176\n",
      "Epoch 213/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1149 - val_loss: 0.1167\n",
      "Epoch 214/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1143 - val_loss: 0.1159\n",
      "Epoch 215/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1138 - val_loss: 0.1165\n",
      "Epoch 216/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1133 - val_loss: 0.1156\n",
      "Epoch 217/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1127 - val_loss: 0.1157\n",
      "Epoch 218/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1123 - val_loss: 0.1137\n",
      "Epoch 219/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1116 - val_loss: 0.1133\n",
      "Epoch 220/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1111 - val_loss: 0.1122\n",
      "Epoch 221/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1105 - val_loss: 0.1115\n",
      "Epoch 222/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1099 - val_loss: 0.1106\n",
      "Epoch 223/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1092 - val_loss: 0.1103\n",
      "Epoch 224/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1086 - val_loss: 0.1095\n",
      "Epoch 225/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.1081 - val_loss: 0.1090\n",
      "Epoch 226/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1075 - val_loss: 0.1084\n",
      "Epoch 227/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1069 - val_loss: 0.1081\n",
      "Epoch 228/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1063 - val_loss: 0.1075\n",
      "Epoch 229/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1057 - val_loss: 0.1077\n",
      "Epoch 230/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1050 - val_loss: 0.1067\n",
      "Epoch 231/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1044 - val_loss: 0.1061\n",
      "Epoch 232/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1037 - val_loss: 0.1054\n",
      "Epoch 233/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1031 - val_loss: 0.1045\n",
      "Epoch 234/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1025 - val_loss: 0.1038\n",
      "Epoch 235/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1018 - val_loss: 0.1032\n",
      "Epoch 236/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.1011 - val_loss: 0.1025\n",
      "Epoch 237/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.1004 - val_loss: 0.1018\n",
      "Epoch 238/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0997 - val_loss: 0.1012\n",
      "Epoch 239/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0991 - val_loss: 0.1014\n",
      "Epoch 240/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0986 - val_loss: 0.1004\n",
      "Epoch 241/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0981 - val_loss: 0.1000\n",
      "Epoch 242/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0977 - val_loss: 0.0995\n",
      "Epoch 243/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0972 - val_loss: 0.1002\n",
      "Epoch 244/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0968 - val_loss: 0.0994\n",
      "Epoch 245/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0964 - val_loss: 0.0992\n",
      "Epoch 246/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0961 - val_loss: 0.0987\n",
      "Epoch 247/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0957 - val_loss: 0.0981\n",
      "Epoch 248/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0953 - val_loss: 0.0975\n",
      "Epoch 249/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0949 - val_loss: 0.0968\n",
      "Epoch 250/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0944 - val_loss: 0.0963\n",
      "Epoch 251/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0940 - val_loss: 0.0957\n",
      "Epoch 252/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0935 - val_loss: 0.0951\n",
      "Epoch 253/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0931 - val_loss: 0.0955\n",
      "Epoch 254/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0927 - val_loss: 0.0946\n",
      "Epoch 255/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0924 - val_loss: 0.0938\n",
      "Epoch 256/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0920 - val_loss: 0.0937\n",
      "Epoch 257/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0916 - val_loss: 0.0933\n",
      "Epoch 258/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0912 - val_loss: 0.0934\n",
      "Epoch 259/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0909 - val_loss: 0.0930\n",
      "Epoch 260/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0905 - val_loss: 0.0923\n",
      "Epoch 261/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0901 - val_loss: 0.0919\n",
      "Epoch 262/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0897 - val_loss: 0.0919\n",
      "Epoch 263/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0894 - val_loss: 0.0922\n",
      "Epoch 264/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0890 - val_loss: 0.0919\n",
      "Epoch 265/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0887 - val_loss: 0.0912\n",
      "Epoch 266/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0883 - val_loss: 0.0908\n",
      "Epoch 267/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0879 - val_loss: 0.0902\n",
      "Epoch 268/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0875 - val_loss: 0.0898\n",
      "Epoch 269/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0871 - val_loss: 0.0898\n",
      "Epoch 270/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0866 - val_loss: 0.0897\n",
      "Epoch 271/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0862 - val_loss: 0.0892\n",
      "Epoch 272/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0857 - val_loss: 0.0887\n",
      "Epoch 273/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0853 - val_loss: 0.0886\n",
      "Epoch 274/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0848 - val_loss: 0.0887\n",
      "Epoch 275/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0843 - val_loss: 0.0889\n",
      "Epoch 276/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0838 - val_loss: 0.0890\n",
      "Epoch 277/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0833 - val_loss: 0.0863\n",
      "Epoch 278/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0828 - val_loss: 0.0859\n",
      "Epoch 279/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0824 - val_loss: 0.0853\n",
      "Epoch 280/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0819 - val_loss: 0.0846\n",
      "Epoch 281/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0814 - val_loss: 0.0842\n",
      "Epoch 282/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0811 - val_loss: 0.0844\n",
      "Epoch 283/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0807 - val_loss: 0.0843\n",
      "Epoch 284/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0803 - val_loss: 0.0839\n",
      "Epoch 285/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0799 - val_loss: 0.0836\n",
      "Epoch 286/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0795 - val_loss: 0.0829\n",
      "Epoch 287/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0791 - val_loss: 0.0827\n",
      "Epoch 288/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0787 - val_loss: 0.0825\n",
      "Epoch 289/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0783 - val_loss: 0.0822\n",
      "Epoch 290/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0779 - val_loss: 0.0817\n",
      "Epoch 291/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0774 - val_loss: 0.0821\n",
      "Epoch 292/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0771 - val_loss: 0.0802\n",
      "Epoch 293/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0766 - val_loss: 0.0801\n",
      "Epoch 294/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0763 - val_loss: 0.0797\n",
      "Epoch 295/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0760 - val_loss: 0.0794\n",
      "Epoch 296/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0756 - val_loss: 0.0791\n",
      "Epoch 297/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0753 - val_loss: 0.0782\n",
      "Epoch 298/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0749 - val_loss: 0.0778\n",
      "Epoch 299/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0746 - val_loss: 0.0774\n",
      "Epoch 300/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0743 - val_loss: 0.0770\n",
      "Epoch 301/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0739 - val_loss: 0.0766\n",
      "Epoch 302/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0735 - val_loss: 0.0762\n",
      "Epoch 303/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0730 - val_loss: 0.0757\n",
      "Epoch 304/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0726 - val_loss: 0.0750\n",
      "Epoch 305/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0721 - val_loss: 0.0745\n",
      "Epoch 306/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0716 - val_loss: 0.0740\n",
      "Epoch 307/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0712 - val_loss: 0.0734\n",
      "Epoch 308/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0708 - val_loss: 0.0727\n",
      "Epoch 309/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0706 - val_loss: 0.0724\n",
      "Epoch 310/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0702 - val_loss: 0.0719\n",
      "Epoch 311/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0699 - val_loss: 0.0715\n",
      "Epoch 312/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0696 - val_loss: 0.0712\n",
      "Epoch 313/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0692 - val_loss: 0.0719\n",
      "Epoch 314/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0690 - val_loss: 0.0715\n",
      "Epoch 315/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0686 - val_loss: 0.0710\n",
      "Epoch 316/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0683 - val_loss: 0.0704\n",
      "Epoch 317/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0680 - val_loss: 0.0703\n",
      "Epoch 318/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0677 - val_loss: 0.0702\n",
      "Epoch 319/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0674 - val_loss: 0.0698\n",
      "Epoch 320/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0670 - val_loss: 0.0695\n",
      "Epoch 321/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0667 - val_loss: 0.0688\n",
      "Epoch 322/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0664 - val_loss: 0.0693\n",
      "Epoch 323/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0662 - val_loss: 0.0693\n",
      "Epoch 324/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0660 - val_loss: 0.0689\n",
      "Epoch 325/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0658 - val_loss: 0.0686\n",
      "Epoch 326/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0656 - val_loss: 0.0691\n",
      "Epoch 327/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0654 - val_loss: 0.0696\n",
      "Epoch 328/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0652 - val_loss: 0.0693\n",
      "Epoch 329/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0650 - val_loss: 0.0685\n",
      "Epoch 330/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0647 - val_loss: 0.0682\n",
      "Epoch 331/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0644 - val_loss: 0.0689\n",
      "Epoch 332/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0641 - val_loss: 0.0688\n",
      "Epoch 333/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0639 - val_loss: 0.0672\n",
      "Epoch 334/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0636 - val_loss: 0.0672\n",
      "Epoch 335/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0633 - val_loss: 0.0674\n",
      "Epoch 336/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0630 - val_loss: 0.0666\n",
      "Epoch 337/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0627 - val_loss: 0.0668\n",
      "Epoch 338/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0624 - val_loss: 0.0657\n",
      "Epoch 339/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0621 - val_loss: 0.0659\n",
      "Epoch 340/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0618 - val_loss: 0.0656\n",
      "Epoch 341/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0615 - val_loss: 0.0659\n",
      "Epoch 342/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0612 - val_loss: 0.0651\n",
      "Epoch 343/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0608 - val_loss: 0.0645\n",
      "Epoch 344/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0605 - val_loss: 0.0660\n",
      "Epoch 345/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0602 - val_loss: 0.0640\n",
      "Epoch 346/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0598 - val_loss: 0.0637\n",
      "Epoch 347/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0595 - val_loss: 0.0635\n",
      "Epoch 348/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0593 - val_loss: 0.0639\n",
      "Epoch 349/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0591 - val_loss: 0.0638\n",
      "Epoch 350/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0589 - val_loss: 0.0636\n",
      "Epoch 351/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0587 - val_loss: 0.0635\n",
      "Epoch 352/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0584 - val_loss: 0.0632\n",
      "Epoch 353/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0581 - val_loss: 0.0629\n",
      "Epoch 354/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0578 - val_loss: 0.0628\n",
      "Epoch 355/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0575 - val_loss: 0.0620\n",
      "Epoch 356/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0573 - val_loss: 0.0618\n",
      "Epoch 357/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0571 - val_loss: 0.0619\n",
      "Epoch 358/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0570 - val_loss: 0.0613\n",
      "Epoch 359/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0569 - val_loss: 0.0615\n",
      "Epoch 360/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0566 - val_loss: 0.0615\n",
      "Epoch 361/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0565 - val_loss: 0.0619\n",
      "Epoch 362/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0563 - val_loss: 0.0614\n",
      "Epoch 363/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0561 - val_loss: 0.0616\n",
      "Epoch 364/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0558 - val_loss: 0.0613\n",
      "Epoch 365/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0556 - val_loss: 0.0610\n",
      "Epoch 366/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0553 - val_loss: 0.0605\n",
      "Epoch 367/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0551 - val_loss: 0.0597\n",
      "Epoch 368/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0548 - val_loss: 0.0593\n",
      "Epoch 369/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0547 - val_loss: 0.0594\n",
      "Epoch 370/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0546 - val_loss: 0.0585\n",
      "Epoch 371/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0544 - val_loss: 0.0581\n",
      "Epoch 372/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0543 - val_loss: 0.0579\n",
      "Epoch 373/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0541 - val_loss: 0.0578\n",
      "Epoch 374/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0539 - val_loss: 0.0584\n",
      "Epoch 375/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0538 - val_loss: 0.0578\n",
      "Epoch 376/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0537 - val_loss: 0.0576\n",
      "Epoch 377/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0535 - val_loss: 0.0572\n",
      "Epoch 378/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0534 - val_loss: 0.0572\n",
      "Epoch 379/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0532 - val_loss: 0.0571\n",
      "Epoch 380/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0530 - val_loss: 0.0570\n",
      "Epoch 381/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0529 - val_loss: 0.0568\n",
      "Epoch 382/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0527 - val_loss: 0.0565\n",
      "Epoch 383/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0524 - val_loss: 0.0563\n",
      "Epoch 384/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0522 - val_loss: 0.0578\n",
      "Epoch 385/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0519 - val_loss: 0.0562\n",
      "Epoch 386/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0518 - val_loss: 0.0548\n",
      "Epoch 387/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0516 - val_loss: 0.0543\n",
      "Epoch 388/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0515 - val_loss: 0.0543\n",
      "Epoch 389/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0514 - val_loss: 0.0542\n",
      "Epoch 390/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0512 - val_loss: 0.0541\n",
      "Epoch 391/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0511 - val_loss: 0.0547\n",
      "Epoch 392/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0509 - val_loss: 0.0546\n",
      "Epoch 393/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0508 - val_loss: 0.0545\n",
      "Epoch 394/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0506 - val_loss: 0.0547\n",
      "Epoch 395/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0504 - val_loss: 0.0549\n",
      "Epoch 396/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0503 - val_loss: 0.0544\n",
      "Epoch 397/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0501 - val_loss: 0.0550\n",
      "Epoch 398/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0500 - val_loss: 0.0545\n",
      "Epoch 399/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0499 - val_loss: 0.0542\n",
      "\n",
      "Epoch 00399: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-07.\n",
      "Epoch 400/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0497 - val_loss: 0.0541\n",
      "Epoch 401/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0497 - val_loss: 0.0534\n",
      "Epoch 402/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0496 - val_loss: 0.0535\n",
      "Epoch 403/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0496 - val_loss: 0.0538\n",
      "Epoch 404/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0495 - val_loss: 0.0537\n",
      "Epoch 405/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0494 - val_loss: 0.0537\n",
      "Epoch 406/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0493 - val_loss: 0.0536\n",
      "Epoch 407/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0492 - val_loss: 0.0527\n",
      "Epoch 408/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0491 - val_loss: 0.0526\n",
      "Epoch 409/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0491 - val_loss: 0.0524\n",
      "Epoch 410/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0490 - val_loss: 0.0523\n",
      "Epoch 411/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0489 - val_loss: 0.0525\n",
      "Epoch 412/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0488 - val_loss: 0.0524\n",
      "Epoch 413/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0486 - val_loss: 0.0525\n",
      "Epoch 414/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0485 - val_loss: 0.0524\n",
      "Epoch 415/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0483 - val_loss: 0.0524\n",
      "Epoch 416/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0482 - val_loss: 0.0531\n",
      "Epoch 417/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0480 - val_loss: 0.0515\n",
      "Epoch 418/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0479 - val_loss: 0.0509\n",
      "Epoch 419/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0478 - val_loss: 0.0508\n",
      "Epoch 420/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0477 - val_loss: 0.0506\n",
      "Epoch 421/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0476 - val_loss: 0.0504\n",
      "Epoch 422/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0475 - val_loss: 0.0503\n",
      "Epoch 423/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0474 - val_loss: 0.0502\n",
      "Epoch 424/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0473 - val_loss: 0.0501\n",
      "Epoch 425/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0471 - val_loss: 0.0499\n",
      "Epoch 426/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0470 - val_loss: 0.0497\n",
      "Epoch 427/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0468 - val_loss: 0.0497\n",
      "Epoch 428/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0467 - val_loss: 0.0492\n",
      "Epoch 429/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0466 - val_loss: 0.0492\n",
      "Epoch 430/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0464 - val_loss: 0.0492\n",
      "Epoch 431/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0463 - val_loss: 0.0491\n",
      "Epoch 432/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0462 - val_loss: 0.0490\n",
      "Epoch 433/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0460 - val_loss: 0.0491\n",
      "Epoch 434/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0459 - val_loss: 0.0489\n",
      "Epoch 435/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0458 - val_loss: 0.0490\n",
      "Epoch 436/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0457 - val_loss: 0.0490\n",
      "Epoch 437/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0456 - val_loss: 0.0489\n",
      "Epoch 438/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0455 - val_loss: 0.0488\n",
      "Epoch 439/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0454 - val_loss: 0.0486\n",
      "Epoch 440/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0452 - val_loss: 0.0485\n",
      "Epoch 441/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0450 - val_loss: 0.0483\n",
      "Epoch 442/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0449 - val_loss: 0.0481\n",
      "Epoch 443/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0446 - val_loss: 0.0480\n",
      "Epoch 444/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0444 - val_loss: 0.0480\n",
      "Epoch 445/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0443 - val_loss: 0.0479\n",
      "Epoch 446/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0441 - val_loss: 0.0477\n",
      "Epoch 447/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0440 - val_loss: 0.0480\n",
      "Epoch 448/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0439 - val_loss: 0.0465\n",
      "Epoch 449/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0438 - val_loss: 0.0464\n",
      "Epoch 450/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0437 - val_loss: 0.0465\n",
      "Epoch 451/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0437 - val_loss: 0.0463\n",
      "Epoch 452/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0436 - val_loss: 0.0460\n",
      "Epoch 453/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0436 - val_loss: 0.0461\n",
      "Epoch 454/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0435 - val_loss: 0.0462\n",
      "Epoch 455/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0435 - val_loss: 0.0466\n",
      "Epoch 456/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0434 - val_loss: 0.0465\n",
      "Epoch 457/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0433 - val_loss: 0.0465\n",
      "Epoch 458/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0433 - val_loss: 0.0464\n",
      "Epoch 459/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0432 - val_loss: 0.0463\n",
      "Epoch 460/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0431 - val_loss: 0.0462\n",
      "Epoch 461/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0430 - val_loss: 0.0458\n",
      "Epoch 462/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0429 - val_loss: 0.0457\n",
      "Epoch 463/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0428 - val_loss: 0.0456\n",
      "Epoch 464/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0428 - val_loss: 0.0455\n",
      "Epoch 465/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0427 - val_loss: 0.0454\n",
      "Epoch 466/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0426 - val_loss: 0.0454\n",
      "Epoch 467/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0426 - val_loss: 0.0454\n",
      "Epoch 468/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0425 - val_loss: 0.0451\n",
      "Epoch 469/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0425 - val_loss: 0.0453\n",
      "Epoch 470/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0424 - val_loss: 0.0452\n",
      "Epoch 471/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0424 - val_loss: 0.0451\n",
      "Epoch 472/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0423 - val_loss: 0.0450\n",
      "Epoch 473/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0422 - val_loss: 0.0445\n",
      "Epoch 474/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0421 - val_loss: 0.0445\n",
      "Epoch 475/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0420 - val_loss: 0.0444\n",
      "Epoch 476/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0419 - val_loss: 0.0444\n",
      "Epoch 477/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0418 - val_loss: 0.0443\n",
      "Epoch 478/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0417 - val_loss: 0.0444\n",
      "Epoch 479/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0416 - val_loss: 0.0447\n",
      "Epoch 480/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0416 - val_loss: 0.0447\n",
      "Epoch 481/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0415 - val_loss: 0.0447\n",
      "Epoch 482/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0415 - val_loss: 0.0447\n",
      "Epoch 483/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0414 - val_loss: 0.0447\n",
      "Epoch 484/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0414 - val_loss: 0.0449\n",
      "Epoch 485/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0413 - val_loss: 0.0448\n",
      "Epoch 486/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0413 - val_loss: 0.0443\n",
      "Epoch 487/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0412 - val_loss: 0.0443\n",
      "\n",
      "Epoch 00487: ReduceLROnPlateau reducing learning rate to 2.499999993688107e-07.\n",
      "Epoch 488/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0412 - val_loss: 0.0440\n",
      "Epoch 489/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0411 - val_loss: 0.0442\n",
      "Epoch 490/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0411 - val_loss: 0.0441\n",
      "Epoch 491/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0411 - val_loss: 0.0442\n",
      "Epoch 492/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0410 - val_loss: 0.0442\n",
      "Epoch 493/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0410 - val_loss: 0.0442\n",
      "Epoch 494/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0410 - val_loss: 0.0442\n",
      "Epoch 495/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0409 - val_loss: 0.0441\n",
      "Epoch 496/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0409 - val_loss: 0.0444\n",
      "Epoch 497/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0408 - val_loss: 0.0444\n",
      "Epoch 498/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0408 - val_loss: 0.0444\n",
      "\n",
      "Epoch 00498: ReduceLROnPlateau reducing learning rate to 1.2499999968440534e-07.\n",
      "Epoch 499/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0443\n",
      "Epoch 500/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0442\n",
      "Epoch 501/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0441\n",
      "Epoch 502/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0407 - val_loss: 0.0440\n",
      "Epoch 503/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0406 - val_loss: 0.0440\n",
      "Epoch 504/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0406 - val_loss: 0.0440\n",
      "Epoch 505/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0406 - val_loss: 0.0439\n",
      "Epoch 506/1000\n",
      "348/348 [==============================] - 118s 340ms/step - loss: 0.0405 - val_loss: 0.0439\n",
      "Epoch 507/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0405 - val_loss: 0.0438\n",
      "Epoch 508/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0404 - val_loss: 0.0438\n",
      "Epoch 509/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0404 - val_loss: 0.0437\n",
      "Epoch 510/1000\n",
      "348/348 [==============================] - 118s 341ms/step - loss: 0.0403 - val_loss: 0.0431\n",
      "Epoch 511/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0403 - val_loss: 0.0430\n",
      "Epoch 512/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0402 - val_loss: 0.0430\n",
      "Epoch 513/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0402 - val_loss: 0.0430\n",
      "Epoch 514/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0402 - val_loss: 0.0429\n",
      "Epoch 515/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0401 - val_loss: 0.0429\n",
      "Epoch 516/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0401 - val_loss: 0.0429\n",
      "Epoch 517/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0400 - val_loss: 0.0429\n",
      "Epoch 518/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0400 - val_loss: 0.0430\n",
      "Epoch 519/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0400 - val_loss: 0.0430\n",
      "Epoch 520/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0430\n",
      "Epoch 521/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0430\n",
      "Epoch 522/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0432\n",
      "Epoch 523/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0399 - val_loss: 0.0431\n",
      "\n",
      "Epoch 00523: ReduceLROnPlateau reducing learning rate to 6.249999984220267e-08.\n",
      "Epoch 524/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 525/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 526/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 527/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 528/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 529/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 530/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 531/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "Epoch 532/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "Epoch 533/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00533: ReduceLROnPlateau reducing learning rate to 3.1249999921101335e-08.\n",
      "Epoch 534/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0430\n",
      "Epoch 535/1000\n",
      "348/348 [==============================] - 119s 341ms/step - loss: 0.0397 - val_loss: 0.0429\n",
      "Epoch 00535: early stopping\n"
     ]
    }
   ],
   "source": [
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu #24\n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_backup.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "#dis_tr_triplets = discard_some_low(training_triplets, 0.7, 0.7)\n",
    "#dis_val_triplets = discard_some_low(validation_triplets, 0.7, 0.7)\n",
    "\n",
    "#low_margin, high_margin, bal_training_triplets = balance_input(dis_tr_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "#vlow_margin, vhigh_margin, bal_val_triplets = balance_input(dis_val_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "\n",
    "history = triplet_model.fit(train_generator_luscinia(luscinia_triplets[:int(luscinia_train_len/10)], M_l, S_l, batchsize, emb_size, path_mel),\n",
    "                           steps_per_epoch=int(int(luscinia_train_len/10)/batchsize), epochs=1000, verbose=1,\n",
    "                           validation_data=train_generator_luscinia(luscinia_triplets[luscinia_train_len:luscinia_train_len+200], M_l, S_l, batchsize, emb_size, path_mel),\n",
    "                           validation_steps=int(200/batchsize), callbacks=[cpCallback, reduce_lr, earlystop])\n",
    "#history = triplet_model.fit(train_generator_mixed(bal_training_triplets, M, S, luscinia_triplets[:luscinia_train_len],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    " #                               steps_per_epoch=int(len(bal_training_triplets)/(lo+hi)), epochs=1000, verbose=1,\n",
    "  #                              validation_data=train_generator_mixed(bal_val_triplets, M, S, luscinia_triplets[luscinia_train_len:],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "   #                             validation_steps=int(len(bal_val_triplets)/(lo+hi)), callbacks=[cpCallback, reduce_lr, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.load_weights('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_backup.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "58/58 [==============================] - 28s 394ms/step - loss: 0.1178 - val_loss: 0.1106\n",
      "Epoch 2/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1201 - val_loss: 0.1079\n",
      "Epoch 3/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1196 - val_loss: 0.1073\n",
      "Epoch 4/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1166 - val_loss: 0.1070\n",
      "Epoch 5/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1183 - val_loss: 0.1068\n",
      "Epoch 6/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1158 - val_loss: 0.1066\n",
      "Epoch 7/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1166 - val_loss: 0.1064\n",
      "Epoch 8/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1157 - val_loss: 0.1062\n",
      "Epoch 9/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1146 - val_loss: 0.1060\n",
      "Epoch 10/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1154 - val_loss: 0.1058\n",
      "Epoch 11/1000\n",
      "58/58 [==============================] - 22s 382ms/step - loss: 0.1144 - val_loss: 0.1056\n",
      "Epoch 12/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1155 - val_loss: 0.1054\n",
      "Epoch 13/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1149 - val_loss: 0.1053\n",
      "Epoch 14/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1144 - val_loss: 0.1051\n",
      "Epoch 15/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1145 - val_loss: 0.1049\n",
      "Epoch 16/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1145 - val_loss: 0.1048\n",
      "Epoch 17/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1156 - val_loss: 0.1046\n",
      "Epoch 18/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1148 - val_loss: 0.1044\n",
      "Epoch 19/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1123 - val_loss: 0.1043\n",
      "Epoch 20/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1138 - val_loss: 0.1041\n",
      "Epoch 21/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1109 - val_loss: 0.1040\n",
      "Epoch 22/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1112 - val_loss: 0.1038\n",
      "Epoch 23/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1100 - val_loss: 0.1037\n",
      "Epoch 24/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1109 - val_loss: 0.1036\n",
      "Epoch 25/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1115 - val_loss: 0.1034\n",
      "Epoch 26/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1109 - val_loss: 0.1033\n",
      "Epoch 27/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1100 - val_loss: 0.1032\n",
      "Epoch 28/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1115 - val_loss: 0.1030\n",
      "Epoch 29/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1100 - val_loss: 0.1029\n",
      "Epoch 30/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1113 - val_loss: 0.1028\n",
      "Epoch 31/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1081 - val_loss: 0.1026\n",
      "Epoch 32/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1111 - val_loss: 0.1025\n",
      "Epoch 33/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1086 - val_loss: 0.1024\n",
      "Epoch 34/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1096 - val_loss: 0.1023\n",
      "Epoch 35/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1096 - val_loss: 0.1022\n",
      "Epoch 36/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1074 - val_loss: 0.1021\n",
      "Epoch 37/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1076 - val_loss: 0.1020\n",
      "Epoch 38/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1099 - val_loss: 0.1019\n",
      "Epoch 39/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1074 - val_loss: 0.1018\n",
      "Epoch 40/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1095 - val_loss: 0.1017\n",
      "Epoch 41/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1076 - val_loss: 0.1016\n",
      "Epoch 42/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1067 - val_loss: 0.1015\n",
      "Epoch 43/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1087 - val_loss: 0.1014\n",
      "Epoch 44/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1061 - val_loss: 0.1013\n",
      "Epoch 45/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1074 - val_loss: 0.1013\n",
      "Epoch 46/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1080 - val_loss: 0.1012\n",
      "Epoch 47/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1068 - val_loss: 0.1011\n",
      "Epoch 48/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1071 - val_loss: 0.1010\n",
      "Epoch 49/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1058 - val_loss: 0.1009\n",
      "Epoch 50/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1061 - val_loss: 0.1008\n",
      "Epoch 51/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1072 - val_loss: 0.1008\n",
      "Epoch 52/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1070 - val_loss: 0.1007\n",
      "Epoch 53/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1077 - val_loss: 0.1006\n",
      "Epoch 54/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1065 - val_loss: 0.1005\n",
      "Epoch 55/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1061 - val_loss: 0.1005\n",
      "Epoch 56/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1070 - val_loss: 0.1004\n",
      "Epoch 57/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1057 - val_loss: 0.1003\n",
      "Epoch 58/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1057 - val_loss: 0.1003\n",
      "Epoch 59/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1042 - val_loss: 0.1002\n",
      "Epoch 60/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1059 - val_loss: 0.1001\n",
      "Epoch 61/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1049 - val_loss: 0.1001\n",
      "Epoch 62/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1052 - val_loss: 0.1000\n",
      "Epoch 63/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1046 - val_loss: 0.1000\n",
      "Epoch 64/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1035 - val_loss: 0.0999\n",
      "Epoch 65/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1027 - val_loss: 0.0998\n",
      "Epoch 66/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1037 - val_loss: 0.0998\n",
      "Epoch 67/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1054 - val_loss: 0.0997\n",
      "Epoch 68/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0997\n",
      "Epoch 69/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1045 - val_loss: 0.0996\n",
      "Epoch 70/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1042 - val_loss: 0.0996\n",
      "Epoch 71/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1043 - val_loss: 0.0995\n",
      "Epoch 72/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.1029 - val_loss: 0.0995\n",
      "Epoch 73/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1036 - val_loss: 0.0994\n",
      "Epoch 74/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1025 - val_loss: 0.0994\n",
      "Epoch 75/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1024 - val_loss: 0.0994\n",
      "Epoch 76/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1038 - val_loss: 0.0993\n",
      "Epoch 77/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0993\n",
      "Epoch 78/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1022 - val_loss: 0.0992\n",
      "Epoch 79/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1040 - val_loss: 0.0992\n",
      "Epoch 80/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1027 - val_loss: 0.0991\n",
      "Epoch 81/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1037 - val_loss: 0.0991\n",
      "Epoch 82/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1027 - val_loss: 0.0991\n",
      "Epoch 83/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0990\n",
      "Epoch 84/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1030 - val_loss: 0.0990\n",
      "Epoch 85/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1025 - val_loss: 0.0990\n",
      "Epoch 86/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1031 - val_loss: 0.0989\n",
      "Epoch 87/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1026 - val_loss: 0.0989\n",
      "Epoch 88/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1033 - val_loss: 0.0989\n",
      "Epoch 89/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1024 - val_loss: 0.0988\n",
      "Epoch 90/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1021 - val_loss: 0.0988\n",
      "Epoch 91/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1021 - val_loss: 0.0988\n",
      "Epoch 92/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0987\n",
      "Epoch 93/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1022 - val_loss: 0.0987\n",
      "Epoch 94/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1011 - val_loss: 0.0987\n",
      "Epoch 95/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1014 - val_loss: 0.0987\n",
      "Epoch 96/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1029 - val_loss: 0.0986\n",
      "Epoch 97/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1013 - val_loss: 0.0986\n",
      "Epoch 98/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1015 - val_loss: 0.0986\n",
      "Epoch 99/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1007 - val_loss: 0.0986\n",
      "Epoch 100/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1017 - val_loss: 0.0985\n",
      "Epoch 101/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1016 - val_loss: 0.0985\n",
      "Epoch 102/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1021 - val_loss: 0.0985\n",
      "Epoch 103/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1010 - val_loss: 0.0985\n",
      "Epoch 104/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1006 - val_loss: 0.0984\n",
      "Epoch 105/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0984\n",
      "Epoch 106/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1014 - val_loss: 0.0984\n",
      "Epoch 107/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1006 - val_loss: 0.0984\n",
      "Epoch 108/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1009 - val_loss: 0.0983\n",
      "Epoch 109/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1006 - val_loss: 0.0983\n",
      "Epoch 110/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1008 - val_loss: 0.0983\n",
      "Epoch 111/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0997 - val_loss: 0.0983\n",
      "Epoch 112/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0998 - val_loss: 0.0983\n",
      "Epoch 113/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1008 - val_loss: 0.0983\n",
      "Epoch 114/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0982\n",
      "Epoch 115/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1017 - val_loss: 0.0982\n",
      "Epoch 116/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1003 - val_loss: 0.0982\n",
      "Epoch 117/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1007 - val_loss: 0.0982\n",
      "Epoch 118/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1003 - val_loss: 0.0981\n",
      "Epoch 119/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1004 - val_loss: 0.0981\n",
      "Epoch 120/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0989 - val_loss: 0.0981\n",
      "Epoch 121/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0983 - val_loss: 0.0981\n",
      "Epoch 122/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1003 - val_loss: 0.0981\n",
      "Epoch 123/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1001 - val_loss: 0.0981\n",
      "Epoch 124/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0995 - val_loss: 0.0981\n",
      "Epoch 125/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1001 - val_loss: 0.0980\n",
      "Epoch 126/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0980\n",
      "Epoch 127/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0980 - val_loss: 0.0980\n",
      "Epoch 128/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0980\n",
      "Epoch 129/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1013 - val_loss: 0.0980\n",
      "Epoch 130/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0980\n",
      "Epoch 131/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0979\n",
      "Epoch 132/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0995 - val_loss: 0.0979\n",
      "Epoch 133/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.1007 - val_loss: 0.0979\n",
      "Epoch 134/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0996 - val_loss: 0.0979\n",
      "Epoch 135/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0979\n",
      "Epoch 136/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0979\n",
      "Epoch 137/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0986 - val_loss: 0.0979\n",
      "Epoch 138/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1003 - val_loss: 0.0979\n",
      "Epoch 139/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0996 - val_loss: 0.0979\n",
      "Epoch 140/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1004 - val_loss: 0.0978\n",
      "Epoch 141/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0978\n",
      "Epoch 142/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0987 - val_loss: 0.0978\n",
      "Epoch 143/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0992 - val_loss: 0.0978\n",
      "Epoch 144/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0978\n",
      "Epoch 145/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 146/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1002 - val_loss: 0.0978\n",
      "Epoch 147/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 148/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0983 - val_loss: 0.0978\n",
      "Epoch 149/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1006 - val_loss: 0.0978\n",
      "Epoch 150/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0977\n",
      "Epoch 151/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1002 - val_loss: 0.0977\n",
      "Epoch 152/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0988 - val_loss: 0.0977\n",
      "Epoch 153/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1004 - val_loss: 0.0977\n",
      "Epoch 154/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0977\n",
      "Epoch 155/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0977\n",
      "Epoch 156/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0992 - val_loss: 0.0977\n",
      "Epoch 157/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0977\n",
      "Epoch 158/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 159/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 4.999999969612645e-09.\n",
      "Epoch 160/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0988 - val_loss: 0.0977\n",
      "Epoch 161/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 162/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0997 - val_loss: 0.0977\n",
      "Epoch 163/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0999 - val_loss: 0.0977\n",
      "Epoch 164/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0977\n",
      "Epoch 165/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0995 - val_loss: 0.0977\n",
      "Epoch 166/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0977\n",
      "Epoch 167/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0977\n",
      "Epoch 168/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0978 - val_loss: 0.0977\n",
      "Epoch 169/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0990 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 2.4999999848063226e-09.\n",
      "Epoch 170/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0977\n",
      "Epoch 171/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0977\n",
      "Epoch 172/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 173/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0977\n",
      "Epoch 174/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0977\n",
      "Epoch 175/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.1000 - val_loss: 0.0977\n",
      "Epoch 176/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 177/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0995 - val_loss: 0.0976\n",
      "Epoch 178/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0977 - val_loss: 0.0976\n",
      "Epoch 179/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0975 - val_loss: 0.0976\n",
      "Epoch 180/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0981 - val_loss: 0.0976\n",
      "Epoch 181/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0998 - val_loss: 0.0976\n",
      "Epoch 182/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 183/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0976\n",
      "Epoch 184/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 185/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0995 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 1.2499999924031613e-09.\n",
      "Epoch 186/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0994 - val_loss: 0.0976\n",
      "Epoch 187/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0969 - val_loss: 0.0976\n",
      "Epoch 188/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 189/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 190/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0985 - val_loss: 0.0976\n",
      "Epoch 191/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 192/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 193/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 194/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.0975 - val_loss: 0.0976\n",
      "Epoch 195/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0975 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 6.249999962015806e-10.\n",
      "Epoch 196/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1002 - val_loss: 0.0976\n",
      "Epoch 197/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 198/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 199/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 200/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 201/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 202/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0971 - val_loss: 0.0976\n",
      "Epoch 203/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0976\n",
      "Epoch 204/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 205/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 3.124999981007903e-10.\n",
      "Epoch 206/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0972 - val_loss: 0.0976\n",
      "Epoch 207/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 208/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 209/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 210/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0974 - val_loss: 0.0976\n",
      "Epoch 211/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 212/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 213/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 214/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0971 - val_loss: 0.0976\n",
      "Epoch 215/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0994 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 1.5624999905039516e-10.\n",
      "Epoch 216/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 217/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 218/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.1000 - val_loss: 0.0976\n",
      "Epoch 219/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 220/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0978 - val_loss: 0.0976\n",
      "Epoch 221/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 222/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 223/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0976 - val_loss: 0.0976\n",
      "Epoch 224/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0997 - val_loss: 0.0976\n",
      "Epoch 225/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0981 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 7.812499952519758e-11.\n",
      "Epoch 226/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 227/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 228/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 229/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 230/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 231/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 232/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 233/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0974 - val_loss: 0.0976\n",
      "Epoch 234/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0994 - val_loss: 0.0976\n",
      "Epoch 235/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0989 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 3.906249976259879e-11.\n",
      "Epoch 236/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 237/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0974 - val_loss: 0.0976\n",
      "Epoch 238/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0977 - val_loss: 0.0976\n",
      "Epoch 239/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 240/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0996 - val_loss: 0.0976\n",
      "Epoch 241/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 242/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 243/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 244/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 245/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 1.9531249881299395e-11.\n",
      "Epoch 246/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 247/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0998 - val_loss: 0.0976\n",
      "Epoch 248/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 249/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 250/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 251/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 252/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 253/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0985 - val_loss: 0.0976\n",
      "Epoch 254/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0986 - val_loss: 0.0976\n",
      "Epoch 255/1000\n",
      "58/58 [==============================] - 22s 381ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00255: ReduceLROnPlateau reducing learning rate to 9.765624940649698e-12.\n",
      "Epoch 256/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 257/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 258/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0983 - val_loss: 0.0976\n",
      "Epoch 259/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 260/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 261/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 262/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0982 - val_loss: 0.0976\n",
      "Epoch 263/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 264/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 265/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0985 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 4.882812470324849e-12.\n",
      "Epoch 266/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0990 - val_loss: 0.0976\n",
      "Epoch 267/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 268/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0987 - val_loss: 0.0976\n",
      "Epoch 269/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 270/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 271/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0977 - val_loss: 0.0976\n",
      "Epoch 272/1000\n",
      "58/58 [==============================] - 22s 377ms/step - loss: 0.0992 - val_loss: 0.0976\n",
      "Epoch 273/1000\n",
      "58/58 [==============================] - 22s 378ms/step - loss: 0.0972 - val_loss: 0.0976\n",
      "Epoch 00273: early stopping\n"
     ]
    }
   ],
   "source": [
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu #24\n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_trained.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "dis_tr_triplets = discard_some_low(training_triplets, 0.7, 0.7)\n",
    "dis_val_triplets = discard_some_low(validation_triplets, 0.7, 0.7)\n",
    "\n",
    "#low_margin, high_margin, bal_training_triplets = balance_input(dis_tr_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "#vlow_margin, vhigh_margin, bal_val_triplets = balance_input(dis_val_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "\n",
    "#history = triplet_model.fit(train_generator_luscinia(luscinia_triplets[:int(luscinia_train_len/10)], M_l, S_l, batchsize, emb_size, path_mel),\n",
    " #                          steps_per_epoch=int(int(luscinia_train_len/10)/batchsize), epochs=1000, verbose=1,\n",
    "  #                         validation_data=train_generator_luscinia(luscinia_triplets[luscinia_train_len:luscinia_train_len+200], M_l, S_l, batchsize, emb_size, path_mel),\n",
    "   #                        validation_steps=int(200/batchsize), callbacks=[cpCallback, reduce_lr, earlystop])\n",
    "#history = triplet_model.fit(train_generator_mixed(bal_training_triplets, M, S, luscinia_triplets[:luscinia_train_len],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    " #                               steps_per_epoch=int(len(bal_training_triplets)/(lo+hi)), epochs=1000, verbose=1,\n",
    "  #                              validation_data=train_generator_mixed(bal_val_triplets, M, S, luscinia_triplets[luscinia_train_len:],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "   #                             validation_steps=int(len(bal_val_triplets)/(lo+hi)), callbacks=[cpCallback, reduce_lr, earlystop])\n",
    "history = triplet_model.fit(train_generator(dis_tr_triplets, M, S, batchsize, emb_size, path_mel),\n",
    "                           steps_per_epoch=int(len(dis_tr_triplets)/batchsize), epochs=1000, verbose=1,\n",
    "                           validation_data=train_generator(dis_val_triplets, M,S, batchsize, emb_size, path_mel),\n",
    "                           validation_steps=int(len(dis_val_triplets)/batchsize), callbacks=[cpCallback, reduce_lr,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "117/117 [==============================] - 50s 380ms/step - loss: 0.6301 - val_loss: 0.5736\n",
      "Epoch 2/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.6108 - val_loss: 0.5940\n",
      "Epoch 3/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.6125 - val_loss: 0.5691\n",
      "Epoch 4/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5836 - val_loss: 0.5496\n",
      "Epoch 5/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5807 - val_loss: 0.5335\n",
      "Epoch 6/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5562 - val_loss: 0.5218\n",
      "Epoch 7/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5476 - val_loss: 0.5129\n",
      "Epoch 8/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5425 - val_loss: 0.5063\n",
      "Epoch 9/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.5374 - val_loss: 0.5008\n",
      "Epoch 10/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5245 - val_loss: 0.4966\n",
      "Epoch 11/1000\n",
      "117/117 [==============================] - 44s 375ms/step - loss: 0.5251 - val_loss: 0.4928\n",
      "Epoch 12/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5145 - val_loss: 0.4898\n",
      "Epoch 13/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5166 - val_loss: 0.4878\n",
      "Epoch 14/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5131 - val_loss: 0.4860\n",
      "Epoch 15/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5089 - val_loss: 0.4847\n",
      "Epoch 16/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5063 - val_loss: 0.4836\n",
      "Epoch 17/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5037 - val_loss: 0.4827\n",
      "Epoch 18/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4987 - val_loss: 0.4817\n",
      "Epoch 19/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.5007 - val_loss: 0.4808\n",
      "Epoch 20/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4962 - val_loss: 0.4801\n",
      "Epoch 21/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4969 - val_loss: 0.4796\n",
      "Epoch 22/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4932 - val_loss: 0.4791\n",
      "Epoch 23/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4944 - val_loss: 0.4787\n",
      "Epoch 24/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4937 - val_loss: 0.4782\n",
      "Epoch 25/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4924 - val_loss: 0.4778\n",
      "Epoch 26/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4897 - val_loss: 0.4774\n",
      "Epoch 27/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4895 - val_loss: 0.4769\n",
      "Epoch 28/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4886 - val_loss: 0.4766\n",
      "Epoch 29/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4868 - val_loss: 0.4761\n",
      "Epoch 30/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4866 - val_loss: 0.4757\n",
      "Epoch 31/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4851 - val_loss: 0.4755\n",
      "Epoch 32/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4878 - val_loss: 0.4754\n",
      "Epoch 33/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4852 - val_loss: 0.4752\n",
      "Epoch 34/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4839 - val_loss: 0.4750\n",
      "Epoch 35/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4822 - val_loss: 0.4748\n",
      "Epoch 36/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4832 - val_loss: 0.4746\n",
      "Epoch 37/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4821 - val_loss: 0.4744\n",
      "Epoch 38/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4808 - val_loss: 0.4742\n",
      "Epoch 39/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4823 - val_loss: 0.4741\n",
      "Epoch 40/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4813 - val_loss: 0.4740\n",
      "Epoch 41/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4791 - val_loss: 0.4738\n",
      "Epoch 42/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4792 - val_loss: 0.4737\n",
      "Epoch 43/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4818 - val_loss: 0.4735\n",
      "Epoch 44/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4766 - val_loss: 0.4732\n",
      "Epoch 45/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4789 - val_loss: 0.4728\n",
      "Epoch 46/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4783 - val_loss: 0.4726\n",
      "Epoch 47/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4794 - val_loss: 0.4725\n",
      "Epoch 48/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4788 - val_loss: 0.4724\n",
      "Epoch 49/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4787 - val_loss: 0.4722\n",
      "Epoch 50/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4766 - val_loss: 0.4721\n",
      "Epoch 51/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4764 - val_loss: 0.4719\n",
      "Epoch 52/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4740 - val_loss: 0.4717\n",
      "Epoch 53/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4773 - val_loss: 0.4715\n",
      "Epoch 54/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4736 - val_loss: 0.4714\n",
      "Epoch 55/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4767 - val_loss: 0.4712\n",
      "Epoch 56/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4729 - val_loss: 0.4711\n",
      "Epoch 57/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4750 - val_loss: 0.4709\n",
      "Epoch 58/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4749 - val_loss: 0.4707\n",
      "Epoch 59/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4733 - val_loss: 0.4705\n",
      "Epoch 60/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4733 - val_loss: 0.4704\n",
      "Epoch 61/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4737 - val_loss: 0.4703\n",
      "Epoch 62/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4711 - val_loss: 0.4701\n",
      "Epoch 63/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4718 - val_loss: 0.4699\n",
      "Epoch 64/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4717 - val_loss: 0.4698\n",
      "Epoch 65/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4714 - val_loss: 0.4697\n",
      "Epoch 66/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4717 - val_loss: 0.4696\n",
      "Epoch 67/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4712 - val_loss: 0.4695\n",
      "Epoch 68/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4688 - val_loss: 0.4694\n",
      "Epoch 69/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4709 - val_loss: 0.4693\n",
      "Epoch 70/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4701 - val_loss: 0.4691\n",
      "Epoch 71/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4702 - val_loss: 0.4690\n",
      "Epoch 72/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4687 - val_loss: 0.4688\n",
      "Epoch 73/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4680 - val_loss: 0.4687\n",
      "Epoch 74/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4708 - val_loss: 0.4686\n",
      "Epoch 75/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4677 - val_loss: 0.4685\n",
      "Epoch 76/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4686 - val_loss: 0.4683\n",
      "Epoch 77/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4686 - val_loss: 0.4681\n",
      "Epoch 78/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4668 - val_loss: 0.4680\n",
      "Epoch 79/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4689 - val_loss: 0.4679\n",
      "Epoch 80/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4677 - val_loss: 0.4678\n",
      "Epoch 81/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4670 - val_loss: 0.4677\n",
      "Epoch 82/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4662 - val_loss: 0.4675\n",
      "Epoch 83/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4687 - val_loss: 0.4674\n",
      "Epoch 84/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4686 - val_loss: 0.4673\n",
      "Epoch 85/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4672 - val_loss: 0.4672\n",
      "Epoch 86/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4676 - val_loss: 0.4670\n",
      "Epoch 87/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4653 - val_loss: 0.4669\n",
      "Epoch 88/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4665 - val_loss: 0.4668\n",
      "Epoch 89/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4655 - val_loss: 0.4668\n",
      "Epoch 90/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4644 - val_loss: 0.4667\n",
      "Epoch 91/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4660 - val_loss: 0.4665\n",
      "Epoch 92/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4638 - val_loss: 0.4665\n",
      "Epoch 93/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4642 - val_loss: 0.4664\n",
      "Epoch 94/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4650 - val_loss: 0.4663\n",
      "Epoch 95/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4649 - val_loss: 0.4663\n",
      "Epoch 96/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4621 - val_loss: 0.4662\n",
      "Epoch 97/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4648 - val_loss: 0.4661\n",
      "Epoch 98/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4645 - val_loss: 0.4660\n",
      "Epoch 99/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4650 - val_loss: 0.4659\n",
      "Epoch 100/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4636 - val_loss: 0.4658\n",
      "Epoch 101/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4631 - val_loss: 0.4657\n",
      "Epoch 102/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4628 - val_loss: 0.4656\n",
      "Epoch 103/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4617 - val_loss: 0.4655\n",
      "Epoch 104/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4629 - val_loss: 0.4654\n",
      "Epoch 105/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4626 - val_loss: 0.4653\n",
      "Epoch 106/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4636 - val_loss: 0.4652\n",
      "Epoch 107/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4617 - val_loss: 0.4651\n",
      "Epoch 108/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4626 - val_loss: 0.4650\n",
      "Epoch 109/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4619 - val_loss: 0.4649\n",
      "Epoch 110/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4626 - val_loss: 0.4648\n",
      "Epoch 111/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4623 - val_loss: 0.4647\n",
      "Epoch 112/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4624 - val_loss: 0.4646\n",
      "Epoch 113/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4619 - val_loss: 0.4645\n",
      "Epoch 114/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4601 - val_loss: 0.4644\n",
      "Epoch 115/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4605 - val_loss: 0.4644\n",
      "Epoch 116/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4607 - val_loss: 0.4643\n",
      "Epoch 117/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4604 - val_loss: 0.4642\n",
      "Epoch 118/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4612 - val_loss: 0.4642\n",
      "Epoch 119/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4608 - val_loss: 0.4641\n",
      "Epoch 120/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4598 - val_loss: 0.4640\n",
      "Epoch 121/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4595 - val_loss: 0.4638\n",
      "Epoch 122/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4596 - val_loss: 0.4638\n",
      "Epoch 123/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4604 - val_loss: 0.4637\n",
      "Epoch 124/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4600 - val_loss: 0.4636\n",
      "Epoch 125/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4590 - val_loss: 0.4635\n",
      "Epoch 126/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4588 - val_loss: 0.4634\n",
      "Epoch 127/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4593 - val_loss: 0.4634\n",
      "Epoch 128/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4595 - val_loss: 0.4633\n",
      "Epoch 129/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4584 - val_loss: 0.4632\n",
      "Epoch 130/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4592 - val_loss: 0.4631\n",
      "Epoch 131/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4591 - val_loss: 0.4631\n",
      "Epoch 132/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4580 - val_loss: 0.4630\n",
      "Epoch 133/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4585 - val_loss: 0.4629\n",
      "Epoch 134/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4585 - val_loss: 0.4628\n",
      "Epoch 135/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4584 - val_loss: 0.4628\n",
      "Epoch 136/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4580 - val_loss: 0.4627\n",
      "Epoch 137/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4571 - val_loss: 0.4627\n",
      "Epoch 138/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4576 - val_loss: 0.4626\n",
      "Epoch 139/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4586 - val_loss: 0.4626\n",
      "Epoch 140/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4592 - val_loss: 0.4625\n",
      "Epoch 141/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4582 - val_loss: 0.4624\n",
      "Epoch 142/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4576 - val_loss: 0.4624\n",
      "Epoch 143/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4565 - val_loss: 0.4624\n",
      "Epoch 144/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4577 - val_loss: 0.4623\n",
      "Epoch 145/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4570 - val_loss: 0.4622\n",
      "Epoch 146/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4568 - val_loss: 0.4622\n",
      "Epoch 147/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4578 - val_loss: 0.4621\n",
      "Epoch 148/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4584 - val_loss: 0.4620\n",
      "Epoch 149/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4569 - val_loss: 0.4620\n",
      "Epoch 150/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4561 - val_loss: 0.4620\n",
      "Epoch 151/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4574 - val_loss: 0.4619\n",
      "Epoch 152/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4566 - val_loss: 0.4618\n",
      "Epoch 153/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4565 - val_loss: 0.4617\n",
      "Epoch 154/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4572 - val_loss: 0.4617\n",
      "Epoch 155/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4562 - val_loss: 0.4617\n",
      "Epoch 156/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4561 - val_loss: 0.4616\n",
      "Epoch 157/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4567 - val_loss: 0.4616\n",
      "Epoch 158/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4555 - val_loss: 0.4615\n",
      "Epoch 159/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4556 - val_loss: 0.4615\n",
      "Epoch 160/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4550 - val_loss: 0.4614\n",
      "Epoch 161/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4560 - val_loss: 0.4613\n",
      "Epoch 162/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4613\n",
      "Epoch 163/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4563 - val_loss: 0.4612\n",
      "Epoch 164/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4560 - val_loss: 0.4612\n",
      "Epoch 165/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4554 - val_loss: 0.4611\n",
      "Epoch 166/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4555 - val_loss: 0.4611\n",
      "Epoch 167/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4553 - val_loss: 0.4611\n",
      "Epoch 168/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4542 - val_loss: 0.4610\n",
      "Epoch 169/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4536 - val_loss: 0.4610\n",
      "Epoch 170/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4554 - val_loss: 0.4610\n",
      "Epoch 171/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4552 - val_loss: 0.4610\n",
      "Epoch 172/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4540 - val_loss: 0.4609\n",
      "Epoch 173/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4609\n",
      "Epoch 174/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4609\n",
      "Epoch 175/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4548 - val_loss: 0.4608\n",
      "Epoch 176/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4540 - val_loss: 0.4608\n",
      "Epoch 177/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4533 - val_loss: 0.4608\n",
      "Epoch 178/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4519 - val_loss: 0.4607\n",
      "Epoch 179/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4537 - val_loss: 0.4607\n",
      "Epoch 180/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4531 - val_loss: 0.4606\n",
      "Epoch 181/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4534 - val_loss: 0.4606\n",
      "Epoch 182/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4535 - val_loss: 0.4606\n",
      "Epoch 183/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4538 - val_loss: 0.4605\n",
      "Epoch 184/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4541 - val_loss: 0.4605\n",
      "Epoch 185/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4522 - val_loss: 0.4604\n",
      "Epoch 186/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4604\n",
      "Epoch 187/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4528 - val_loss: 0.4604\n",
      "Epoch 188/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4530 - val_loss: 0.4603\n",
      "Epoch 189/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4539 - val_loss: 0.4603\n",
      "Epoch 190/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4531 - val_loss: 0.4602\n",
      "Epoch 191/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4521 - val_loss: 0.4602\n",
      "Epoch 192/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4514 - val_loss: 0.4602\n",
      "Epoch 193/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4517 - val_loss: 0.4601\n",
      "Epoch 194/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4522 - val_loss: 0.4601\n",
      "Epoch 195/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4523 - val_loss: 0.4601\n",
      "Epoch 196/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4512 - val_loss: 0.4600\n",
      "Epoch 197/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4513 - val_loss: 0.4599\n",
      "Epoch 198/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4523 - val_loss: 0.4599\n",
      "Epoch 199/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4506 - val_loss: 0.4599\n",
      "Epoch 200/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4510 - val_loss: 0.4599\n",
      "Epoch 201/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4598\n",
      "Epoch 202/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4511 - val_loss: 0.4598\n",
      "Epoch 203/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4597\n",
      "Epoch 204/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4596\n",
      "Epoch 205/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4508 - val_loss: 0.4596\n",
      "Epoch 206/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4518 - val_loss: 0.4595\n",
      "Epoch 207/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4523 - val_loss: 0.4594\n",
      "Epoch 208/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4516 - val_loss: 0.4594\n",
      "Epoch 209/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4505 - val_loss: 0.4594\n",
      "Epoch 210/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4507 - val_loss: 0.4593\n",
      "Epoch 211/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4511 - val_loss: 0.4593\n",
      "Epoch 212/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4505 - val_loss: 0.4592\n",
      "Epoch 213/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4506 - val_loss: 0.4592\n",
      "Epoch 214/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4512 - val_loss: 0.4592\n",
      "Epoch 215/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4513 - val_loss: 0.4592\n",
      "Epoch 216/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4505 - val_loss: 0.4591\n",
      "Epoch 217/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4494 - val_loss: 0.4591\n",
      "Epoch 218/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4499 - val_loss: 0.4591\n",
      "Epoch 219/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4493 - val_loss: 0.4591\n",
      "Epoch 220/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4494 - val_loss: 0.4591\n",
      "Epoch 221/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4496 - val_loss: 0.4590\n",
      "Epoch 222/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4504 - val_loss: 0.4590\n",
      "Epoch 223/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4495 - val_loss: 0.4590\n",
      "Epoch 224/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4506 - val_loss: 0.4590\n",
      "Epoch 225/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4511 - val_loss: 0.4590\n",
      "Epoch 226/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4499 - val_loss: 0.4590\n",
      "Epoch 227/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4498 - val_loss: 0.4590\n",
      "Epoch 228/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4494 - val_loss: 0.4590\n",
      "Epoch 229/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4497 - val_loss: 0.4590\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "Epoch 230/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4495 - val_loss: 0.4590\n",
      "Epoch 231/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4495 - val_loss: 0.4590\n",
      "Epoch 232/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4499 - val_loss: 0.4589\n",
      "Epoch 233/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4493 - val_loss: 0.4589\n",
      "Epoch 234/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4486 - val_loss: 0.4589\n",
      "Epoch 235/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4485 - val_loss: 0.4589\n",
      "Epoch 236/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4501 - val_loss: 0.4589\n",
      "Epoch 237/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4487 - val_loss: 0.4588\n",
      "Epoch 238/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4483 - val_loss: 0.4588\n",
      "Epoch 239/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4486 - val_loss: 0.4588\n",
      "Epoch 240/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4492 - val_loss: 0.4588\n",
      "Epoch 241/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4488 - val_loss: 0.4587\n",
      "Epoch 242/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4491 - val_loss: 0.4587\n",
      "Epoch 243/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4485 - val_loss: 0.4587\n",
      "Epoch 244/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4480 - val_loss: 0.4587\n",
      "Epoch 245/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4484 - val_loss: 0.4587\n",
      "Epoch 246/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4482 - val_loss: 0.4587\n",
      "Epoch 247/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4498 - val_loss: 0.4587\n",
      "Epoch 248/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4490 - val_loss: 0.4586\n",
      "Epoch 249/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4489 - val_loss: 0.4586\n",
      "Epoch 250/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4586\n",
      "Epoch 251/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4586\n",
      "Epoch 252/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4496 - val_loss: 0.4586\n",
      "Epoch 253/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4482 - val_loss: 0.4586\n",
      "Epoch 254/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4487 - val_loss: 0.4586\n",
      "Epoch 255/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4479 - val_loss: 0.4586\n",
      "Epoch 256/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4474 - val_loss: 0.4586\n",
      "Epoch 257/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4484 - val_loss: 0.4585\n",
      "Epoch 258/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4483 - val_loss: 0.4585\n",
      "Epoch 259/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4585\n",
      "Epoch 260/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4585\n",
      "Epoch 261/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4492 - val_loss: 0.4585\n",
      "Epoch 262/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4585\n",
      "Epoch 263/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4491 - val_loss: 0.4584\n",
      "Epoch 264/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4481 - val_loss: 0.4584\n",
      "Epoch 265/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4483 - val_loss: 0.4584\n",
      "Epoch 266/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4584\n",
      "Epoch 267/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4584\n",
      "Epoch 268/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4471 - val_loss: 0.4583\n",
      "Epoch 269/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4474 - val_loss: 0.4583\n",
      "Epoch 270/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4583\n",
      "Epoch 271/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4487 - val_loss: 0.4582\n",
      "Epoch 272/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4582\n",
      "Epoch 273/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4478 - val_loss: 0.4582\n",
      "Epoch 274/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4489 - val_loss: 0.4582\n",
      "Epoch 275/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4582\n",
      "Epoch 276/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4479 - val_loss: 0.4581\n",
      "Epoch 277/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4581\n",
      "Epoch 278/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4484 - val_loss: 0.4581\n",
      "Epoch 279/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4480 - val_loss: 0.4581\n",
      "Epoch 280/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4580\n",
      "Epoch 281/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4580\n",
      "Epoch 282/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4478 - val_loss: 0.4580\n",
      "Epoch 283/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4476 - val_loss: 0.4580\n",
      "Epoch 284/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4472 - val_loss: 0.4580\n",
      "Epoch 285/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4580\n",
      "Epoch 286/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4580\n",
      "Epoch 287/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4579\n",
      "Epoch 288/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4475 - val_loss: 0.4579\n",
      "Epoch 289/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4468 - val_loss: 0.4579\n",
      "Epoch 290/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4473 - val_loss: 0.4579\n",
      "Epoch 291/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4579\n",
      "Epoch 292/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4486 - val_loss: 0.4579\n",
      "Epoch 293/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4464 - val_loss: 0.4579\n",
      "Epoch 294/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4475 - val_loss: 0.4578\n",
      "Epoch 295/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4459 - val_loss: 0.4578\n",
      "Epoch 296/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4463 - val_loss: 0.4578\n",
      "Epoch 297/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4577\n",
      "Epoch 298/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4453 - val_loss: 0.4577\n",
      "Epoch 299/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4477 - val_loss: 0.4577\n",
      "Epoch 300/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4472 - val_loss: 0.4577\n",
      "Epoch 301/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4576\n",
      "Epoch 302/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4469 - val_loss: 0.4576\n",
      "Epoch 303/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4576\n",
      "Epoch 304/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4472 - val_loss: 0.4576\n",
      "Epoch 305/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4467 - val_loss: 0.4575\n",
      "Epoch 306/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4575\n",
      "Epoch 307/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4471 - val_loss: 0.4576\n",
      "Epoch 308/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4473 - val_loss: 0.4576\n",
      "Epoch 309/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4472 - val_loss: 0.4576\n",
      "Epoch 310/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4576\n",
      "Epoch 311/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4468 - val_loss: 0.4576\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "Epoch 312/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4576\n",
      "Epoch 313/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4471 - val_loss: 0.4576\n",
      "Epoch 314/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4464 - val_loss: 0.4576\n",
      "Epoch 315/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4460 - val_loss: 0.4576\n",
      "Epoch 316/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4469 - val_loss: 0.4575\n",
      "Epoch 317/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4449 - val_loss: 0.4575\n",
      "Epoch 318/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4575\n",
      "Epoch 319/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4575\n",
      "Epoch 320/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4467 - val_loss: 0.4575\n",
      "Epoch 321/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4575\n",
      "Epoch 322/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4471 - val_loss: 0.4575\n",
      "Epoch 323/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4575\n",
      "Epoch 324/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4463 - val_loss: 0.4575\n",
      "Epoch 325/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4575\n",
      "Epoch 326/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4574\n",
      "Epoch 327/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4455 - val_loss: 0.4574\n",
      "Epoch 328/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4446 - val_loss: 0.4574\n",
      "Epoch 329/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4458 - val_loss: 0.4574\n",
      "Epoch 330/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4467 - val_loss: 0.4574\n",
      "Epoch 331/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4459 - val_loss: 0.4574\n",
      "Epoch 332/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4574\n",
      "Epoch 333/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4454 - val_loss: 0.4574\n",
      "Epoch 334/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4457 - val_loss: 0.4574\n",
      "Epoch 335/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4463 - val_loss: 0.4574\n",
      "Epoch 336/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4456 - val_loss: 0.4574\n",
      "Epoch 337/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4573\n",
      "Epoch 338/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "Epoch 339/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4443 - val_loss: 0.4573\n",
      "Epoch 340/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00340: ReduceLROnPlateau reducing learning rate to 1.2500000146076218e-08.\n",
      "Epoch 341/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4451 - val_loss: 0.4573\n",
      "Epoch 342/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 343/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4463 - val_loss: 0.4573\n",
      "Epoch 344/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 345/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 346/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4573\n",
      "Epoch 347/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 348/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 349/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 350/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 351/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4444 - val_loss: 0.4573\n",
      "Epoch 352/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4452 - val_loss: 0.4573\n",
      "Epoch 353/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00353: ReduceLROnPlateau reducing learning rate to 6.250000073038109e-09.\n",
      "Epoch 354/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4458 - val_loss: 0.4573\n",
      "Epoch 355/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4465 - val_loss: 0.4573\n",
      "Epoch 356/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4448 - val_loss: 0.4573\n",
      "Epoch 357/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4467 - val_loss: 0.4573\n",
      "Epoch 358/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4459 - val_loss: 0.4573\n",
      "Epoch 359/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "Epoch 360/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4452 - val_loss: 0.4573\n",
      "Epoch 361/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4451 - val_loss: 0.4573\n",
      "Epoch 362/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 363/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4458 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00363: ReduceLROnPlateau reducing learning rate to 3.1250000365190544e-09.\n",
      "Epoch 364/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4454 - val_loss: 0.4573\n",
      "Epoch 365/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 366/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 367/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4452 - val_loss: 0.4573\n",
      "Epoch 368/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4446 - val_loss: 0.4573\n",
      "Epoch 369/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 370/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4456 - val_loss: 0.4573\n",
      "Epoch 371/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 372/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4442 - val_loss: 0.4573\n",
      "Epoch 373/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4453 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00373: ReduceLROnPlateau reducing learning rate to 1.5625000182595272e-09.\n",
      "Epoch 374/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 375/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4457 - val_loss: 0.4573\n",
      "Epoch 376/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 377/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4444 - val_loss: 0.4573\n",
      "Epoch 378/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4462 - val_loss: 0.4573\n",
      "Epoch 379/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4467 - val_loss: 0.4573\n",
      "Epoch 380/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4449 - val_loss: 0.4573\n",
      "Epoch 381/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 382/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 383/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00383: ReduceLROnPlateau reducing learning rate to 7.812500091297636e-10.\n",
      "Epoch 384/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 385/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4441 - val_loss: 0.4573\n",
      "Epoch 386/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 387/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 388/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 389/1000\n",
      "117/117 [==============================] - 44s 374ms/step - loss: 0.4459 - val_loss: 0.4573\n",
      "Epoch 390/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4444 - val_loss: 0.4573\n",
      "Epoch 391/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4470 - val_loss: 0.4573\n",
      "Epoch 392/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4460 - val_loss: 0.4573\n",
      "Epoch 393/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4447 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00393: ReduceLROnPlateau reducing learning rate to 3.906250045648818e-10.\n",
      "Epoch 394/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 395/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4467 - val_loss: 0.4573\n",
      "Epoch 396/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 397/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 398/1000\n",
      "117/117 [==============================] - 44s 372ms/step - loss: 0.4465 - val_loss: 0.4573\n",
      "Epoch 399/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4455 - val_loss: 0.4573\n",
      "Epoch 400/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4466 - val_loss: 0.4573\n",
      "Epoch 401/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4450 - val_loss: 0.4573\n",
      "Epoch 402/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.4461 - val_loss: 0.4573\n",
      "Epoch 00402: early stopping\n"
     ]
    }
   ],
   "source": [
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu #26\n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_MIXED_margin_loss.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "dis_tr_triplets = discard_some_low(training_triplets, 0.7, 0.7)\n",
    "dis_val_triplets = discard_some_low(validation_triplets, 0.7, 0.7)\n",
    "\n",
    "low_margin, high_margin, bal_training_triplets = balance_input(dis_tr_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "vlow_margin, vhigh_margin, bal_val_triplets = balance_input(dis_val_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "\n",
    "#history = triplet_model.fit(train_generator_luscinia(luscinia_triplets[:int(luscinia_train_len/10)], M_l, S_l, batchsize, emb_size, path_mel),\n",
    " #                          steps_per_epoch=int(int(luscinia_train_len/10)/batchsize), epochs=1000, verbose=1,\n",
    "  #                         validation_data=train_generator_luscinia(luscinia_triplets[luscinia_train_len:luscinia_train_len+200], M_l, S_l, batchsize, emb_size, path_mel),\n",
    "   #                        validation_steps=int(200/batchsize), callbacks=[cpCallback, reduce_lr, earlystop])\n",
    "history = triplet_model.fit(train_generator_mixed(bal_training_triplets, M, S, luscinia_triplets[:luscinia_train_len],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                steps_per_epoch=int(len(bal_training_triplets)/(lo+hi)), epochs=1000, verbose=1,\n",
    "                                validation_data=train_generator_mixed(bal_val_triplets, M, S, luscinia_triplets[luscinia_train_len:],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                validation_steps=int(len(bal_val_triplets)/(lo+hi)), callbacks=[cpCallback, reduce_lr, earlystop])\n",
    "#history = triplet_model.fit(train_generator(dis_tr_triplets, M, S, batchsize, emb_size, path_mel),\n",
    " #                          steps_per_epoch=int(len(dis_tr_triplets)/batchsize), epochs=1000, verbose=1,\n",
    "  #                         validation_data=train_generator(dis_val_triplets, M,S, batchsize, emb_size, path_mel),\n",
    "   #                        validation_steps=int(len(dis_val_triplets)/batchsize), callbacks=[cpCallback, reduce_lr,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE + MIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "117/117 [==============================] - 50s 380ms/step - loss: 0.0973 - val_loss: 0.0863\n",
      "Epoch 2/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0958 - val_loss: 0.0856\n",
      "Epoch 3/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0949 - val_loss: 0.0852\n",
      "Epoch 4/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0937 - val_loss: 0.0849\n",
      "Epoch 5/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0921 - val_loss: 0.0845\n",
      "Epoch 6/1000\n",
      "117/117 [==============================] - 44s 375ms/step - loss: 0.0927 - val_loss: 0.0842\n",
      "Epoch 7/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0911 - val_loss: 0.0839\n",
      "Epoch 8/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0919 - val_loss: 0.0836\n",
      "Epoch 9/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0894 - val_loss: 0.0834\n",
      "Epoch 10/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0896 - val_loss: 0.0831\n",
      "Epoch 11/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0880 - val_loss: 0.0828\n",
      "Epoch 12/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0874 - val_loss: 0.0826\n",
      "Epoch 13/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0871 - val_loss: 0.0824\n",
      "Epoch 14/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0871 - val_loss: 0.0822\n",
      "Epoch 15/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0865 - val_loss: 0.0820\n",
      "Epoch 16/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0837 - val_loss: 0.0818\n",
      "Epoch 17/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0843 - val_loss: 0.0816\n",
      "Epoch 18/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0842 - val_loss: 0.0815\n",
      "Epoch 19/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0828 - val_loss: 0.0813\n",
      "Epoch 20/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0832 - val_loss: 0.0812\n",
      "Epoch 21/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0832 - val_loss: 0.0810\n",
      "Epoch 22/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0824 - val_loss: 0.0809\n",
      "Epoch 23/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0821 - val_loss: 0.0807\n",
      "Epoch 24/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0799 - val_loss: 0.0806\n",
      "Epoch 25/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0817 - val_loss: 0.0805\n",
      "Epoch 26/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0806 - val_loss: 0.0804\n",
      "Epoch 27/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0807 - val_loss: 0.0803\n",
      "Epoch 28/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0788 - val_loss: 0.0801\n",
      "Epoch 29/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0803 - val_loss: 0.0800\n",
      "Epoch 30/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0802 - val_loss: 0.0799\n",
      "Epoch 31/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0807 - val_loss: 0.0798\n",
      "Epoch 32/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0788 - val_loss: 0.0797\n",
      "Epoch 33/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0781 - val_loss: 0.0797\n",
      "Epoch 34/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0802 - val_loss: 0.0796\n",
      "Epoch 35/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0789 - val_loss: 0.0795\n",
      "Epoch 36/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0791 - val_loss: 0.0794\n",
      "Epoch 37/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0778 - val_loss: 0.0794\n",
      "Epoch 38/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0779 - val_loss: 0.0793\n",
      "Epoch 39/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0781 - val_loss: 0.0792\n",
      "Epoch 40/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0777 - val_loss: 0.0792\n",
      "Epoch 41/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0786 - val_loss: 0.0791\n",
      "Epoch 42/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0780 - val_loss: 0.0791\n",
      "Epoch 43/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0783 - val_loss: 0.0790\n",
      "Epoch 44/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0779 - val_loss: 0.0790\n",
      "Epoch 45/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0777 - val_loss: 0.0789\n",
      "Epoch 46/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0765 - val_loss: 0.0789\n",
      "Epoch 47/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0770 - val_loss: 0.0788\n",
      "Epoch 48/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0763 - val_loss: 0.0788\n",
      "Epoch 49/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0768 - val_loss: 0.0788\n",
      "Epoch 50/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.0769 - val_loss: 0.0787\n",
      "Epoch 51/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0768 - val_loss: 0.0787\n",
      "Epoch 52/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0758 - val_loss: 0.0787\n",
      "Epoch 53/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0762 - val_loss: 0.0787\n",
      "Epoch 54/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0760 - val_loss: 0.0786\n",
      "Epoch 55/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0766 - val_loss: 0.0786\n",
      "Epoch 56/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0753 - val_loss: 0.0786\n",
      "Epoch 57/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0749 - val_loss: 0.0786\n",
      "Epoch 58/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.0753 - val_loss: 0.0785\n",
      "Epoch 59/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0751 - val_loss: 0.0785\n",
      "Epoch 60/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0754 - val_loss: 0.0785\n",
      "Epoch 61/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0755 - val_loss: 0.0785\n",
      "Epoch 62/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0752 - val_loss: 0.0785\n",
      "Epoch 63/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0749 - val_loss: 0.0785\n",
      "Epoch 64/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0750 - val_loss: 0.0785\n",
      "Epoch 65/1000\n",
      "117/117 [==============================] - 44s 373ms/step - loss: 0.0751 - val_loss: 0.0785\n",
      "Epoch 66/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0750 - val_loss: 0.0785\n",
      "Epoch 67/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0748 - val_loss: 0.0785\n",
      "Epoch 68/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0756 - val_loss: 0.0784\n",
      "Epoch 69/1000\n",
      "117/117 [==============================] - 43s 372ms/step - loss: 0.0747 - val_loss: 0.0784\n",
      "Epoch 70/1000\n",
      " 66/117 [===============>..............] - ETA: 17s - loss: 0.0731"
     ]
    }
   ],
   "source": [
    "triplet_model.load_weights('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_margin_loss_backup.h5')\n",
    "lo = 6\n",
    "hi = 8\n",
    "lu = 10\n",
    "batchsize = lo+hi+lu #26\n",
    "\n",
    "cpCallback = ModelCheckpoint('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_MIXED_margin_loss.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-12)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "dis_tr_triplets = discard_some_low(training_triplets, 0.7, 0.7)\n",
    "dis_val_triplets = discard_some_low(validation_triplets, 0.7, 0.7)\n",
    "\n",
    "low_margin, high_margin, bal_training_triplets = balance_input(dis_tr_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "vlow_margin, vhigh_margin, bal_val_triplets = balance_input(dis_val_triplets, 0.7, hi_balance = hi, lo_balance = lo)\n",
    "\n",
    "#history = triplet_model.fit(train_generator_luscinia(luscinia_triplets[:int(luscinia_train_len/10)], M_l, S_l, batchsize, emb_size, path_mel),\n",
    " #                          steps_per_epoch=int(int(luscinia_train_len/10)/batchsize), epochs=1000, verbose=1,\n",
    "  #                         validation_data=train_generator_luscinia(luscinia_triplets[luscinia_train_len:luscinia_train_len+200], M_l, S_l, batchsize, emb_size, path_mel),\n",
    "   #                        validation_steps=int(200/batchsize), callbacks=[cpCallback, reduce_lr, earlystop])\n",
    "history = triplet_model.fit(train_generator_mixed(bal_training_triplets, M, S, luscinia_triplets[:luscinia_train_len],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                steps_per_epoch=int(len(bal_training_triplets)/(lo+hi)), epochs=1000, verbose=1,\n",
    "                                validation_data=train_generator_mixed(bal_val_triplets, M, S, luscinia_triplets[luscinia_train_len:],M_l, S_l, batchsize, lo, hi, lu, emb_size, path_mel),\n",
    "                                validation_steps=int(len(bal_val_triplets)/(lo+hi)), callbacks=[cpCallback, reduce_lr, earlystop])\n",
    "#history = triplet_model.fit(train_generator(dis_tr_triplets, M, S, batchsize, emb_size, path_mel),\n",
    " #                          steps_per_epoch=int(len(dis_tr_triplets)/batchsize), epochs=1000, verbose=1,\n",
    "  #                         validation_data=train_generator(dis_val_triplets, M,S, batchsize, emb_size, path_mel),\n",
    "   #                        validation_steps=int(len(dis_val_triplets)/batchsize), callbacks=[cpCallback, reduce_lr,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_high_evaluation(path_mel, vhigh_margin, vlow_margin, triplet_model, margin = 0.00000001, max_margin = 0.0000001, step = 0.00000001):\n",
    "    # pos, neg, anc\n",
    "    while margin < max_margin:\n",
    "        acc_cnt = 0\n",
    "        high_cnt = 0\n",
    "        low_cnt = 0\n",
    "        for triplet in vhigh_margin:\n",
    "            tr_pos = triplet[1][:-4]+'.pckl'\n",
    "            tr_neg = triplet[2][:-4]+'.pckl'\n",
    "            tr_anc = triplet[3][:-4]+'.pckl'\n",
    "\n",
    "            f = open(path_mel+tr_anc, 'rb')\n",
    "            anc = pickle.load(f).T\n",
    "            f.close()\n",
    "            anc = (anc - M)/S\n",
    "            anc = np.expand_dims(anc, axis=0)\n",
    "            anc = np.expand_dims(anc, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_pos, 'rb')\n",
    "            pos = pickle.load(f).T\n",
    "            f.close()\n",
    "            pos = (pos - M)/S\n",
    "            pos = np.expand_dims(pos, axis=0)\n",
    "            pos = np.expand_dims(pos, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_neg, 'rb')\n",
    "            neg = pickle.load(f).T\n",
    "            f.close()\n",
    "            neg = (neg - M)/S\n",
    "            neg = np.expand_dims(neg, axis=0)\n",
    "            neg = np.expand_dims(neg, axis=-1)\n",
    "\n",
    "            y_pred = triplet_model.predict([anc, pos, neg])\n",
    "\n",
    "            anchor1 = y_pred[:, 0:emb_size]\n",
    "            positive1 = y_pred[:, emb_size:emb_size*2]\n",
    "            negative1 = y_pred[:, emb_size*2:emb_size*3]\n",
    "\n",
    "            pos_dist = np.sqrt(np.sum(np.square(anchor1 - positive1), axis=1))[0]\n",
    "            neg_dist = np.sqrt(np.sum(np.square(anchor1 - negative1), axis=1))[0]\n",
    "\n",
    "            if np.square(neg_dist) > np.square(pos_dist) + margin:\n",
    "                acc_cnt += 1\n",
    "                high_cnt += 1\n",
    "        for triplet in vlow_margin:\n",
    "            tr_pos = triplet[1][:-4]+'.pckl'\n",
    "            tr_neg = triplet[2][:-4]+'.pckl'\n",
    "            tr_anc = triplet[3][:-4]+'.pckl'\n",
    "\n",
    "            f = open(path_mel+tr_anc, 'rb')\n",
    "            anc = pickle.load(f).T\n",
    "            f.close()\n",
    "            anc = (anc - M)/S\n",
    "            anc = np.expand_dims(anc, axis=0)\n",
    "            anc = np.expand_dims(anc, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_pos, 'rb')\n",
    "            pos = pickle.load(f).T\n",
    "            f.close()\n",
    "            pos = (pos - M)/S\n",
    "            pos = np.expand_dims(pos, axis=0)\n",
    "            pos = np.expand_dims(pos, axis=-1)\n",
    "\n",
    "            f = open(path_mel+tr_neg, 'rb')\n",
    "            neg = pickle.load(f).T\n",
    "            f.close()\n",
    "            neg = (neg - M)/S\n",
    "            neg = np.expand_dims(neg, axis=0)\n",
    "            neg = np.expand_dims(neg, axis=-1)\n",
    "\n",
    "            y_pred = triplet_model.predict([anc, pos, neg])\n",
    "\n",
    "            anchor1 = y_pred[:, 0:emb_size]\n",
    "            positive1 = y_pred[:, emb_size:emb_size*2]\n",
    "            negative1 = y_pred[:, emb_size*2:emb_size*3]\n",
    "\n",
    "            pos_dist = np.sqrt(np.sum(np.square(anchor1 - positive1), axis=1))[0]\n",
    "            neg_dist = np.sqrt(np.sum(np.square(anchor1 - negative1), axis=1))[0]\n",
    "\n",
    "            if np.abs(np.square(pos_dist) - np.square(neg_dist)) <= margin:\n",
    "                acc_cnt+=1\n",
    "                low_cnt+=1\n",
    "        print('MARGIN = ', margin)\n",
    "        print('Macro-average Low-High margin accuracy: ',0.5*(high_cnt/(len(vhigh_margin)) + low_cnt/(len(vlow_margin)))*100, '%')\n",
    "        print('Micro-average Low-High margin accuracy: ',(acc_cnt/(len(vhigh_margin)+len(vlow_margin)))*100, '%') \n",
    "        print('High margin accuracy: ',(high_cnt/(len(vhigh_margin)))*100, '%')  \n",
    "        print('Low margin accuracy: ',(low_cnt/(len(vlow_margin)))*100, '%')  \n",
    "        margin += step\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_margin = pickle.load( open(path_files+'train_triplets_low_50_70_ACC70.pckl', 'rb'))\n",
    "vlow_margin = pickle.load(open(path_files+'val_triplets_low_50_70_ACC70.pckl', 'rb'))\n",
    "high_margin = pickle.load(open(path_files+'train_triplets_high_50_70_ACC70.pckl', 'rb'))\n",
    "vhigh_margin =pickle.load(open(path_files+'val_triplets_high_50_70_ACC70.pckl', 'rb'))\n",
    "tlow_margin =pickle.load(open(path_files+'test_triplets_low_50_70_ACC70.pckl', 'rb'))\n",
    "thigh_margin = pickle.load(open(path_files+'test_triplets_high_50_70_ACC70.pckl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_high_evauation(path_mel, high_margin, low_margin, triplet_model, margin = 0.0, max_margin = 0.01, step = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path_files+'TunedLusciniaV1e.csv'\n",
    "comp = []\n",
    "header = 1\n",
    "with open(file) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        if header == 1:\n",
    "            header = 0\n",
    "            print(row)\n",
    "            continue\n",
    "        if row:\n",
    "            if len(row[-1].split(' '))>0:\n",
    "                row[-1] = row[-1].replace(' ', '_')\n",
    "            if len(row[-2].split(' '))>0:\n",
    "                row[-2] = row[-2].replace(' ', '_')\n",
    "            comp.append([row[-3], row[-2], row[-1]])\n",
    "luscinia_tuned = np.asarray(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luscinia_comparison(path_mel, vhigh_margin, triplet_model, luscinia_tuned):\n",
    "    acc = 0\n",
    "    agree = []\n",
    "    for triplet in vhigh_margin:\n",
    "        tr_pos = triplet[1][:-4]+'.pckl'\n",
    "        tr_neg = triplet[2][:-4]+'.pckl'\n",
    "        tr_anc = triplet[3][:-4]+'.pckl'\n",
    "\n",
    "        f = open(path_mel+tr_anc, 'rb')\n",
    "        anc = pickle.load(f).T\n",
    "        f.close()\n",
    "        anc = (anc - M)/S\n",
    "        anc = np.expand_dims(anc, axis=0)\n",
    "        anc = np.expand_dims(anc, axis=-1)\n",
    "\n",
    "        f = open(path_mel+tr_pos, 'rb')\n",
    "        pos = pickle.load(f).T\n",
    "        f.close()\n",
    "        pos = (pos - M)/S\n",
    "        pos = np.expand_dims(pos, axis=0)\n",
    "        pos = np.expand_dims(pos, axis=-1)\n",
    "\n",
    "        f = open(path_mel+tr_neg, 'rb')\n",
    "        neg = pickle.load(f).T\n",
    "        f.close()\n",
    "        neg = (neg - M)/S\n",
    "        neg = np.expand_dims(neg, axis=0)\n",
    "        neg = np.expand_dims(neg, axis=-1)\n",
    "\n",
    "        y_pred = triplet_model.predict([anc, pos, neg])\n",
    "\n",
    "        anchor1 = y_pred[:, 0:emb_size]\n",
    "        positive1 = y_pred[:, emb_size:emb_size*2]\n",
    "        negative1 = y_pred[:, emb_size*2:emb_size*3]\n",
    "\n",
    "        pos_dist = np.sum(np.abs(anchor1 - positive1), axis=1)[0]\n",
    "        neg_dist = np.sum(np.abs(anchor1 - negative1), axis=1)[0]\n",
    "\n",
    "        if np.square(neg_dist) > np.square(pos_dist):\n",
    "            dnn = 1\n",
    "        else:\n",
    "            dnn = 0\n",
    "        \n",
    "        pos = triplet[1]\n",
    "        neg = triplet[2]\n",
    "        anc = triplet[3]\n",
    "            \n",
    "        pos_i_1 = np.squeeze(np.argwhere(luscinia_tuned[:,-1]==pos), axis=1)\n",
    "        neg_i_1 = np.squeeze(np.argwhere(luscinia_tuned[:,-1]==neg), axis=1)\n",
    "        pos_i_2 = np.squeeze(np.argwhere(luscinia_tuned[:,-2]==pos), axis=1)\n",
    "        neg_i_2 = np.squeeze(np.argwhere(luscinia_tuned[:,-2]==neg), axis=1)\n",
    "            \n",
    "        anc_i_1 = np.squeeze(np.argwhere(luscinia_tuned[:,-1] == anc), axis=1)\n",
    "        anc_i_2 = np.squeeze(np.argwhere(luscinia_tuned[:,-2] == anc), axis=1)\n",
    "\n",
    "        if len(np.intersect1d(pos_i_1, anc_i_2, assume_unique=True))>0:\n",
    "            row_p = np.intersect1d(pos_i_1, anc_i_2, assume_unique=True)[0]\n",
    "        elif len(np.intersect1d(pos_i_2, anc_i_1, assume_unique=True))>0:\n",
    "            row_p = np.intersect1d(pos_i_2, anc_i_1, assume_unique=True)[0]\n",
    "        else:\n",
    "            print('ERROR')\n",
    "                \n",
    "        if len(np.intersect1d(neg_i_1, anc_i_2, assume_unique=True))>0:\n",
    "            row_n = np.intersect1d(neg_i_1, anc_i_2, assume_unique=True)[0]\n",
    "        elif len(np.intersect1d(neg_i_2, anc_i_1, assume_unique=True))>0:\n",
    "            row_n = np.intersect1d(neg_i_2, anc_i_1, assume_unique=True)[0]\n",
    "        else:\n",
    "            print('ERROR')\n",
    "           \n",
    "\n",
    "        acc_p = 1 - float(luscinia_tuned[row_p,-3])\n",
    "        acc_n = 1- float(luscinia_tuned[row_n,-3])\n",
    "\n",
    "        if np.square(float(acc_p)) > np.square(float(acc_n)):  # sim_p > sim_n + margin\n",
    "            lusc = 1\n",
    "        else:\n",
    "            lusc = 0\n",
    "        \n",
    "        if dnn == lusc:\n",
    "            acc += 1\n",
    "        agree.append([dnn,lusc])\n",
    "    print('total agreement ', acc, ' out of ', len(vhigh_margin))\n",
    "    print('agreement(%): ', acc/len(vhigh_margin)*100)\n",
    "    return agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.load_weights('ZF_emb_'+str(emb_size)+'D_LUSCINIA_PRE_MIXED_margin_loss.h5')\n",
    "margin = luscinia_comparison(path_mel, thigh_margin, triplet_model, luscinia_tuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
